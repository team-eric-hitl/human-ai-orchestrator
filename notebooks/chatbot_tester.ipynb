{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Chatbot Agent with pre-generated questions.\n",
    "It's designed for users with little programming experience.\n",
    "\n",
    "## Features:\n",
    "- Load and edit agent configuration settings\n",
    "- Load test questions from JSON files\n",
    "- Process questions through the Chatbot Agent\n",
    "- Review and analyze results\n",
    "- Export results with timestamps\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load test questions from file (generated using question_generator.ipynb)\n",
    "4. Review questions before processing\n",
    "5. Run the agent and review results\n",
    "\n",
    "## Question Generation:\n",
    "Use the separate `question_generator.ipynb` notebook to create test questions, then load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML  # Changed from: import yaml\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports (this helps with relative imports)\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.chatbot_agent import ChatbotAgentNode\n",
    "from src.core.config.agent_config_manager import AgentConfigManager\n",
    "from src.integrations.llm_providers import LLMProviderFactory\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start testing the Chatbot Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration Settings\n",
    "\n",
    "The following cell loads the current configuration for the Chatbot Agent.\n",
    "You can edit these values to customize the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from files and create temporary editable copies\n",
    "config_base_path = Path('/workspace/config')\n",
    "agent_config_path = config_base_path / 'agents' / 'chatbot_agent'\n",
    "temp_config_dir = Path('/tmp/chatbot_agent_configs')\n",
    "\n",
    "def load_and_create_temp_configs():\n",
    "    \"\"\"Load all configuration files and create temporary editable copies with comments preserved\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Create YAML instance for comment preservation\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True\n",
    "    yaml.default_flow_style = False\n",
    "    \n",
    "    # Create temp directory\n",
    "    temp_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load original files for parsing (to access data)\n",
    "    with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "        configs['agent'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "        configs['prompts'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "        configs['models'] = yaml.load(f)\n",
    "    \n",
    "    # Load shared models for reference\n",
    "    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n",
    "        configs['shared_models'] = yaml.load(f)\n",
    "    \n",
    "    # Create temp file paths\n",
    "    temp_agent_path = temp_config_dir / 'config.yaml'\n",
    "    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n",
    "    temp_models_path = temp_config_dir / 'models.yaml'\n",
    "    \n",
    "    # Copy original files to temp directory to preserve comments and formatting\n",
    "    import shutil\n",
    "    shutil.copy2(agent_config_path / 'config.yaml', temp_agent_path)\n",
    "    shutil.copy2(agent_config_path / 'prompts.yaml', temp_prompts_path)  \n",
    "    shutil.copy2(agent_config_path / 'models.yaml', temp_models_path)\n",
    "    \n",
    "    return configs, {\n",
    "        'agent_config': temp_agent_path,\n",
    "        'prompts_config': temp_prompts_path,\n",
    "        'models_config': temp_models_path\n",
    "    }\n",
    "\n",
    "# Load configurations and create temp files\n",
    "configs, temp_file_paths = load_and_create_temp_configs()\n",
    "\n",
    "print(\"üìÅ Configuration files loaded and temporary copies created with comments preserved!\")\n",
    "print(f\"Agent name: {configs['agent']['agent']['name']}\")\n",
    "print(f\"Agent version: {configs['agent']['agent']['version']}\")\n",
    "\n",
    "# Get preferred model from models config (not agent config)\n",
    "preferred_model = \"Unknown\"\n",
    "if 'primary_model' in configs['models']:\n",
    "    preferred_model = configs['models']['primary_model']\n",
    "elif 'preferred' in configs['models']:\n",
    "    preferred_model = configs['models']['preferred']\n",
    "\n",
    "print(f\"Preferred model: {preferred_model}\")\n",
    "print(f\"\\nüíæ Temporary config files created at:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "print(f\"\\nüí° These temp files retain original comments and can be edited directly in Step 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Editable Configuration Settings\n",
    "\n",
    "Edit these settings to customize how the Chatbot Agent behaves.\n",
    "These variables map directly to the configuration files and can be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display and edit configuration files in separate windows\n",
    "\n",
    "def load_config_file_contents():\n",
    "    \"\"\"Load current config file contents from temp files\"\"\"\n",
    "    with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "        agent_config_content = f.read()\n",
    "    with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "        prompts_config_content = f.read()\n",
    "    with open(temp_file_paths['models_config'], 'r') as f:\n",
    "        models_config_content = f.read()\n",
    "    \n",
    "    return agent_config_content, prompts_config_content, models_config_content\n",
    "\n",
    "# Load current config file contents\n",
    "agent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration File Editor\")\n",
    "print(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\n",
    "print(\"Changes are saved to temporary files and will be used in Step 5.\\n\")\n",
    "\n",
    "# Create text areas for each config file\n",
    "print(\"üìÑ 1. Agent Configuration (config.yaml)\")\n",
    "print(\"Contains: agent settings, behavior, escalation thresholds\")\n",
    "\n",
    "agent_config_editor = widgets.Textarea(\n",
    "    value=agent_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_agent_config(button):\n",
    "    \"\"\"Save agent config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(agent_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['agent_config'], 'w') as f:\n",
    "            f.write(agent_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Agent config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n",
    "\n",
    "agent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\n",
    "agent_save_btn.on_click(save_agent_config)\n",
    "\n",
    "display(agent_config_editor)\n",
    "display(agent_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 2. Prompts Configuration (prompts.yaml)\")\n",
    "print(\"Contains: system prompts, response guidelines, communication style\")\n",
    "\n",
    "prompts_config_editor = widgets.Textarea(\n",
    "    value=prompts_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_prompts_config(button):\n",
    "    \"\"\"Save prompts config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(prompts_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['prompts_config'], 'w') as f:\n",
    "            f.write(prompts_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Prompts config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n",
    "\n",
    "prompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\n",
    "prompts_save_btn.on_click(save_prompts_config)\n",
    "\n",
    "display(prompts_config_editor)\n",
    "display(prompts_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 3. Models Configuration (models.yaml)\")\n",
    "print(\"Contains: preferred model, fallback models, model-specific settings\")\n",
    "\n",
    "# Show available model aliases from shared models config\n",
    "def display_available_models():\n",
    "    \"\"\"Display available model aliases and their actual models\"\"\"\n",
    "    try:\n",
    "        shared_models = configs['shared_models']\n",
    "        \n",
    "        # Extract model aliases and models sections\n",
    "        model_aliases = shared_models.get('model_aliases', {})\n",
    "        models = shared_models.get('models', {})\n",
    "        \n",
    "        if not model_aliases:\n",
    "            print(\"‚ùå No model aliases found in shared configuration\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nüîç Available Model Aliases:\")\n",
    "        print(\"Use these aliases in your models configuration below:\\n\")\n",
    "        \n",
    "        # Group by provider for better organization\n",
    "        providers = {}\n",
    "        for alias, actual_model_name in model_aliases.items():\n",
    "            # Get model details from models section\n",
    "            model_details = models.get(actual_model_name, {})\n",
    "            provider = model_details.get('type', 'unknown')\n",
    "            description = model_details.get('description', '')\n",
    "            \n",
    "            if provider not in providers:\n",
    "                providers[provider] = []\n",
    "            providers[provider].append({\n",
    "                'alias': alias,\n",
    "                'model_name': actual_model_name,\n",
    "                'description': description\n",
    "            })\n",
    "        \n",
    "        # Display by provider\n",
    "        for provider, provider_models in providers.items():\n",
    "            print(f\"üì° {provider.upper()} Provider:\")\n",
    "            for model in provider_models:\n",
    "                desc = f\" - {model['description']}\" if model['description'] else \"\"\n",
    "                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n",
    "            print()\n",
    "        \n",
    "        # Show current configuration\n",
    "        try:\n",
    "            # Use ruamel.yaml for parsing\n",
    "            yaml = YAML()\n",
    "            yaml.preserve_quotes = True\n",
    "            current_models_config = yaml.load(models_config_content)\n",
    "            current_preferred = current_models_config.get('primary_model', current_models_config.get('preferred', 'unknown'))\n",
    "            current_fallback = current_models_config.get('fallback', [])\n",
    "        except:\n",
    "            current_preferred = 'unknown'\n",
    "            current_fallback = []\n",
    "        \n",
    "        print(f\"üìã Current Models Configuration:\")\n",
    "        print(f\"  Preferred: {current_preferred}\")\n",
    "        print(f\"  Fallback: {current_fallback}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üí° Tips for editing:\")\n",
    "        print(\"  ‚Ä¢ Change 'primary_model' or 'preferred' to any alias from the list above\")\n",
    "        print(\"  ‚Ä¢ Add/remove aliases in the 'fallback' list\")\n",
    "        print(\"  ‚Ä¢ Aliases are case-sensitive\")\n",
    "        print(\"  ‚Ä¢ Invalid aliases will cause errors during processing\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading available models: {e}\")\n",
    "        print(\"Continuing with models configuration editor...\\n\")\n",
    "\n",
    "# Display available models before showing the editor\n",
    "display_available_models()\n",
    "\n",
    "models_config_editor = widgets.Textarea(\n",
    "    value=models_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='200px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_models_config(button):\n",
    "    \"\"\"Save models config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        parsed_config = yaml.load(models_config_editor.value)\n",
    "        \n",
    "        # Additional validation for model aliases\n",
    "        if isinstance(parsed_config, dict):\n",
    "            preferred = parsed_config.get('primary_model') or parsed_config.get('preferred')\n",
    "            fallback = parsed_config.get('fallback', [])\n",
    "            \n",
    "            # Get available aliases\n",
    "            model_aliases = configs['shared_models'].get('model_aliases', {})\n",
    "            \n",
    "            # Check if preferred model exists\n",
    "            if preferred and preferred not in model_aliases:\n",
    "                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n",
    "            \n",
    "            # Check fallback models\n",
    "            for fb_model in fallback:\n",
    "                if fb_model not in model_aliases:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Fallback model '{fb_model}' not found in available model aliases\")\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['models_config'], 'w') as f:\n",
    "            f.write(models_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Models config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in models config: {e}\")\n",
    "\n",
    "models_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\n",
    "models_save_btn.on_click(save_models_config)\n",
    "\n",
    "display(models_config_editor)\n",
    "display(models_save_btn)\n",
    "\n",
    "# Save All button for convenience\n",
    "def save_all_configs(button):\n",
    "    \"\"\"Save all config changes at once\"\"\"\n",
    "    save_agent_config(None)\n",
    "    save_prompts_config(None)\n",
    "    save_models_config(None)\n",
    "\n",
    "print(\"\\nüíæ Save All Changes\")\n",
    "save_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\n",
    "save_all_btn.on_click(save_all_configs)\n",
    "display(save_all_btn)\n",
    "\n",
    "print(f\"\\nüíæ Temp config files location:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"  ‚Ä¢ Edit YAML directly in the text areas above\")\n",
    "print(\"  ‚Ä¢ Use Save buttons to apply changes to temp files\")\n",
    "print(\"  ‚Ä¢ YAML syntax is validated before saving\")\n",
    "print(\"  ‚Ä¢ Model aliases are validated against available models\")\n",
    "print(\"  ‚Ä¢ Changes will be used in Step 5 when processing questions\")\n",
    "print(\"  ‚Ä¢ Original config files remain unchanged\")\n",
    "print(\"  ‚Ä¢ Comments and formatting are preserved during editing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2.5: Understanding Chatbot Internal Logic\n\nNow that you've seen the configuration settings, let's understand how the chatbot processes customer queries and how your configuration changes affect its behavior.\n\n### How the Chatbot Agent Works Internally\n\nThe Chatbot Agent follows a sophisticated multi-stage process for each customer interaction:\n\n#### 1. **Initial Assessment Phase**\n- **Query Analysis**: The chatbot first analyzes the customer's question to understand:\n  - Intent and urgency level\n  - Technical complexity \n  - Emotional tone (frustration, confusion, etc.)\n  - Required information categories\n- **Context Integration**: Retrieves previous conversation history and relevant customer data\n- **Confidence Scoring**: Assigns a confidence score (0.0-1.0) based on how well it understands the query\n\n#### 2. **Response Generation Phase**\n- **System Prompt Application**: Uses the `system` prompt from `prompts.yaml` to establish personality and behavior\n- **Model Selection**: Chooses the appropriate AI model based on query complexity:\n  - Simple queries ‚Üí Uses `primary_model` from `models.yaml`\n  - Complex technical queries ‚Üí May escalate to premium models if configured\n  - Reasoning-heavy queries ‚Üí Uses models optimized for reasoning\n- **Response Crafting**: Generates response following the guidelines in `prompts.yaml`\n\n#### 3. **Quality Control Phase**\n- **Escalation Threshold Check**: Compares confidence score against thresholds in `config.yaml`:\n  - If confidence < `escalation.confidence_threshold` ‚Üí Triggers escalation\n  - If query complexity > agent capabilities ‚Üí Routes to human specialist\n- **Response Validation**: Ensures response meets quality standards\n- **Safety Checks**: Verifies no inappropriate or harmful content\n\n### How Configuration Settings Affect Behavior\n\n#### **Temperature Setting** (`config.yaml: settings.temperature`)\n- **Low (0.0-0.3)**: More deterministic, consistent responses\n  - *Best for*: Policy explanations, factual information\n  - *Trade-off*: May sound robotic or repetitive\n- **Medium (0.4-0.7)**: Balanced creativity and consistency  \n  - *Best for*: General customer service, explanations\n  - *Trade-off*: Good balance of personality and accuracy\n- **High (0.8-1.0)**: More creative, varied responses\n  - *Best for*: Empathetic responses, complex problem-solving\n  - *Trade-off*: May be less predictable or occasionally off-topic\n\n#### **System Prompt** (`prompts.yaml: system`)\nThis is the chatbot's \"personality and instructions.\" Key components:\n- **Role Definition**: \"You are a professional customer service chatbot...\"\n- **Communication Style**: Friendly, helpful, professional tone guidelines\n- **Response Structure**: How to organize information in responses\n- **Escalation Guidance**: When and how to refer customers to human agents\n- **Service Standards**: Quality expectations and customer satisfaction goals\n\n*Example Impact*: Adding \"Be concise and direct\" to the system prompt will make responses shorter and more to-the-point.\n\n#### **Model Selection** (`models.yaml`)\n- **Primary Model**: The default model used for most queries\n  - `anthropic_general_standard`: Balanced performance, good for most queries\n  - `local_general_standard`: Faster local processing, may be less sophisticated\n  - `openai_general_standard`: High-quality responses, requires API access\n- **Fallback Models**: Used when primary model is unavailable\n- **Model Preferences by Query Type**: Different models for different scenarios\n  - `general_queries.primary`: Standard customer service questions\n  - `complex_queries.primary`: Technical or complicated issues  \n  - `escalation_queries.primary`: When preparing to transfer to human\n\n#### **Escalation Thresholds** (`config.yaml: escalation`)\n- **Confidence Threshold**: Minimum confidence to attempt answering\n  - Lower values ‚Üí More willing to try answering difficult questions\n  - Higher values ‚Üí More conservative, escalates uncertain queries faster\n- **Max Attempts**: How many clarification questions to ask before escalating\n- **Priority Routing**: Which human departments to route different query types\n\n#### **Behavior Settings** (`config.yaml: behavior`)\n- **Response Length Preferences**: Target length for different response types\n- **Follow-up Strategy**: How proactive to be in asking clarifying questions\n- **Empathy Level**: How much emotional intelligence to apply\n- **Technical Depth**: How detailed to get with technical explanations\n\n### Real-World Impact Examples\n\n#### Scenario: Customer asks \"My claim was denied, what do I do?\"\n\n**With Conservative Settings** (high escalation threshold, low temperature):\n- High confidence threshold ‚Üí May immediately escalate to human agent\n- Low temperature ‚Üí Gives standard, policy-based response\n- Result: Quick escalation, consistent but potentially impersonal\n\n**With Balanced Settings** (medium thresholds, medium temperature):\n- Medium confidence ‚Üí Asks clarifying questions first\n- Balanced temperature ‚Üí Provides empathetic but accurate response\n- Result: Attempts to help while showing understanding\n\n**With Aggressive Settings** (low escalation threshold, high temperature):\n- Low confidence threshold ‚Üí Attempts detailed explanation\n- High temperature ‚Üí Creative, personalized response approach\n- Result: More comprehensive help but potentially longer interaction\n\n### Configuration Optimization Tips\n\n1. **For High-Volume, Simple Queries**:\n   - Use faster models (`local_general_standard`)\n   - Lower temperature for consistency\n   - Higher escalation thresholds to reduce human load\n\n2. **For Complex Technical Support**:\n   - Use reasoning-optimized models (`anthropic_reasoning_premium`)\n   - Medium-high temperature for creative problem-solving\n   - Lower escalation thresholds to ensure accuracy\n\n3. **For Frustrated Customers**:\n   - Emphasize empathy in system prompts\n   - Medium temperature for personalized responses\n   - Faster escalation to human agents when emotions are high\n\n4. **For Cost Optimization**:\n   - Prefer local models over API-based models\n   - Use budget models for simple queries\n   - Set appropriate escalation thresholds to balance cost and quality\n\n### Monitoring and Adjustment\n\nThe chatbot's performance can be monitored through:\n- **Confidence Scores**: Track how certain the chatbot is about responses\n- **Escalation Rates**: Monitor how often queries are passed to humans\n- **Customer Satisfaction**: Measure response quality and helpfulness\n- **Response Times**: Balance quality with speed requirements\n- **Conversation Length**: Optimize for efficiency while maintaining quality\n\n*Understanding these internals helps you make informed configuration changes that improve customer experience while managing operational costs.*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Questions\n",
    "\n",
    "Load test questions from a JSON file. Use question_generator.ipynb to create new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File upload widget for loading questions\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.json',\n",
    "    multiple=False,\n",
    "    description='Upload questions file:'\n",
    ")\n",
    "\n",
    "# Instructions for file format\n",
    "print(\"üìù Load Test Questions\")\n",
    "print(\"\\nüí° How to get test questions:\")\n",
    "print(\"1. Use question_generator.ipynb to create test questions\")\n",
    "print(\"2. Upload the generated JSON file below\")\n",
    "print(\"3. Supported formats: Full export or questions-only JSON\")\n",
    "print(\"\\nüìÑ Expected JSON format:\")\n",
    "print(\"- Full export: {'metadata': {...}, 'questions': [...]}\")\n",
    "print(\"- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\")\n",
    "print(\"\\nüìÅ Upload your questions file:\")\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Load test questions from uploaded file\ntest_questions = []\n\ndef load_questions_from_file(file_content, filename):\n    \"\"\"Load questions from uploaded JSON file\"\"\"\n    try:\n        # Handle different content types\n        if isinstance(file_content, memoryview):\n            # Convert memoryview to bytes\n            content_bytes = file_content.tobytes()\n        elif hasattr(file_content, 'decode'):\n            # Already bytes\n            content_bytes = file_content\n        else:\n            # Convert to bytes if it's a string or other type\n            content_bytes = str(file_content).encode('utf-8')\n        \n        # Decode to string and parse JSON\n        data = json.loads(content_bytes.decode('utf-8'))\n        \n        # Handle different JSON formats\n        if isinstance(data, dict):\n            # Full export format with metadata\n            if 'questions' in data:\n                questions = data['questions']\n                metadata = data.get('metadata', {})\n                print(f\"üìÑ Loaded file with metadata:\")\n                print(f\"  Generation model: {metadata.get('generation_model', 'unknown')}\")\n                print(f\"  Generation timestamp: {metadata.get('generation_timestamp', 'unknown')}\")\n                print(f\"  Question count: {metadata.get('question_count', len(questions))}\")\n                return questions\n            else:\n                # Single question object\n                return [data]\n        elif isinstance(data, list):\n            # Questions-only format\n            return data\n        else:\n            print(f\"‚ùå Unexpected data format: {type(data)}\")\n            return []\n            \n    except json.JSONDecodeError as e:\n        print(f\"‚ùå JSON parsing error: {e}\")\n        return []\n    except Exception as e:\n        print(f\"‚ùå Error loading file: {e}\")\n        print(f\"   File content type: {type(file_content)}\")\n        return []\n\ndef validate_questions(questions):\n    \"\"\"Validate and normalize question format\"\"\"\n    validated_questions = []\n    \n    for i, q in enumerate(questions):\n        if isinstance(q, dict):\n            # Ensure required fields exist\n            validated_q = {\n                \"id\": q.get(\"id\", i + 1),\n                \"question\": q.get(\"question\", f\"Question {i + 1}\"),\n                \"customer_type\": q.get(\"customer_type\", \"normal\"),\n                \"complexity\": q.get(\"complexity\", \"medium\")\n            }\n            validated_questions.append(validated_q)\n        else:\n            # Convert string to dict if needed\n            validated_q = {\n                \"id\": i + 1,\n                \"question\": str(q),\n                \"customer_type\": \"normal\",\n                \"complexity\": \"medium\"\n            }\n            validated_questions.append(validated_q)\n    \n    return validated_questions\n\n# Process uploaded file\nif file_upload.value:\n    uploaded_file = None\n    filename = None\n    file_content = None\n    \n    # Handle different file upload widget formats\n    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n        print(\"üìã Using tuple format\")\n        uploaded_file = file_upload.value[0]\n        filename = uploaded_file['name']\n        file_content = uploaded_file['content']\n        print(f\"üîç File content type: {type(file_content)}\")\n    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n        print(\"üìã Using dict format\")\n        uploaded_file = list(file_upload.value.values())[0]\n        filename = uploaded_file['metadata']['name']\n        file_content = uploaded_file['content']\n        print(f\"üîç File content type: {type(file_content)}\")\n    else:\n        print(f\"‚ùå Unable to read uploaded file format\")\n        print(f\"   Type: {type(file_upload.value)}\")\n        print(f\"   Length: {len(file_upload.value) if hasattr(file_upload.value, '__len__') else 'No length'}\")\n        print(f\"   Content: {file_upload.value}\")\n    \n    if uploaded_file and filename and file_content is not None:\n        print(f\"üìÅ Loading questions from: {filename}\")\n        \n        raw_questions = load_questions_from_file(file_content, filename)\n        \n        if raw_questions:\n            test_questions = validate_questions(raw_questions)\n            print(f\"‚úÖ Loaded {len(test_questions)} test questions\")\n            \n            # Display first few questions as preview\n            print(\"\\nüìã Preview of loaded questions:\")\n            for i, q in enumerate(test_questions[:3]):\n                print(f\"  {i+1}. {q['question']} [{q['customer_type']}]\")\n            if len(test_questions) > 3:\n                print(f\"  ... and {len(test_questions) - 3} more questions\")\n                \n            # Show distribution\n            df_preview = pd.DataFrame(test_questions)\n            print(\"\\nüìä Question Distribution:\")\n            print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n            print(f\"  Complexities: {dict(df_preview['complexity'].value_counts())}\")\n            \n        else:\n            print(\"‚ùå No questions loaded from file\")\n    else:\n        print(\"‚ùå Error accessing uploaded file\")\n        print(f\"   uploaded_file: {uploaded_file is not None}\")\n        print(f\"   filename: {filename}\")\n        print(f\"   file_content: {file_content is not None}\")\nelse:\n    print(\"‚ö†Ô∏è Please upload a JSON file with test questions.\")\n    print(\"üí° Use question_generator.ipynb to create test questions first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review and Edit Questions\n",
    "\n",
    "Review the loaded questions and make any edits before processing."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Create interactive question editor\nif test_questions:\n    print(\"üìù Question Editor - You can modify questions before processing\")\n    print(\"Edit the questions in the table below, then run the next cell to process them.\\n\")\n    \n    # Convert to DataFrame for easy editing\n    df = pd.DataFrame(test_questions)\n    \n    # Ensure required columns exist\n    if 'id' not in df.columns:\n        df['id'] = range(1, len(df) + 1)\n    if 'customer_type' not in df.columns:\n        df['customer_type'] = 'normal'\n    if 'complexity' not in df.columns:\n        df['complexity'] = 'medium'\n    \n    # Display editable table\n    print(\"Current questions (you can edit the JSON below if needed):\")\n    display(df)\n    \n    # Show JSON for manual editing if needed\n    questions_json = widgets.Textarea(\n        value=json.dumps(test_questions, indent=2),\n        description=\"Questions JSON:\",\n        layout=widgets.Layout(width='100%', height='200px')\n    )\n    \n    print(\"\\nAdvanced: Edit questions as JSON (optional):\")\n    display(questions_json)\n    \n    def update_questions_from_json():\n        \"\"\"Update questions from the JSON editor\"\"\"\n        global test_questions\n        try:\n            test_questions = json.loads(questions_json.value)\n            print(\"‚úÖ Questions updated from JSON editor\")\n        except Exception as e:\n            print(f\"‚ùå Error parsing JSON: {e}\")\n    \n    # Button to update from JSON\n    update_btn = widgets.Button(description=\"Update from JSON\")\n    update_btn.on_click(lambda b: update_questions_from_json())\n    display(update_btn)\n    \nelse:\n    print(\"‚ö†Ô∏è No questions loaded. Please upload a questions file in the previous step.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Complete Conversations with Customer AI\n",
    "\n",
    "Continue conversations between customer AI and chatbot AI until natural resolution or escalation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results and Settings\n",
    "\n",
    "Save the results and configuration settings to files with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Review Results\n",
    "\n",
    "Display and analyze the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and analyze conversation results\n",
    "if 'results' in locals() and results:\n",
    "    print(\"üìã Conversation Results Review and Analysis\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'id': r.get('id'),\n",
    "            'customer_type': r.get('customer_type'),\n",
    "            'complexity': r.get('complexity'),\n",
    "            'total_turns': r.get('total_turns', 0),\n",
    "            'final_outcome': r.get('final_outcome'),\n",
    "            'customer_satisfaction': r.get('customer_satisfaction', 0),\n",
    "            'conversation_complete': r.get('conversation_complete', False)\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"=== CONVERSATION SUMMARY STATISTICS ===\")\n",
    "    print(f\"Total conversations: {len(df_results)}\")\n",
    "    print(f\"Completed conversations: {df_results['conversation_complete'].sum()}\")\n",
    "    print(f\"Average turns per conversation: {df_results['total_turns'].mean():.1f}\")\n",
    "    print(f\"Average customer satisfaction: {df_results['customer_satisfaction'].mean():.3f}\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(\"\\n=== CONVERSATION OUTCOMES ===\")\n",
    "    outcome_counts = df_results['final_outcome'].value_counts()\n",
    "    for outcome, count in outcome_counts.items():\n",
    "        percentage = count / len(df_results) * 100\n",
    "        print(f\"{outcome}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analysis by customer type\n",
    "    print(\"\\n=== ANALYSIS BY CUSTOMER TYPE ===\")\n",
    "    customer_analysis = df_results.groupby('customer_type').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    customer_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(customer_analysis)\n",
    "    \n",
    "    # Satisfaction rate by customer type\n",
    "    print(\"\\nSatisfaction Rate by Customer Type:\")\n",
    "    for ctype in df_results['customer_type'].unique():\n",
    "        ctype_data = df_results[df_results['customer_type'] == ctype]\n",
    "        satisfied = len(ctype_data[ctype_data['final_outcome'] == 'satisfied'])\n",
    "        total = len(ctype_data)\n",
    "        print(f\"  {ctype}: {satisfied}/{total} ({satisfied/total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by complexity\n",
    "    print(\"\\n=== ANALYSIS BY COMPLEXITY ===\")\n",
    "    complexity_analysis = df_results.groupby('complexity').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    complexity_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(complexity_analysis)\n",
    "    \n",
    "    # Show detailed results table\n",
    "    print(\"\\n=== CONVERSATION SUMMARY TABLE ===\")\n",
    "    display_df = df_results[['id', 'customer_type', 'complexity', 'total_turns', 'final_outcome', 'customer_satisfaction']].copy()\n",
    "    display_df['customer_satisfaction'] = display_df['customer_satisfaction'].round(3)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Show sample full conversations\n",
    "    print(\"\\n=== SAMPLE FULL CONVERSATIONS ===\")\n",
    "    \n",
    "    # Show 2 satisfied and 2 escalated conversations for analysis\n",
    "    satisfied_conversations = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    def display_conversation(conversation_data, max_turns=3):\n",
    "        \"\"\"Display a conversation with turn-by-turn analysis\"\"\"\n",
    "        conv_id = conversation_data.get('id')\n",
    "        customer_type = conversation_data.get('customer_type')\n",
    "        complexity = conversation_data.get('complexity')\n",
    "        outcome = conversation_data.get('final_outcome')\n",
    "        satisfaction = conversation_data.get('customer_satisfaction', 0)\n",
    "        \n",
    "        print(f\"\\n--- Conversation {conv_id} ({customer_type}, {complexity}) ---\")\n",
    "        print(f\"Original Question: {conversation_data.get('original_question', '')}\")\n",
    "        print(f\"Final Outcome: {outcome} (Customer Satisfaction: {satisfaction:.2f})\")\n",
    "        \n",
    "        conversation_history = conversation_data.get('conversation_history', [])\n",
    "        turns_to_show = min(max_turns, len(conversation_history))\n",
    "        \n",
    "        for i, turn in enumerate(conversation_history[:turns_to_show]):\n",
    "            turn_num = turn.get('turn_number', i+1)\n",
    "            print(f\"\\n  Turn {turn_num}:\")\n",
    "            print(f\"    Customer: {turn.get('customer_query', '')[:150]}{'...' if len(turn.get('customer_query', '')) > 150 else ''}\")\n",
    "            print(f\"    Chatbot:  {turn.get('chatbot_response', '')[:150]}{'...' if len(turn.get('chatbot_response', '')) > 150 else ''}\")\n",
    "            \n",
    "            if 'customer_response' in turn:\n",
    "                print(f\"    Customer Reply: {turn.get('customer_response', '')[:150]}{'...' if len(turn.get('customer_response', '')) > 150 else ''}\")\n",
    "                print(f\"    Satisfaction: {turn.get('customer_satisfaction', 0):.2f}\")\n",
    "        \n",
    "        if len(conversation_history) > turns_to_show:\n",
    "            print(f\"  ... and {len(conversation_history) - turns_to_show} more turns\")\n",
    "    \n",
    "    # Show satisfied conversations\n",
    "    if satisfied_conversations:\n",
    "        print(\"\\nüü¢ SATISFIED CUSTOMER CONVERSATIONS:\")\n",
    "        for conv in satisfied_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Show escalated conversations  \n",
    "    if escalated_conversations:\n",
    "        print(\"\\nüî¥ ESCALATED CONVERSATIONS:\")\n",
    "        for conv in escalated_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Conversation insights\n",
    "    print(\"\\n=== CONVERSATION INSIGHTS ===\")\n",
    "    \n",
    "    # Turn analysis\n",
    "    short_conversations = len(df_results[df_results['total_turns'] <= 2])\n",
    "    medium_conversations = len(df_results[(df_results['total_turns'] > 2) & (df_results['total_turns'] <= 5)])\n",
    "    long_conversations = len(df_results[df_results['total_turns'] > 5])\n",
    "    \n",
    "    print(f\"Conversation Length Distribution:\")\n",
    "    print(f\"  Short (1-2 turns): {short_conversations} ({short_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (3-5 turns): {medium_conversations} ({medium_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Long (6+ turns): {long_conversations} ({long_conversations/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Satisfaction insights\n",
    "    high_satisfaction = len(df_results[df_results['customer_satisfaction'] >= 0.8])\n",
    "    medium_satisfaction = len(df_results[(df_results['customer_satisfaction'] >= 0.5) & (df_results['customer_satisfaction'] < 0.8)])\n",
    "    low_satisfaction = len(df_results[df_results['customer_satisfaction'] < 0.5])\n",
    "    \n",
    "    print(f\"\\nCustomer Satisfaction Distribution:\")\n",
    "    print(f\"  High (0.8+): {high_satisfaction} ({high_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.5-0.8): {medium_satisfaction} ({medium_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Low (<0.5): {low_satisfaction} ({low_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\")\n",
    "    print(\"üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to review. Please complete conversations first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}