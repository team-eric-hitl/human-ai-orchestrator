{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Chatbot Agent with pre-generated questions.\n",
    "It's designed for users with little programming experience.\n",
    "\n",
    "## Features:\n",
    "- Load and edit agent configuration settings\n",
    "- Load test questions from JSON files\n",
    "- Process questions through the Chatbot Agent\n",
    "- Review and analyze results\n",
    "- Export results with timestamps\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load test questions from file (generated using question_generator.ipynb)\n",
    "4. Review questions before processing\n",
    "5. Run the agent and review results\n",
    "\n",
    "## Question Generation:\n",
    "Use the separate `question_generator.ipynb` notebook to create test questions, then load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Ready to start testing the Chatbot Agent.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports (this helps with relative imports)\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.chatbot_agent import ChatbotAgentNode\n",
    "from src.core.config.agent_config_manager import AgentConfigManager\n",
    "from src.integrations.llm_providers import LLMProviderFactory\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start testing the Chatbot Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration Settings\n",
    "\n",
    "The following cell loads the current configuration for the Chatbot Agent.\n",
    "You can edit these values to customize the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration files loaded and temporary copies created!\n",
      "Agent name: chatbot_agent\n",
      "Agent version: 1.0.0\n",
      "Preferred model: anthropic_general_budget\n",
      "\n",
      "üíæ Temporary config files created at:\n",
      "  agent_config: /tmp/chatbot_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/chatbot_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/chatbot_agent_configs/models.yaml\n",
      "\n",
      "üí° These temp files can be edited directly and will be used in Step 5 if modified.\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from files and create temporary editable copies\n",
    "config_base_path = Path('/workspace/config')\n",
    "agent_config_path = config_base_path / 'agents' / 'chatbot_agent'\n",
    "temp_config_dir = Path('/tmp/chatbot_agent_configs')\n",
    "\n",
    "def load_and_create_temp_configs():\n",
    "    \"\"\"Load all configuration files and create temporary editable copies\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Create temp directory\n",
    "    temp_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load and copy agent config\n",
    "    with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "        configs['agent'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load and copy prompts\n",
    "    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "        configs['prompts'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load and copy models\n",
    "    with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "        configs['models'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load shared models for reference\n",
    "    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n",
    "        configs['shared_models'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Save temporary copies for editing\n",
    "    temp_agent_path = temp_config_dir / 'config.yaml'\n",
    "    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n",
    "    temp_models_path = temp_config_dir / 'models.yaml'\n",
    "    \n",
    "    with open(temp_agent_path, 'w') as f:\n",
    "        yaml.dump(configs['agent'], f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    with open(temp_prompts_path, 'w') as f:\n",
    "        yaml.dump(configs['prompts'], f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    with open(temp_models_path, 'w') as f:\n",
    "        yaml.dump(configs['models'], f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    return configs, {\n",
    "        'agent_config': temp_agent_path,\n",
    "        'prompts_config': temp_prompts_path,\n",
    "        'models_config': temp_models_path\n",
    "    }\n",
    "\n",
    "# Load configurations and create temp files\n",
    "configs, temp_file_paths = load_and_create_temp_configs()\n",
    "\n",
    "print(\"üìÅ Configuration files loaded and temporary copies created!\")\n",
    "print(f\"Agent name: {configs['agent']['agent']['name']}\")\n",
    "print(f\"Agent version: {configs['agent']['agent']['version']}\")\n",
    "print(f\"Preferred model: {configs['agent']['models']['preferred']}\")\n",
    "print(f\"\\nüíæ Temporary config files created at:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "print(f\"\\nüí° These temp files can be edited directly and will be used in Step 5 if modified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Editable Configuration Settings\n",
    "\n",
    "Edit these settings to customize how the Chatbot Agent behaves.\n",
    "These variables map directly to the configuration files and can be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration File Editor\n",
      "Edit the YAML configuration files below and use the Save buttons to apply changes.\n",
      "Changes are saved to temporary files and will be used in Step 5.\n",
      "\n",
      "üìÑ 1. Agent Configuration (config.yaml)\n",
      "Contains: agent settings, behavior, escalation thresholds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d558ea1127c46d4acf20cc5c90911a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"agent:\\n  name: chatbot_agent\\n  version: 1.0.0\\n  description: Primary response generation ag‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b82e0943fb74f6d9b77a0f5b9dcc89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Agent Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 2. Prompts Configuration (prompts.yaml)\n",
      "Contains: system prompts, response guidelines, communication style\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764c565c1ccb4810a6a7977042bab86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"system: 'You are a professional customer service chatbot dedicated to providing exceptional\\n ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f83c3b603fc4ea5b6b471e4ed1919b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Prompts Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 3. Models Configuration (models.yaml)\n",
      "Contains: preferred model, fallback models, model-specific settings\n",
      "\n",
      "üîç Available Model Aliases:\n",
      "Use these aliases in your models configuration below:\n",
      "\n",
      "üì° ANTHROPIC Provider:\n",
      "  ‚Ä¢ anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 - Anthropic Claude 3.5 Haiku - fast and efficient\n",
      "  ‚Ä¢ anthropic_general_standard ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_reasoning_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_coding_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_flagship ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "\n",
      "üì° OPENAI Provider:\n",
      "  ‚Ä¢ openai_general_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "  ‚Ä¢ openai_general_budget ‚Üí gpt-3.5-turbo - OpenAI GPT-3.5 Turbo - fast and cost-effective\n",
      "  ‚Ä¢ openai_coding_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "\n",
      "üì° LLAMA Provider:\n",
      "  ‚Ä¢ local_general_standard ‚Üí llama-7b - Llama 7B model - good balance of speed and quality\n",
      "  ‚Ä¢ local_general_premium ‚Üí llama-13b - Llama 13B model - higher quality, slower inference\n",
      "  ‚Ä¢ local_coding_standard ‚Üí codellama-7b - CodeLlama 7B - optimized for code generation\n",
      "\n",
      "üì° MISTRAL Provider:\n",
      "  ‚Ä¢ local_general_budget ‚Üí mistral-7b - Mistral 7B Instruct - excellent instruction following\n",
      "\n",
      "üìã Current Models Configuration:\n",
      "  Preferred: unknown\n",
      "  Fallback: []\n",
      "\n",
      "üí° Tips for editing:\n",
      "  ‚Ä¢ Change 'preferred' to any alias from the list above\n",
      "  ‚Ä¢ Add/remove aliases in the 'fallback' list\n",
      "  ‚Ä¢ Aliases are case-sensitive\n",
      "  ‚Ä¢ Invalid aliases will cause errors during processing\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08d140705994d89a945288bbb0103dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='primary_model: local_general_standard\\nmodel_preferences:\\n  general_queries:\\n    primary: an‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e9e23930ed417c8ee7b7a0370e6b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Models Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Save All Changes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425932e72910482ca5845dd5876a2c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Save All Configs', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Temp config files location:\n",
      "  agent_config: /tmp/chatbot_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/chatbot_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/chatbot_agent_configs/models.yaml\n",
      "\n",
      "üí° Tips:\n",
      "  ‚Ä¢ Edit YAML directly in the text areas above\n",
      "  ‚Ä¢ Use Save buttons to apply changes to temp files\n",
      "  ‚Ä¢ YAML syntax is validated before saving\n",
      "  ‚Ä¢ Model aliases are validated against available models\n",
      "  ‚Ä¢ Changes will be used in Step 5 when processing questions\n",
      "  ‚Ä¢ Original config files remain unchanged\n"
     ]
    }
   ],
   "source": [
    "# Display and edit configuration files in separate windows\n",
    "\n",
    "def load_config_file_contents():\n",
    "    \"\"\"Load current config file contents from temp files\"\"\"\n",
    "    with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "        agent_config_content = f.read()\n",
    "    with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "        prompts_config_content = f.read()\n",
    "    with open(temp_file_paths['models_config'], 'r') as f:\n",
    "        models_config_content = f.read()\n",
    "    \n",
    "    return agent_config_content, prompts_config_content, models_config_content\n",
    "\n",
    "# Load current config file contents\n",
    "agent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration File Editor\")\n",
    "print(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\n",
    "print(\"Changes are saved to temporary files and will be used in Step 5.\\n\")\n",
    "\n",
    "# Create text areas for each config file\n",
    "print(\"üìÑ 1. Agent Configuration (config.yaml)\")\n",
    "print(\"Contains: agent settings, behavior, escalation thresholds\")\n",
    "\n",
    "agent_config_editor = widgets.Textarea(\n",
    "    value=agent_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_agent_config(button):\n",
    "    \"\"\"Save agent config changes\"\"\"\n",
    "    try:\n",
    "        # Validate YAML syntax\n",
    "        yaml.safe_load(agent_config_editor.value)\n",
    "        \n",
    "        # Save to temp file\n",
    "        with open(temp_file_paths['agent_config'], 'w') as f:\n",
    "            f.write(agent_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Agent config saved successfully!\")\n",
    "        \n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving agent config: {e}\")\n",
    "\n",
    "agent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\n",
    "agent_save_btn.on_click(save_agent_config)\n",
    "\n",
    "display(agent_config_editor)\n",
    "display(agent_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 2. Prompts Configuration (prompts.yaml)\")\n",
    "print(\"Contains: system prompts, response guidelines, communication style\")\n",
    "\n",
    "prompts_config_editor = widgets.Textarea(\n",
    "    value=prompts_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_prompts_config(button):\n",
    "    \"\"\"Save prompts config changes\"\"\"\n",
    "    try:\n",
    "        # Validate YAML syntax\n",
    "        yaml.safe_load(prompts_config_editor.value)\n",
    "        \n",
    "        # Save to temp file\n",
    "        with open(temp_file_paths['prompts_config'], 'w') as f:\n",
    "            f.write(prompts_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Prompts config saved successfully!\")\n",
    "        \n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving prompts config: {e}\")\n",
    "\n",
    "prompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\n",
    "prompts_save_btn.on_click(save_prompts_config)\n",
    "\n",
    "display(prompts_config_editor)\n",
    "display(prompts_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 3. Models Configuration (models.yaml)\")\n",
    "print(\"Contains: preferred model, fallback models, model-specific settings\")\n",
    "\n",
    "# Show available model aliases from shared models config\n",
    "def display_available_models():\n",
    "    \"\"\"Display available model aliases and their actual models\"\"\"\n",
    "    try:\n",
    "        shared_models = configs['shared_models']\n",
    "        \n",
    "        # Extract model aliases and models sections\n",
    "        model_aliases = shared_models.get('model_aliases', {})\n",
    "        models = shared_models.get('models', {})\n",
    "        \n",
    "        if not model_aliases:\n",
    "            print(\"‚ùå No model aliases found in shared configuration\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nüîç Available Model Aliases:\")\n",
    "        print(\"Use these aliases in your models configuration below:\\n\")\n",
    "        \n",
    "        # Group by provider for better organization\n",
    "        providers = {}\n",
    "        for alias, actual_model_name in model_aliases.items():\n",
    "            # Get model details from models section\n",
    "            model_details = models.get(actual_model_name, {})\n",
    "            provider = model_details.get('type', 'unknown')\n",
    "            description = model_details.get('description', '')\n",
    "            \n",
    "            if provider not in providers:\n",
    "                providers[provider] = []\n",
    "            providers[provider].append({\n",
    "                'alias': alias,\n",
    "                'model_name': actual_model_name,\n",
    "                'description': description\n",
    "            })\n",
    "        \n",
    "        # Display by provider\n",
    "        for provider, provider_models in providers.items():\n",
    "            print(f\"üì° {provider.upper()} Provider:\")\n",
    "            for model in provider_models:\n",
    "                desc = f\" - {model['description']}\" if model['description'] else \"\"\n",
    "                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n",
    "            print()\n",
    "        \n",
    "        # Show current configuration\n",
    "        try:\n",
    "            current_models_config = yaml.safe_load(models_config_content)\n",
    "            current_preferred = current_models_config.get('preferred', 'unknown')\n",
    "            current_fallback = current_models_config.get('fallback', [])\n",
    "        except:\n",
    "            current_preferred = 'unknown'\n",
    "            current_fallback = []\n",
    "        \n",
    "        print(f\"üìã Current Models Configuration:\")\n",
    "        print(f\"  Preferred: {current_preferred}\")\n",
    "        print(f\"  Fallback: {current_fallback}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üí° Tips for editing:\")\n",
    "        print(\"  ‚Ä¢ Change 'preferred' to any alias from the list above\")\n",
    "        print(\"  ‚Ä¢ Add/remove aliases in the 'fallback' list\")\n",
    "        print(\"  ‚Ä¢ Aliases are case-sensitive\")\n",
    "        print(\"  ‚Ä¢ Invalid aliases will cause errors during processing\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading available models: {e}\")\n",
    "        print(\"Continuing with models configuration editor...\\n\")\n",
    "\n",
    "# Display available models before showing the editor\n",
    "display_available_models()\n",
    "\n",
    "models_config_editor = widgets.Textarea(\n",
    "    value=models_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='200px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_models_config(button):\n",
    "    \"\"\"Save models config changes\"\"\"\n",
    "    try:\n",
    "        # Validate YAML syntax\n",
    "        parsed_config = yaml.safe_load(models_config_editor.value)\n",
    "        \n",
    "        # Additional validation for model aliases\n",
    "        if isinstance(parsed_config, dict):\n",
    "            preferred = parsed_config.get('preferred')\n",
    "            fallback = parsed_config.get('fallback', [])\n",
    "            \n",
    "            # Get available aliases\n",
    "            model_aliases = configs['shared_models'].get('model_aliases', {})\n",
    "            \n",
    "            # Check if preferred model exists\n",
    "            if preferred and preferred not in model_aliases:\n",
    "                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n",
    "            \n",
    "            # Check fallback models\n",
    "            for fb_model in fallback:\n",
    "                if fb_model not in model_aliases:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Fallback model '{fb_model}' not found in available model aliases\")\n",
    "        \n",
    "        # Save to temp file\n",
    "        with open(temp_file_paths['models_config'], 'w') as f:\n",
    "            f.write(models_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Models config saved successfully!\")\n",
    "        \n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"‚ùå YAML syntax error in models config: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving models config: {e}\")\n",
    "\n",
    "models_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\n",
    "models_save_btn.on_click(save_models_config)\n",
    "\n",
    "display(models_config_editor)\n",
    "display(models_save_btn)\n",
    "\n",
    "# Save All button for convenience\n",
    "def save_all_configs(button):\n",
    "    \"\"\"Save all config changes at once\"\"\"\n",
    "    save_agent_config(None)\n",
    "    save_prompts_config(None)\n",
    "    save_models_config(None)\n",
    "\n",
    "print(\"\\nüíæ Save All Changes\")\n",
    "save_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\n",
    "save_all_btn.on_click(save_all_configs)\n",
    "display(save_all_btn)\n",
    "\n",
    "print(f\"\\nüíæ Temp config files location:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"  ‚Ä¢ Edit YAML directly in the text areas above\")\n",
    "print(\"  ‚Ä¢ Use Save buttons to apply changes to temp files\")\n",
    "print(\"  ‚Ä¢ YAML syntax is validated before saving\")\n",
    "print(\"  ‚Ä¢ Model aliases are validated against available models\")\n",
    "print(\"  ‚Ä¢ Changes will be used in Step 5 when processing questions\")\n",
    "print(\"  ‚Ä¢ Original config files remain unchanged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Questions\n",
    "\n",
    "Load test questions from a JSON file. Use question_generator.ipynb to create new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Load Test Questions\n",
      "\n",
      "üí° How to get test questions:\n",
      "1. Use question_generator.ipynb to create test questions\n",
      "2. Upload the generated JSON file below\n",
      "3. Supported formats: Full export or questions-only JSON\n",
      "\n",
      "üìÑ Expected JSON format:\n",
      "- Full export: {'metadata': {...}, 'questions': [...]}\n",
      "- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\n",
      "\n",
      "üìÅ Upload your questions file:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f3d4cdeb7844ac8d7fc65196c978cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.json', description='Upload questions file:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File upload widget for loading questions\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.json',\n",
    "    multiple=False,\n",
    "    description='Upload questions file:'\n",
    ")\n",
    "\n",
    "# Instructions for file format\n",
    "print(\"üìù Load Test Questions\")\n",
    "print(\"\\nüí° How to get test questions:\")\n",
    "print(\"1. Use question_generator.ipynb to create test questions\")\n",
    "print(\"2. Upload the generated JSON file below\")\n",
    "print(\"3. Supported formats: Full export or questions-only JSON\")\n",
    "print(\"\\nüìÑ Expected JSON format:\")\n",
    "print(\"- Full export: {'metadata': {...}, 'questions': [...]}\")\n",
    "print(\"- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\")\n",
    "print(\"\\nüìÅ Upload your questions file:\")\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Using tuple format\n",
      "üîç File content type: <class 'memoryview'>\n",
      "üìÅ Loading questions from: questions_only_20250718_1744.json\n",
      "‚úÖ Loaded 20 test questions\n",
      "\n",
      "üìã Preview of loaded questions:\n",
      "  1. Why did my premium increase by $200? This is ridiculous - I haven't had any claims! [frustrated]\n",
      "  2. I just had a car accident. What do I need to do right now? [urgent]\n",
      "  3. Can someone explain what a deductible is? I keep seeing this word but don't understand. [confused]\n",
      "  ... and 17 more questions\n",
      "\n",
      "üìä Question Distribution:\n",
      "  Customer types: {'frustrated': np.int64(5), 'urgent': np.int64(5), 'confused': np.int64(5), 'normal': np.int64(5)}\n",
      "  Complexities: {'medium': np.int64(7), 'simple': np.int64(7), 'complex': np.int64(6)}\n"
     ]
    }
   ],
   "source": [
    "# Load test questions from uploaded file\n",
    "test_questions = []\n",
    "\n",
    "def load_questions_from_file(file_content, filename):\n",
    "    \"\"\"Load questions from uploaded JSON file\"\"\"\n",
    "    try:\n",
    "        # Handle different content types\n",
    "        if isinstance(file_content, memoryview):\n",
    "            # Convert memoryview to bytes\n",
    "            content_bytes = file_content.tobytes()\n",
    "        elif hasattr(file_content, 'decode'):\n",
    "            # Already bytes\n",
    "            content_bytes = file_content\n",
    "        else:\n",
    "            # Convert to bytes if it's a string or other type\n",
    "            content_bytes = str(file_content).encode('utf-8')\n",
    "        \n",
    "        # Decode to string and parse JSON\n",
    "        data = json.loads(content_bytes.decode('utf-8'))\n",
    "        \n",
    "        # Handle different JSON formats\n",
    "        if isinstance(data, dict):\n",
    "            # Full export format with metadata\n",
    "            if 'questions' in data:\n",
    "                questions = data['questions']\n",
    "                metadata = data.get('metadata', {})\n",
    "                print(f\"üìÑ Loaded file with metadata:\")\n",
    "                print(f\"  Generation model: {metadata.get('generation_model', 'unknown')}\")\n",
    "                print(f\"  Generation timestamp: {metadata.get('generation_timestamp', 'unknown')}\")\n",
    "                print(f\"  Question count: {metadata.get('question_count', len(questions))}\")\n",
    "                return questions\n",
    "            else:\n",
    "                # Single question object\n",
    "                return [data]\n",
    "        elif isinstance(data, list):\n",
    "            # Questions-only format\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected data format: {type(data)}\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        print(f\"   File content type: {type(file_content)}\")\n",
    "        return []\n",
    "\n",
    "def validate_questions(questions):\n",
    "    \"\"\"Validate and normalize question format\"\"\"\n",
    "    validated_questions = []\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        if isinstance(q, dict):\n",
    "            # Ensure required fields exist\n",
    "            validated_q = {\n",
    "                \"id\": q.get(\"id\", i + 1),\n",
    "                \"question\": q.get(\"question\", f\"Question {i + 1}\"),\n",
    "                \"customer_type\": q.get(\"customer_type\", \"normal\"),\n",
    "                \"complexity\": q.get(\"complexity\", \"medium\")\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "        else:\n",
    "            # Convert string to dict if needed\n",
    "            validated_q = {\n",
    "                \"id\": i + 1,\n",
    "                \"question\": str(q),\n",
    "                \"customer_type\": \"normal\",\n",
    "                \"complexity\": \"medium\"\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "    \n",
    "    return validated_questions\n",
    "\n",
    "# Process uploaded file\n",
    "if file_upload.value:\n",
    "    uploaded_file = None\n",
    "    filename = None\n",
    "    file_content = None\n",
    "    \n",
    "    # Handle different file upload widget formats\n",
    "    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using tuple format\")\n",
    "        uploaded_file = file_upload.value[0]\n",
    "        filename = uploaded_file['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using dict format\")\n",
    "        uploaded_file = list(file_upload.value.values())[0]\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Unable to read uploaded file format\")\n",
    "        print(f\"   Type: {type(file_upload.value)}\")\n",
    "        print(f\"   Length: {len(file_upload.value) if hasattr(file_upload.value, '__len__') else 'No length'}\")\n",
    "        print(f\"   Content: {file_upload.value}\")\n",
    "    \n",
    "    if uploaded_file and filename and file_content is not None:\n",
    "        print(f\"üìÅ Loading questions from: {filename}\")\n",
    "        \n",
    "        raw_questions = load_questions_from_file(file_content, filename)\n",
    "        \n",
    "        if raw_questions:\n",
    "            test_questions = validate_questions(raw_questions)\n",
    "            print(f\"‚úÖ Loaded {len(test_questions)} test questions\")\n",
    "            \n",
    "            # Display first few questions as preview\n",
    "            print(\"\\nüìã Preview of loaded questions:\")\n",
    "            for i, q in enumerate(test_questions[:3]):\n",
    "                print(f\"  {i+1}. {q['question']} [{q['customer_type']}]\")\n",
    "            if len(test_questions) > 3:\n",
    "                print(f\"  ... and {len(test_questions) - 3} more questions\")\n",
    "                \n",
    "            # Show distribution\n",
    "            df_preview = pd.DataFrame(test_questions)\n",
    "            print(\"\\nüìä Question Distribution:\")\n",
    "            print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n",
    "            print(f\"  Complexities: {dict(df_preview['complexity'].value_counts())}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå No questions loaded from file\")\n",
    "    else:\n",
    "        print(\"‚ùå Error accessing uploaded file\")\n",
    "        print(f\"   uploaded_file: {uploaded_file is not None}\")\n",
    "        print(f\"   filename: {filename}\")\n",
    "        print(f\"   file_content: {file_content is not None}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please upload a JSON file with test questions.\")\n",
    "    print(\"üí° Use question_generator.ipynb to create test questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review and Edit Questions\n",
    "\n",
    "Review the loaded questions and make any edits before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Question Editor - You can modify questions before processing\n",
      "Edit the questions in the table below, then run the next cell to process them.\n",
      "\n",
      "Current questions (you can edit the JSON below if needed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Why did my premium increase by $200? This is r...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I just had a car accident. What do I need to d...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Can someone explain what a deductible is? I ke...</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How do I add my teenage daughter to my auto po...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been a customer for 15 years and you deni...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What's the difference between comprehensive an...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>My house just flooded - I need emergency assis...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Can I get proof of insurance sent to my phone?...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How do I update my billing information?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>What exactly is an 'act of God' and why isn't ...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>I've been on hold for 45 minutes and keep gett...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>My neighbor's tree fell on my garage during th...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Can I schedule a review of my current coverage...</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Someone broke into my car last night. I need t...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Why am I paying more than my friend for the sa...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>What documents do I need to submit for my wate...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>I don't understand all these terms in my polic...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>My windshield cracked on the highway. Is this ...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>I've been trying to file a claim online for ho...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>When does my policy expire and how do I renew it?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question customer_type  \\\n",
       "0    1  Why did my premium increase by $200? This is r...    frustrated   \n",
       "1    2  I just had a car accident. What do I need to d...        urgent   \n",
       "2    3  Can someone explain what a deductible is? I ke...      confused   \n",
       "3    4  How do I add my teenage daughter to my auto po...        normal   \n",
       "4    5  I've been a customer for 15 years and you deni...    frustrated   \n",
       "5    6  What's the difference between comprehensive an...      confused   \n",
       "6    7  My house just flooded - I need emergency assis...        urgent   \n",
       "7    8  Can I get proof of insurance sent to my phone?...        urgent   \n",
       "8    9            How do I update my billing information?        normal   \n",
       "9   10  What exactly is an 'act of God' and why isn't ...      confused   \n",
       "10  11  I've been on hold for 45 minutes and keep gett...    frustrated   \n",
       "11  12  My neighbor's tree fell on my garage during th...      confused   \n",
       "12  13  Can I schedule a review of my current coverage...        normal   \n",
       "13  14  Someone broke into my car last night. I need t...        urgent   \n",
       "14  15  Why am I paying more than my friend for the sa...    frustrated   \n",
       "15  16  What documents do I need to submit for my wate...        normal   \n",
       "16  17  I don't understand all these terms in my polic...      confused   \n",
       "17  18  My windshield cracked on the highway. Is this ...        urgent   \n",
       "18  19  I've been trying to file a claim online for ho...    frustrated   \n",
       "19  20  When does my policy expire and how do I renew it?        normal   \n",
       "\n",
       "   complexity  \n",
       "0      medium  \n",
       "1      simple  \n",
       "2      simple  \n",
       "3      simple  \n",
       "4     complex  \n",
       "5      medium  \n",
       "6     complex  \n",
       "7      simple  \n",
       "8      simple  \n",
       "9      medium  \n",
       "10     medium  \n",
       "11    complex  \n",
       "12     medium  \n",
       "13    complex  \n",
       "14     medium  \n",
       "15     simple  \n",
       "16    complex  \n",
       "17     medium  \n",
       "18    complex  \n",
       "19     simple  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced: Edit questions as JSON (optional):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a394f709e00841f7bd30a4472c108f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='[\\n  {\\n    \"id\": 1,\\n    \"question\": \"Why did my premium increase by $200? This is ridiculous‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5fb09e93f948b2b2dfe312307259ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update from JSON', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive question editor\n",
    "if test_questions:\n",
    "    print(\"üìù Question Editor - You can modify questions before processing\")\n",
    "    print(\"Edit the questions in the table below, then run the next cell to process them.\\n\")\n",
    "    \n",
    "    # Convert to DataFrame for easy editing\n",
    "    df = pd.DataFrame(test_questions)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "    if 'customer_type' not in df.columns:\n",
    "        df['customer_type'] = 'normal'\n",
    "    if 'complexity' not in df.columns:\n",
    "        df['complexity'] = 'medium'\n",
    "    \n",
    "    # Display editable table\n",
    "    print(\"Current questions (you can edit the JSON below if needed):\")\n",
    "    display(df)\n",
    "    \n",
    "    # Show JSON for manual editing if needed\n",
    "    questions_json = widgets.Textarea(\n",
    "        value=json.dumps(test_questions, indent=2),\n",
    "        description=\"Questions JSON:\",\n",
    "        layout=widgets.Layout(width='100%', height='200px')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAdvanced: Edit questions as JSON (optional):\")\n",
    "    display(questions_json)\n",
    "    \n",
    "    def update_questions_from_json():\n",
    "        \"\"\"Update questions from the JSON editor\"\"\"\n",
    "        global test_questions\n",
    "        try:\n",
    "            test_questions = json.loads(questions_json.value)\n",
    "            print(\"‚úÖ Questions updated from JSON editor\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing JSON: {e}\")\n",
    "    \n",
    "    # Button to update from JSON\n",
    "    update_btn = widgets.Button(description=\"Update from JSON\")\n",
    "    update_btn.on_click(lambda b: update_questions_from_json())\n",
    "    display(update_btn)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please upload a questions file in the previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Questions Through Chatbot Agent\n",
    "\n",
    "Run the questions through the Chatbot Agent and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process questions through the Chatbot Agent using temp configs if available\nif not test_questions:\n    print(\"‚ö†Ô∏è No questions to process. Please load or generate questions first.\")\nelse:\n    print(f\"ü§ñ Processing {len(test_questions)} questions through Chatbot Agent...\")\n    \n    # Check if temp config files exist and load them\n    def load_config_for_processing():\n        \"\"\"Load config from temp files if they exist, otherwise use base configs\"\"\"\n        if all(path.exists() for path in temp_file_paths.values()):\n            print(\"üìÅ Using temporary config files with your edits...\")\n            \n            # Load from temp files\n            with open(temp_file_paths['agent_config'], 'r') as f:\n                agent_config = yaml.safe_load(f)\n            with open(temp_file_paths['prompts_config'], 'r') as f:\n                prompts_config = yaml.safe_load(f)\n            with open(temp_file_paths['models_config'], 'r') as f:\n                models_config = yaml.safe_load(f)\n            \n            return agent_config, prompts_config, models_config, True\n        else:\n            print(\"üìÅ Using base config files...\")\n            return configs['agent'], configs['prompts'], configs['models'], False\n    \n    # Load configuration for processing\n    current_agent_config, current_prompts_config, current_models_config, using_temp_files = load_config_for_processing()\n    \n    # Extract current values - FIXED: Use standardized model config structure\n    if 'primary_model' in current_models_config:\n        current_preferred_model = current_models_config['primary_model']\n    elif 'preferred' in current_models_config:\n        current_preferred_model = current_models_config['preferred']\n    elif 'model_preferences' in current_models_config and current_models_config['model_preferences']:\n        # Get first task preferences as fallback\n        first_task = next(iter(current_models_config['model_preferences'].values()), {})\n        current_preferred_model = first_task.get('primary', 'anthropic_general_budget')\n    else:\n        current_preferred_model = 'anthropic_general_budget'  # fallback\n    \n    current_temperature = current_agent_config['settings']['temperature']\n    \n    print(f\"Using model: {current_preferred_model}\")\n    print(f\"Temperature: {current_temperature}\")\n    print(f\"Config source: {'Temporary files (with edits)' if using_temp_files else 'Base config files'}\")\n    print(\"\\n\" + \"=\"*50)\n    \n    def analyze_chatbot_response(response: str, confidence: float) -> dict:\n        \"\"\"\n        Analyze chatbot response to determine escalation and input needs\n        \"\"\"\n        response_lower = response.lower()\n        \n        # Escalation indicators - phrases that suggest referring to human/department\n        escalation_phrases = [\n            \"let me transfer you to\",\n            \"i'll connect you with\",\n            \"please contact our\",\n            \"speak with a specialist\",\n            \"escalate this to\",\n            \"connect you with the\",\n            \"transfer to our\",\n            \"speak with someone from\",\n            \"you'll need to contact\",\n            \"reach out to our\",\n            \"i recommend speaking with\",\n            \"forward this to\"\n        ]\n        \n        # More input indicators - phrases requesting additional information\n        input_request_phrases = [\n            \"could you provide\",\n            \"can you tell me more about\",\n            \"what is your\",\n            \"to better assist you\",\n            \"i need more information\",\n            \"could you clarify\",\n            \"can you specify\",\n            \"what type of\",\n            \"please let me know\",\n            \"can you share\",\n            \"what specific\",\n            \"which policy\"\n                    ]\n        \n        # Question indicators (asking user for info to continue conversation)\n        question_patterns = [\n            \"could you\",\n            \"can you\",\n            \"would you mind\",\n            \"please provide\",\n            \"do you have\",\n            \"what is\",\n            \"which\",\n            \"when did\",\n            \"?\"\n        ]\n        \n        # Check for escalation intent\n        escalation_detected = any(phrase in response_lower for phrase in escalation_phrases)\n        \n        # Check for information requests\n        input_needed = any(phrase in response_lower for phrase in input_request_phrases)\n        \n        # Check if response is primarily asking questions to continue conversation\n        question_count = response.count('?')\n        is_asking_questions = question_count > 0 and any(pattern in response_lower for pattern in question_patterns)\n        \n        # Confidence-based escalation (very low confidence = likely needs human help)\n        low_confidence_escalation = confidence < 0.6\n        \n        # Final determination with priority logic\n        needs_escalation = escalation_detected or low_confidence_escalation\n        # Only flag needs_more_input if NOT escalating (escalation takes priority)\n        needs_more_input = (input_needed or is_asking_questions) and not needs_escalation\n        \n        return {\n            'needs_escalation': needs_escalation,\n            'needs_more_input': needs_more_input,\n            'escalation_reason': 'explicit_transfer' if escalation_detected else 'low_confidence' if low_confidence_escalation else None,\n            'input_reason': 'information_request' if input_needed else 'clarifying_questions' if is_asking_questions else None,\n            'question_count': question_count,\n            'confidence_score': confidence,\n            'analysis_details': {\n                'escalation_detected': escalation_detected,\n                'input_needed': input_needed,\n                'is_asking_questions': is_asking_questions,\n                'low_confidence': low_confidence_escalation\n            }\n        }\n    \n    # Create a simple mock context provider for testing\n    class MockContextProvider:\n        \"\"\"Simple mock context provider for testing\"\"\"\n        \n        def get_context_summary(self, user_id: str, session_id: str) -> dict:\n            \"\"\"Return a simple mock context summary\"\"\"\n            return {\n                'entries_count': 0,\n                'type_breakdown': {},\n                'recent_queries': [],\n                'escalation_count': 0,\n                'last_activity': None\n            }\n        \n        def save_context_entry(self, entry) -> bool:\n            \"\"\"Mock save method\"\"\"\n            return True\n        \n        def get_recent_context(self, user_id: str, session_id: str, limit: int = 10) -> list:\n            \"\"\"Mock recent context method\"\"\"\n            return []\n    \n    # Initialize the Chatbot Agent\n    try:\n        # Create config manager with the correct config directory\n        config_manager = AgentConfigManager(config_dir='/workspace/config')\n        \n        # Override config if using temp files - FIXED: Use correct attributes\n        if using_temp_files:\n            # Create a temporary AgentConfig object with our custom configs\n            from src.core.config.agent_config_manager import AgentConfig\n            \n            # Create custom agent config object\n            agent_section = current_agent_config.get('agent', {})\n            custom_agent_config = AgentConfig(\n                name=agent_section.get('name', 'chatbot_agent'),\n                description=agent_section.get('description', ''),\n                type=agent_section.get('type', 'agent'),\n                version=agent_section.get('version', '1.0.0'),\n                models=current_models_config,\n                settings=current_agent_config.get('settings', {}),\n                prompts=current_prompts_config,\n                behavior=current_agent_config.get('behavior', {}),\n                escalation=current_agent_config.get('escalation', {}),\n                evaluation=current_agent_config.get('evaluation', {}),\n                routing=current_agent_config.get('routing', {})\n            )\n            \n            # Override the agent config in the manager\n            config_manager._agents['chatbot_agent'] = custom_agent_config\n            print(\"‚úÖ Overrode config manager with temporary configs\")\n        \n        # Create a mock context provider for testing\n        context_provider = MockContextProvider()\n        \n        # Initialize Chatbot Agent\n        chatbot_agent = ChatbotAgentNode(config_manager, context_provider)\n        \n        print(\"‚úÖ Chatbot Agent initialized successfully\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error initializing Chatbot Agent: {e}\")\n        print(\"Continuing with mock responses for demonstration...\")\n        chatbot_agent = None\n    \n    # Process each question\n    results = []\n    \n    for i, question_data in enumerate(test_questions):\n        question_id = question_data.get('id', i + 1)\n        question_text = question_data.get('question', '')\n        customer_type = question_data.get('customer_type', 'normal')\n        complexity = question_data.get('complexity', 'medium')\n        \n        print(f\"\\nüîÑ Processing question {question_id}: {question_text[:60]}...\")\n        \n        try:\n            if chatbot_agent:\n                # Create state for the Chatbot Agent\n                from datetime import datetime\n                state = {\n                    'query': question_text,\n                    'user_id': 'test_user',\n                    'session_id': f'test_session_{i}',\n                    'query_id': f'query_{question_id}',\n                    'timestamp': datetime.now().isoformat(),\n                    'messages': []\n                }\n                \n                # Process through agent using __call__ method\n                response_state = chatbot_agent(state)\n                \n                answer = response_state.get('ai_response', 'No response generated')\n                confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n                \n                # Use improved flag analysis\n                flag_analysis = analyze_chatbot_response(answer, confidence)\n                needs_escalation = flag_analysis['needs_escalation']\n                needs_more_input = flag_analysis['needs_more_input']\n                \n            else:\n                # Mock processing for demonstration\n                import random\n                answer = f\"Thank you for your question about {question_text[:30]}... I'd be happy to help you with that. [This is a mock response for demonstration]\"\n                confidence = random.uniform(0.6, 0.95)\n                \n                # Use improved flag analysis even for mock responses\n                flag_analysis = analyze_chatbot_response(answer, confidence)\n                needs_escalation = flag_analysis['needs_escalation']\n                needs_more_input = flag_analysis['needs_more_input']\n            \n            # Create result entry with enhanced analysis\n            result = {\n                'id': question_id,\n                'original_question': question_text,\n                'customer_type': customer_type,\n                'complexity': complexity,\n                'ai_answer': answer,\n                'confidence_score': confidence,\n                'needs_escalation': needs_escalation,\n                'needs_more_input': needs_more_input,\n                'escalation_reason': flag_analysis.get('escalation_reason'),\n                'input_reason': flag_analysis.get('input_reason'),\n                'question_count': flag_analysis.get('question_count', 0),\n                'analysis_details': flag_analysis.get('analysis_details', {}),\n                'processing_time': datetime.now().isoformat(),\n                'model_used': current_preferred_model,\n                'temperature': current_temperature,\n                'config_source': 'temp_files' if using_temp_files else 'base_files'\n            }\n            \n            results.append(result)\n            \n            # Show progress with enhanced status\n            if needs_more_input:\n                reason = flag_analysis.get('input_reason', 'unknown')\n                status = f\"üîÑ Needs more input ({reason})\"\n            elif needs_escalation:\n                reason = flag_analysis.get('escalation_reason', 'unknown')\n                status = f\"‚ö†Ô∏è Escalation needed ({reason})\"\n            else:\n                status = \"‚úÖ Complete\"\n            \n            print(f\"   {status} (confidence: {confidence:.2f})\")\n            \n        except Exception as e:\n            print(f\"   ‚ùå Error processing question: {e}\")\n            # Add error result\n            result = {\n                'id': question_id,\n                'original_question': question_text,\n                'customer_type': customer_type,\n                'complexity': complexity,\n                'ai_answer': f\"Error processing question: {e}\",\n                'confidence_score': 0.0,\n                'needs_escalation': True,\n                'needs_more_input': False,\n                'escalation_reason': 'processing_error',\n                'input_reason': None,\n                'question_count': 0,\n                'analysis_details': {'error': True},\n                'processing_time': datetime.now().isoformat(),\n                'model_used': current_preferred_model,\n                'temperature': current_temperature,\n                'config_source': 'temp_files' if using_temp_files else 'base_files',\n                'error': str(e)\n            }\n            results.append(result)\n    \n    print(\"\\n\" + \"=\"*50)\n    print(f\"‚úÖ Processing complete! Processed {len(results)} questions.\")\n    \n    # Enhanced summary statistics\n    total_questions = len(results)\n    needs_more_input_count = sum(1 for r in results if r['needs_more_input'])\n    needs_escalation_count = sum(1 for r in results if r['needs_escalation'])\n    avg_confidence = sum(r['confidence_score'] for r in results) / total_questions if total_questions > 0 else 0\n    \n    # Breakdown by escalation reason\n    escalation_reasons = {}\n    input_reasons = {}\n    for r in results:\n        if r['needs_escalation'] and r['escalation_reason']:\n            escalation_reasons[r['escalation_reason']] = escalation_reasons.get(r['escalation_reason'], 0) + 1\n        if r['needs_more_input'] and r['input_reason']:\n            input_reasons[r['input_reason']] = input_reasons.get(r['input_reason'], 0) + 1\n    \n    print(f\"\\nüìä Enhanced Summary:\")\n    print(f\"  Total questions: {total_questions}\")\n    print(f\"  Config source: {'Temporary files (with edits)' if using_temp_files else 'Base config files'}\")\n    print(f\"  Need more input: {needs_more_input_count} ({needs_more_input_count/total_questions*100:.1f}%)\")\n    if input_reasons:\n        print(f\"    Reasons: {input_reasons}\")\n    print(f\"  Need escalation: {needs_escalation_count} ({needs_escalation_count/total_questions*100:.1f}%)\")\n    if escalation_reasons:\n        print(f\"    Reasons: {escalation_reasons}\")\n    print(f\"  Average confidence: {avg_confidence:.2f}\")\n    \n    # Store config source info for export\n    experiment_config_source = 'temp_files' if using_temp_files else 'base_files'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Complete Conversations with Customer AI\n",
    "\n",
    "Continue conversations between customer AI and chatbot AI until natural resolution or escalation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Starting full conversations for 20 questions...\n",
      "This will simulate realistic customer-chatbot interactions until resolution.\n",
      "\n",
      "üó£Ô∏è Starting full conversation for question 1\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:29:07.143 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:29:10.975 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:29:29.533 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:29:44.493 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:29:49.085 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 1: escalation_request (2 turns, satisfaction: 0.30)\n",
      "üó£Ô∏è Starting full conversation for question 2\n",
      "   Customer: urgent, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:29:49.313 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:29:52.886 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:30:07.887 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:30:20.175 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:30:36.038 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:30:48.756 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:31:10.681 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:31:20.371 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 4 turns\n",
      "Question 2: escalation_request (4 turns, satisfaction: 0.65)\n",
      "üó£Ô∏è Starting full conversation for question 3\n",
      "   Customer: confused, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:31:20.580 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:31:21.925 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:31:39.477 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 3: satisfied (1 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 4\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:31:39.692 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:31:41.241 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:31:54.587 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:32:10.874 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:32:26.113 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:32:43.137 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:32:50.008 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 4: escalation_request (3 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 5\n",
      "   Customer: frustrated, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:32:50.232 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:32:51.595 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:32:56.091 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:33:12.533 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 1 turns\n",
      "Question 5: escalation_request (1 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 6\n",
      "   Customer: confused, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:33:12.744 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:33:14.284 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:33:40.550 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:33:56.707 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:34:29.832 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:34:49.413 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:34:56.493 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 3 turns\n",
      "Question 6: satisfied (3 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 7\n",
      "   Customer: urgent, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:34:56.707 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:34:58.302 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:35:12.639 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:35:33.894 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:35:53.505 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:35:59.887 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:36:07.475 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 4 turns\n",
      "Question 7: escalation_request (4 turns, satisfaction: 0.50)\n",
      "üó£Ô∏è Starting full conversation for question 8\n",
      "   Customer: urgent, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:36:07.686 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:36:09.147 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:36:33.654 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:36:52.867 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:37:14.513 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:37:32.114 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:37:50.534 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:37:59.862 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 4 turns\n",
      "Question 8: escalation_request (4 turns, satisfaction: 0.65)\n",
      "üó£Ô∏è Starting full conversation for question 9\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:38:00.091 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:38:01.659 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:38:18.072 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:38:33.429 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 9: satisfied (1 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 10\n",
      "   Customer: confused, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:38:33.651 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:38:35.032 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:38:47.837 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:39:02.370 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:39:07.544 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:39:14.829 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:39:39.353 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:40:00.690 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:40:18.877 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 12:40:23.024 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 6: Processing...\n",
      "‚úÖ 12:40:24.278 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 6 turns\n",
      "Question 10: escalation_request (6 turns, satisfaction: 0.50)\n",
      "üó£Ô∏è Starting full conversation for question 11\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:40:24.509 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:40:26.244 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:40:41.784 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:40:56.263 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:41:11.717 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 11: escalation_request (2 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 12\n",
      "   Customer: confused, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:41:11.952 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:41:13.374 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:41:17.161 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:41:43.786 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:41:57.984 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:42:07.139 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:42:19.962 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 12:42:27.780 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 5 turns\n",
      "Question 12: satisfied (5 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 13\n",
      "   Customer: normal, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:42:27.994 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:42:29.334 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:42:42.258 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:43:01.937 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:43:24.651 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:43:38.676 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:43:57.590 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 3 turns\n",
      "Question 13: satisfied (3 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 14\n",
      "   Customer: urgent, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:43:57.834 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:43:59.216 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:44:36.118 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:44:58.325 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:45:05.183 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:45:13.328 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:45:48.243 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 12:46:01.627 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 4 turns\n",
      "Question 14: escalation_request (4 turns, satisfaction: 0.50)\n",
      "üó£Ô∏è Starting full conversation for question 15\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:46:01.870 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:46:03.513 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:46:13.502 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:46:37.412 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:46:52.507 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 15: escalation_request (2 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 16\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:46:52.731 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:46:54.263 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:47:21.306 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:47:34.307 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 16: satisfied (1 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 17\n",
      "   Customer: confused, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:47:34.541 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:47:38.297 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:47:49.727 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:47:59.625 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 2 turns\n",
      "Question 17: satisfied (2 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 18\n",
      "   Customer: urgent, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:47:59.861 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:48:01.386 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:48:25.900 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:48:52.637 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:49:01.031 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:49:10.262 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 12:49:17.121 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 18: escalation_request (3 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 19\n",
      "   Customer: frustrated, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:49:17.364 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:49:18.811 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:49:31.383 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:49:48.917 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 12:50:04.979 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 19: escalation_request (2 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 20\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:50:05.206 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:50:06.758 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 12:50:15.205 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 12:50:28.081 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 20: satisfied (1 turns, satisfaction: 0.70)\n",
      "\n",
      "‚úÖ All conversations completed!\n",
      "\n",
      "üìä Conversation Summary:\n",
      "  Completed conversations: 20/20\n",
      "  Customer satisfaction: 8 (40.0%)\n",
      "  Escalations: 12 (60.0%)\n",
      "  Average turns per conversation: 2.7\n",
      "  Average customer satisfaction: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Complete conversations with customer AI simulation\n",
    "if 'results' in locals() and results:\n",
    "    import random  # Add missing import\n",
    "    \n",
    "    def create_customer_ai_simulator(customer_type, complexity, model_name='anthropic_general_budget'):\n",
    "        \"\"\"Create a customer AI simulator based on customer profile\"\"\"\n",
    "        \n",
    "        # Define customer personas\n",
    "        customer_personas = {\n",
    "            'frustrated': {\n",
    "                'style': 'Impatient, demanding, may use caps or exclamation points. Wants quick resolution.',\n",
    "                'follow_up_tendency': 'high',\n",
    "                'satisfaction_threshold': 0.8,\n",
    "                'escalation_patience': 2  # Will demand escalation after 2 responses if not satisfied\n",
    "            },\n",
    "            'urgent': {\n",
    "                'style': 'Time-sensitive, focused on immediate action. Professional but hurried.',\n",
    "                'follow_up_tendency': 'high', \n",
    "                'satisfaction_threshold': 0.75,\n",
    "                'escalation_patience': 3\n",
    "            },\n",
    "            'confused': {\n",
    "                'style': 'Asks many clarifying questions, needs simple explanations. Polite but persistent.',\n",
    "                'follow_up_tendency': 'very_high',\n",
    "                'satisfaction_threshold': 0.7,\n",
    "                'escalation_patience': 4\n",
    "            },\n",
    "            'normal': {\n",
    "                'style': 'Professional, patient, reasonable expectations.',\n",
    "                'follow_up_tendency': 'medium',\n",
    "                'satisfaction_threshold': 0.65,\n",
    "                'escalation_patience': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Define complexity behaviors\n",
    "        complexity_behaviors = {\n",
    "            'simple': {\n",
    "                'satisfaction_boost': 0.1,  # Easier to satisfy\n",
    "                'max_turns': 3,\n",
    "                'question_depth': 'surface-level'\n",
    "            },\n",
    "            'medium': {\n",
    "                'satisfaction_boost': 0.0,\n",
    "                'max_turns': 5,\n",
    "                'question_depth': 'moderate detail'\n",
    "            },\n",
    "            'complex': {\n",
    "                'satisfaction_boost': -0.1,  # Harder to satisfy\n",
    "                'max_turns': 7,\n",
    "                'question_depth': 'detailed technical'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        persona = customer_personas.get(customer_type, customer_personas['normal'])\n",
    "        behavior = complexity_behaviors.get(complexity, complexity_behaviors['medium'])\n",
    "        \n",
    "        class CustomerAISimulator:\n",
    "            def __init__(self, customer_type, complexity):\n",
    "                self.customer_type = customer_type  # Store customer type as instance variable\n",
    "                self.complexity = complexity  # Store complexity as instance variable\n",
    "                self.persona = persona\n",
    "                self.behavior = behavior\n",
    "                self.turn_count = 0\n",
    "                self.satisfaction_level = 0.0\n",
    "                self.conversation_history = []\n",
    "                \n",
    "                # Create LLM for customer simulation\n",
    "                try:\n",
    "                    provider_factory = LLMProviderFactory()\n",
    "                    self.llm = provider_factory.create_provider(model_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create customer AI simulator LLM: {e}\")\n",
    "                    self.llm = None\n",
    "            \n",
    "            def generate_response(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate customer response to chatbot\"\"\"\n",
    "                self.turn_count += 1\n",
    "                \n",
    "                # Analyze satisfaction with chatbot response\n",
    "                satisfaction_score = self._analyze_satisfaction(chatbot_response)\n",
    "                self.satisfaction_level = satisfaction_score\n",
    "                \n",
    "                # Check if conversation should end\n",
    "                is_satisfied = satisfaction_score >= (self.persona['satisfaction_threshold'] + self.behavior['satisfaction_boost'])\n",
    "                is_escalating = self.turn_count >= self.persona['escalation_patience'] and satisfaction_score < 0.6\n",
    "                is_max_turns = self.turn_count >= self.behavior['max_turns']\n",
    "                \n",
    "                if is_satisfied:\n",
    "                    return self._generate_satisfaction_response()\n",
    "                elif is_escalating:\n",
    "                    return self._generate_escalation_request()\n",
    "                elif is_max_turns:\n",
    "                    return self._generate_final_response()\n",
    "                else:\n",
    "                    return self._generate_follow_up_question(chatbot_response, original_question)\n",
    "            \n",
    "            def _analyze_satisfaction(self, response):\n",
    "                \"\"\"Analyze how satisfied the customer would be with the response\"\"\"\n",
    "                response_lower = response.lower()\n",
    "                \n",
    "                # Positive indicators\n",
    "                positive_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['specifically', 'here\\'s how', 'i can help', 'let me explain']):\n",
    "                    positive_score += 0.2\n",
    "                if any(phrase in response_lower for phrase in ['immediately', 'right away', 'quickly']):\n",
    "                    positive_score += 0.15\n",
    "                if len(response) > 100:  # Detailed response\n",
    "                    positive_score += 0.1\n",
    "                \n",
    "                # Negative indicators  \n",
    "                negative_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['i need more information', 'could you provide', 'what type']):\n",
    "                    negative_score += 0.3  # Asking for more info is frustrating\n",
    "                if any(phrase in response_lower for phrase in ['unfortunately', 'however', 'but']):\n",
    "                    negative_score += 0.1\n",
    "                if response.count('?') > 2:  # Too many questions back\n",
    "                    negative_score += 0.2\n",
    "                \n",
    "                # Base satisfaction varies by customer type - use self.customer_type\n",
    "                base_satisfaction = {\n",
    "                    'frustrated': 0.3,\n",
    "                    'urgent': 0.4, \n",
    "                    'confused': 0.5,\n",
    "                    'normal': 0.6\n",
    "                }.get(self.customer_type, 0.5)\n",
    "                \n",
    "                return max(0.0, min(1.0, base_satisfaction + positive_score - negative_score))\n",
    "            \n",
    "            def _generate_satisfaction_response(self):\n",
    "                \"\"\"Generate a satisfied customer response\"\"\"\n",
    "                satisfied_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"Okay, that makes sense. Thanks for clearing that up.\",\n",
    "                        \"Finally! Thank you for the explanation.\",\n",
    "                        \"Alright, I understand now. That helps.\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"Perfect, that's exactly what I needed to know. Thank you!\",\n",
    "                        \"Great, I'll do that right away. Thanks for the quick help!\",\n",
    "                        \"Excellent, that answers my question. Much appreciated.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"Oh I see! That makes much more sense now. Thank you for explaining it so clearly.\",\n",
    "                        \"Thank you! That explanation really helped me understand.\",\n",
    "                        \"Perfect! Now I get it. I really appreciate your patience.\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"Thank you for the helpful information. That resolves my question.\",\n",
    "                        \"Great, that's exactly what I needed to know. Thanks!\",\n",
    "                        \"Perfect, I understand now. Thank you for your assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(satisfied_responses.get(self.customer_type, satisfied_responses['normal']))\n",
    "            \n",
    "            def _generate_escalation_request(self):\n",
    "                \"\"\"Generate escalation request\"\"\"\n",
    "                escalation_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"This isn't working. I need to speak to someone who can actually help me!\",\n",
    "                        \"I'm getting nowhere with this. Transfer me to a supervisor NOW!\",\n",
    "                        \"Enough! Get me a human who knows what they're doing!\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"I need this resolved immediately. Can you transfer me to someone who can handle this urgently?\",\n",
    "                        \"Time is critical here. I need to speak with a specialist right away.\",\n",
    "                        \"This is urgent - please connect me with someone who can resolve this now.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"I'm still really confused. Could you please connect me with someone who can walk me through this step by step?\",\n",
    "                        \"I don't think I'm understanding this correctly. Can I speak with someone who can explain this more simply?\",\n",
    "                        \"I'm getting more confused. Could you transfer me to someone who specializes in helping customers like me?\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"I think I need to speak with a specialist about this. Could you please transfer me?\",\n",
    "                        \"This seems like it might require human expertise. Can you connect me with the right department?\",\n",
    "                        \"I'd like to speak with someone who can provide more detailed assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(escalation_responses.get(self.customer_type, escalation_responses['normal']))\n",
    "            \n",
    "            def _generate_final_response(self):\n",
    "                \"\"\"Generate final response when max turns reached\"\"\"\n",
    "                final_responses = {\n",
    "                    'frustrated': \"Look, I've been going in circles here. Just transfer me to someone else.\",\n",
    "                    'urgent': \"I've spent too much time on this already. I need to speak with a human.\",\n",
    "                    'confused': \"I'm still not clear on this. I think I need to talk to someone in person.\",\n",
    "                    'normal': \"I think this might be beyond what we can resolve here. Could you transfer me to the appropriate department?\"\n",
    "                }\n",
    "                \n",
    "                return final_responses.get(self.customer_type, final_responses['normal'])\n",
    "            \n",
    "            def _generate_follow_up_question(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate intelligent follow-up question using AI if available\"\"\"\n",
    "                \n",
    "                if self.llm:\n",
    "                    # Use AI to generate contextual follow-up - FIX: Use correct method signature\n",
    "                    prompt = f'''You are a {self.customer_type} customer with a {self.complexity} question about insurance. \n",
    "                    \n",
    "Your personality: {self.persona['style']}\n",
    "Question complexity: {self.behavior['question_depth']}\n",
    "Turn {self.turn_count} of conversation.\n",
    "\n",
    "Original question: {original_question}\n",
    "Chatbot's response: {chatbot_response}\n",
    "\n",
    "Generate a follow-up response that a {self.customer_type} customer would realistically ask. Be specific to the chatbot's response and maintain the personality. Make it {self.behavior['question_depth']} in nature.\n",
    "\n",
    "Respond as the customer would (2-3 sentences max):'''\n",
    "                    \n",
    "                    try:\n",
    "                        # FIX: Use correct method signature (prompt, system_prompt)\n",
    "                        response = self.llm.generate_response(prompt)\n",
    "                        \n",
    "                        if response and len(response.strip()) > 10:\n",
    "                            return response.strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: AI follow-up generation failed: {e}\")\n",
    "                \n",
    "                # Fallback to template-based responses\n",
    "                return self._generate_template_follow_up(chatbot_response, original_question)\n",
    "            \n",
    "            def _generate_template_follow_up(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate template-based follow-up questions\"\"\"\n",
    "                \n",
    "                # Analyze what the chatbot asked for\n",
    "                response_lower = chatbot_response.lower()\n",
    "                \n",
    "                if 'policy number' in response_lower:\n",
    "                    return f\"My policy number is POL-{random.randint(100000, 999999)}. Now what?\"\n",
    "                elif 'claim number' in response_lower:\n",
    "                    return f\"It's claim #{random.randint(10000, 99999)}. What's the next step?\"\n",
    "                elif 'type of coverage' in response_lower or 'which coverage' in response_lower:\n",
    "                    coverage_types = ['comprehensive', 'collision', 'liability', 'homeowners', 'auto']\n",
    "                    return f\"I have {random.choice(coverage_types)} coverage. Does that help?\"\n",
    "                elif 'when did' in response_lower or 'what date' in response_lower:\n",
    "                    return \"This happened yesterday around 3 PM. What do I do now?\"\n",
    "                elif 'how much' in response_lower or 'what amount' in response_lower:\n",
    "                    return f\"It's about ${random.randint(500, 5000)} in damage. What's next?\"\n",
    "                elif 'documents' in response_lower or 'paperwork' in response_lower:\n",
    "                    return \"I have photos and a police report. How do I submit them?\"\n",
    "                else:\n",
    "                    # Generic follow-ups based on customer type\n",
    "                    generic_followups = {\n",
    "                        'frustrated': \"That doesn't really answer my question. Can you be more specific?\",\n",
    "                        'urgent': \"Okay, but what do I do RIGHT NOW? This is time-sensitive!\",\n",
    "                        'confused': \"I'm still not sure I understand. Can you explain it differently?\",\n",
    "                        'normal': \"Could you provide more specific steps on what I should do next?\"\n",
    "                    }\n",
    "                    return generic_followups.get(self.customer_type, generic_followups['normal'])\n",
    "        \n",
    "        return CustomerAISimulator(customer_type, complexity)  # Pass parameters to constructor\n",
    "    \n",
    "    def conduct_full_conversation(question_data, max_conversation_turns=8):\n",
    "        \"\"\"Conduct a full conversation between customer AI and chatbot until resolution\"\"\"\n",
    "        \n",
    "        question_id = question_data.get('id', 1)\n",
    "        question_text = question_data.get('question', '')\n",
    "        customer_type = question_data.get('customer_type', 'normal')\n",
    "        complexity = question_data.get('complexity', 'medium')\n",
    "        \n",
    "        # Get the model from questions file or use current preferred model\n",
    "        model_to_use = question_data.get('model', current_preferred_model)\n",
    "        \n",
    "        print(f\"üó£Ô∏è Starting full conversation for question {question_id}\")\n",
    "        print(f\"   Customer: {customer_type}, Complexity: {complexity}, Model: {model_to_use}\")\n",
    "        \n",
    "        # Create customer AI simulator\n",
    "        try:\n",
    "            customer_ai = create_customer_ai_simulator(customer_type, complexity, model_to_use)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error creating customer AI: {e}\")\n",
    "            return {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'error': f\"Failed to create customer AI: {e}\",\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "        \n",
    "        # Initialize conversation\n",
    "        conversation_history = []\n",
    "        current_query = question_text\n",
    "        turn_number = 1\n",
    "        \n",
    "        # Conversation loop\n",
    "        while turn_number <= max_conversation_turns:\n",
    "            print(f\"   Turn {turn_number}: Processing...\")\n",
    "            \n",
    "            try:\n",
    "                if chatbot_agent:\n",
    "                    # Create state for chatbot\n",
    "                    state = {\n",
    "                        'query': current_query,\n",
    "                        'user_id': 'test_user',\n",
    "                        'session_id': f'conversation_session_{question_id}',\n",
    "                        'query_id': f'query_{question_id}_turn_{turn_number}',\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'messages': []  # Start with empty messages for each turn\n",
    "                    }\n",
    "                    \n",
    "                    # Get chatbot response\n",
    "                    response_state = chatbot_agent(state)\n",
    "                    chatbot_response = response_state.get('ai_response', 'No response generated')\n",
    "                    confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n",
    "                    \n",
    "                else:\n",
    "                    # Mock chatbot response\n",
    "                    chatbot_response = f\"Thank you for your question. Let me help you with that... [Mock response turn {turn_number}]\"\n",
    "                    confidence = 0.8\n",
    "                \n",
    "                # Add to conversation history\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'chatbot_response': chatbot_response,\n",
    "                    'confidence': confidence,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                \n",
    "                # Analyze chatbot response for escalation\n",
    "                response_analysis = analyze_chatbot_response(chatbot_response, confidence)\n",
    "                \n",
    "                # Check if chatbot is escalating\n",
    "                if response_analysis['needs_escalation']:\n",
    "                    print(f\"   üîÑ Chatbot escalating: {response_analysis['escalation_reason']}\")\n",
    "                    conversation_turn['chatbot_action'] = 'escalation'\n",
    "                    conversation_turn['escalation_reason'] = response_analysis['escalation_reason']\n",
    "                    break\n",
    "                \n",
    "                # Generate customer response using AI simulator\n",
    "                try:\n",
    "                    customer_response = customer_ai.generate_response(chatbot_response, question_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error generating customer response: {e}\")\n",
    "                    customer_response = \"I'm having trouble understanding this. Can you help me differently?\"\n",
    "                \n",
    "                # Add customer response to turn\n",
    "                conversation_turn['customer_response'] = customer_response\n",
    "                conversation_turn['customer_satisfaction'] = customer_ai.satisfaction_level\n",
    "                \n",
    "                # Check if customer is satisfied (conversation ends)\n",
    "                if any(phrase in customer_response.lower() for phrase in ['thank you', 'that helps', 'perfect', 'great', 'excellent', 'makes sense']):\n",
    "                    if customer_ai.satisfaction_level >= customer_ai.persona['satisfaction_threshold']:\n",
    "                        print(f\"   ‚úÖ Customer satisfied!\")\n",
    "                        conversation_turn['customer_action'] = 'satisfied'\n",
    "                        break\n",
    "                \n",
    "                # Check if customer is requesting escalation\n",
    "                if any(phrase in customer_response.lower() for phrase in ['transfer', 'supervisor', 'specialist', 'human', 'someone else']):\n",
    "                    print(f\"   ‚¨ÜÔ∏è Customer requesting escalation\")\n",
    "                    conversation_turn['customer_action'] = 'escalation_request'\n",
    "                    break\n",
    "                \n",
    "                # Prepare for next turn\n",
    "                current_query = customer_response\n",
    "                turn_number += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error in turn {turn_number}: {e}\")\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                break\n",
    "        \n",
    "        # Determine final outcome\n",
    "        final_turn = conversation_history[-1] if conversation_history else {}\n",
    "        final_outcome = final_turn.get('customer_action', final_turn.get('chatbot_action', 'conversation_incomplete'))\n",
    "        \n",
    "        conversation_result = {\n",
    "            'id': question_id,\n",
    "            'original_question': question_text,\n",
    "            'customer_type': customer_type,\n",
    "            'complexity': complexity,\n",
    "            'model_used': model_to_use,\n",
    "            'conversation_history': conversation_history,\n",
    "            'total_turns': len(conversation_history),\n",
    "            'final_outcome': final_outcome,\n",
    "            'customer_satisfaction': customer_ai.satisfaction_level if customer_ai else 0.0,\n",
    "            'processing_time': datetime.now().isoformat(),\n",
    "            'conversation_complete': final_outcome in ['satisfied', 'escalation', 'escalation_request']\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìã Conversation complete: {final_outcome} in {len(conversation_history)} turns\")\n",
    "        return conversation_result\n",
    "    \n",
    "    # Process conversations for all questions\n",
    "    print(f\"ü§ñ Starting full conversations for {len(test_questions)} questions...\")\n",
    "    print(f\"This will simulate realistic customer-chatbot interactions until resolution.\\n\")\n",
    "    \n",
    "    conversation_results = []\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        try:\n",
    "            conversation_result = conduct_full_conversation(question_data)\n",
    "            conversation_results.append(conversation_result)\n",
    "            \n",
    "            # Show progress\n",
    "            outcome = conversation_result['final_outcome']\n",
    "            turns = conversation_result['total_turns']\n",
    "            satisfaction = conversation_result.get('customer_satisfaction', 0)\n",
    "            \n",
    "            print(f\"Question {question_data['id']}: {outcome} ({turns} turns, satisfaction: {satisfaction:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing conversation {question_data['id']}: {e}\")\n",
    "            # Add error result\n",
    "            error_result = {\n",
    "                'id': question_data['id'],\n",
    "                'original_question': question_data.get('question', ''),\n",
    "                'customer_type': question_data.get('customer_type', 'normal'),\n",
    "                'complexity': question_data.get('complexity', 'medium'),\n",
    "                'error': str(e),\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "            conversation_results.append(error_result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ All conversations completed!\")\n",
    "    \n",
    "    # Update results with conversation data\n",
    "    results = conversation_results\n",
    "    \n",
    "    # Summary statistics\n",
    "    completed_conversations = [r for r in results if r.get('conversation_complete', False)]\n",
    "    satisfied_customers = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    avg_turns = sum(r.get('total_turns', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    avg_satisfaction = sum(r.get('customer_satisfaction', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    \n",
    "    print(f\"\\nüìä Conversation Summary:\")\n",
    "    print(f\"  Completed conversations: {len(completed_conversations)}/{len(results)}\")\n",
    "    print(f\"  Customer satisfaction: {len(satisfied_customers)} ({len(satisfied_customers)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Escalations: {len(escalated_conversations)} ({len(escalated_conversations)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Average turns per conversation: {avg_turns:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {avg_satisfaction:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please load questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results and Settings\n",
    "\n",
    "Save the results and configuration settings to files with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Exporting with temporary config file contents...\n",
      "üìÑ Conversation results exported to: /workspace/notebooks/experiment_runs/chatbot_agent_output_20250720_1250.json\n",
      "‚öôÔ∏è Structured YAML settings exported to: /workspace/notebooks/experiment_runs/chatbot_agent_settings_20250720_1250.yaml\n",
      "\n",
      "üìä Export Summary:\n",
      "  Timestamp: 20250720_1250\n",
      "  Results file: chatbot_agent_output_20250720_1250.json (JSON)\n",
      "  Settings file: chatbot_agent_settings_20250720_1250.yaml (YAML)\n",
      "  Config source: temp_files\n",
      "  Total conversations: 20\n",
      "  Completed conversations: 20\n",
      "  Satisfaction rate: 40.0%\n",
      "  Escalation rate: 60.0%\n",
      "  Average turns per conversation: 2.7\n",
      "  Average customer satisfaction: 0.59\n",
      "  Files saved to: /workspace/notebooks/experiment_runs\n",
      "\n",
      "üìã YAML Structure for Import:\n",
      "  experiment.id: chatbot_agent_20250720_1250\n",
      "  base_configs.agent_config: Complete agent configuration\n",
      "  base_configs.prompts_config: Complete prompts configuration\n",
      "  base_configs.models_config: Complete models configuration\n",
      "  üí° Use this YAML file to recreate exact experiment conditions\n"
     ]
    }
   ],
   "source": [
    "# Export conversation results and settings in structured YAML format\n",
    "if 'results' in locals() and results:\n",
    "    # Create timestamp for filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    # Create experiment_runs directory if it doesn't exist\n",
    "    experiment_dir = Path('/workspace/notebooks/experiment_runs')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load current config contents from temp files if they exist\n",
    "    def get_config_contents_for_export():\n",
    "        \"\"\"Get current config file contents for export\"\"\"\n",
    "        if all(path.exists() for path in temp_file_paths.values()):\n",
    "            print(\"üìÅ Exporting with temporary config file contents...\")\n",
    "            \n",
    "            with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "                agent_config_content = f.read()\n",
    "            with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "                prompts_config_content = f.read()\n",
    "            with open(temp_file_paths['models_config'], 'r') as f:\n",
    "                models_config_content = f.read()\n",
    "            \n",
    "            return agent_config_content, prompts_config_content, models_config_content, True\n",
    "        else:\n",
    "            print(\"üìÅ Exporting with base config file contents...\")\n",
    "            \n",
    "            with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "                agent_config_content = f.read()\n",
    "            with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "                prompts_config_content = f.read()\n",
    "            with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "                models_config_content = f.read()\n",
    "            \n",
    "            return agent_config_content, prompts_config_content, models_config_content, False\n",
    "    \n",
    "    # Get config contents for export\n",
    "    agent_config_content, prompts_config_content, models_config_content, used_temp_configs = get_config_contents_for_export()\n",
    "    \n",
    "    # Export results with full conversation data (JSON format for complex data)\n",
    "    results_filename = f\"chatbot_agent_output_{timestamp}.json\"\n",
    "    results_path = experiment_dir / results_filename\n",
    "    \n",
    "    # Create comprehensive export with conversation analysis\n",
    "    export_data = {\n",
    "        'experiment_metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'agent_type': 'chatbot_agent',\n",
    "            'experiment_type': 'full_conversation_simulation',\n",
    "            'total_questions': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'generation_model': current_preferred_model,\n",
    "            'generation_timestamp': timestamp,\n",
    "            'config_source': experiment_config_source\n",
    "        },\n",
    "        'conversation_results': results,\n",
    "        'summary_statistics': {\n",
    "            'total_conversations': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'satisfied_customers': len([r for r in results if r.get('final_outcome') == 'satisfied']),\n",
    "            'escalated_conversations': len([r for r in results if 'escalation' in r.get('final_outcome', '')]),\n",
    "            'error_conversations': len([r for r in results if r.get('final_outcome') == 'error']),\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in results) / len(results) if results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in results) / len(results) if results else 0,\n",
    "            'satisfaction_rate': len([r for r in results if r.get('final_outcome') == 'satisfied']) / len(results) * 100 if results else 0,\n",
    "            'escalation_rate': len([r for r in results if 'escalation' in r.get('final_outcome', '')]) / len(results) * 100 if results else 0\n",
    "        },\n",
    "        'conversation_analysis': {\n",
    "            'by_customer_type': {},\n",
    "            'by_complexity': {},\n",
    "            'by_outcome': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Analyze by customer type\n",
    "    customer_types = list(set(r.get('customer_type', 'unknown') for r in results))\n",
    "    for ctype in customer_types:\n",
    "        ctype_results = [r for r in results if r.get('customer_type') == ctype]\n",
    "        export_data['conversation_analysis']['by_customer_type'][ctype] = {\n",
    "            'count': len(ctype_results),\n",
    "            'satisfaction_rate': len([r for r in ctype_results if r.get('final_outcome') == 'satisfied']) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'escalation_rate': len([r for r in ctype_results if 'escalation' in r.get('final_outcome', '')]) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by complexity\n",
    "    complexities = list(set(r.get('complexity', 'unknown') for r in results))\n",
    "    for complexity in complexities:\n",
    "        complexity_results = [r for r in results if r.get('complexity') == complexity]\n",
    "        export_data['conversation_analysis']['by_complexity'][complexity] = {\n",
    "            'count': len(complexity_results),\n",
    "            'satisfaction_rate': len([r for r in complexity_results if r.get('final_outcome') == 'satisfied']) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'escalation_rate': len([r for r in complexity_results if 'escalation' in r.get('final_outcome', '')]) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by outcome\n",
    "    outcomes = list(set(r.get('final_outcome', 'unknown') for r in results))\n",
    "    for outcome in outcomes:\n",
    "        outcome_results = [r for r in results if r.get('final_outcome') == outcome]\n",
    "        export_data['conversation_analysis']['by_outcome'][outcome] = {\n",
    "            'count': len(outcome_results),\n",
    "            'percentage': len(outcome_results) / len(results) * 100 if results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0\n",
    "        }\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Conversation results exported to: {results_path}\")\n",
    "    \n",
    "    # NEW: Export structured YAML experiment settings file\n",
    "    settings_filename = f\"chatbot_agent_settings_{timestamp}.yaml\"\n",
    "    settings_path = experiment_dir / settings_filename\n",
    "    \n",
    "    # Get uploaded filename safely\n",
    "    uploaded_filename = 'unknown'\n",
    "    if file_upload.value:\n",
    "        if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = file_upload.value[0]['name']\n",
    "        elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = list(file_upload.value.values())[0]['metadata']['name']\n",
    "    \n",
    "    # Create structured YAML export\n",
    "    yaml_experiment_data = {\n",
    "        'experiment': {\n",
    "            'id': f\"chatbot_agent_{timestamp}\",\n",
    "            'timestamp': timestamp,\n",
    "            'agent_type': 'chatbot_agent',\n",
    "            'experiment_type': 'full_conversation_simulation',\n",
    "            'config_source': experiment_config_source,\n",
    "            'input_file': uploaded_filename,\n",
    "            'total_questions': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)])\n",
    "        },\n",
    "        'base_configs': {\n",
    "            'agent_config': agent_config_content,\n",
    "            'prompts_config': prompts_config_content,\n",
    "            'models_config': models_config_content\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write YAML file with literal block scalars to preserve formatting\n",
    "    yaml_content = f\"\"\"# Chatbot Agent Experiment Settings\n",
    "# Generated: {timestamp}\n",
    "# Config source: {experiment_config_source}\n",
    "\n",
    "experiment:\n",
    "  id: \"chatbot_agent_{timestamp}\"\n",
    "  timestamp: \"{timestamp}\"\n",
    "  agent_type: \"chatbot_agent\"\n",
    "  experiment_type: \"full_conversation_simulation\"\n",
    "  config_source: \"{experiment_config_source}\"\n",
    "  input_file: \"{uploaded_filename}\"\n",
    "  total_questions: {len(results)}\n",
    "  completed_conversations: {len([r for r in results if r.get('conversation_complete', False)])}\n",
    "\n",
    "base_configs:\n",
    "  agent_config: |\n",
    "{yaml.dump(yaml.safe_load(agent_config_content), default_flow_style=False, sort_keys=False).rstrip()}\n",
    "\n",
    "  prompts_config: |\n",
    "{yaml.dump(yaml.safe_load(prompts_config_content), default_flow_style=False, sort_keys=False).rstrip()}\n",
    "\n",
    "  models_config: |\n",
    "{yaml.dump(yaml.safe_load(models_config_content), default_flow_style=False, sort_keys=False).rstrip()}\n",
    "\"\"\"\n",
    "    \n",
    "    with open(settings_path, 'w') as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Structured YAML settings exported to: {settings_path}\")\n",
    "    \n",
    "    # Show enhanced summary\n",
    "    stats = export_data['summary_statistics']\n",
    "    print(f\"\\nüìä Export Summary:\")\n",
    "    print(f\"  Timestamp: {timestamp}\")\n",
    "    print(f\"  Results file: {results_filename} (JSON)\")\n",
    "    print(f\"  Settings file: {settings_filename} (YAML)\")\n",
    "    print(f\"  Config source: {experiment_config_source}\")\n",
    "    print(f\"  Total conversations: {stats['total_conversations']}\")\n",
    "    print(f\"  Completed conversations: {stats['completed_conversations']}\")\n",
    "    print(f\"  Satisfaction rate: {stats['satisfaction_rate']:.1f}%\")\n",
    "    print(f\"  Escalation rate: {stats['escalation_rate']:.1f}%\")\n",
    "    print(f\"  Average turns per conversation: {stats['average_turns']:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {stats['average_satisfaction']:.2f}\")\n",
    "    print(f\"  Files saved to: {experiment_dir.absolute()}\")\n",
    "    \n",
    "    print(f\"\\nüìã YAML Structure for Import:\")\n",
    "    print(f\"  experiment.id: chatbot_agent_{timestamp}\")\n",
    "    print(f\"  base_configs.agent_config: Complete agent configuration\")\n",
    "    print(f\"  base_configs.prompts_config: Complete prompts configuration\")\n",
    "    print(f\"  base_configs.models_config: Complete models configuration\")\n",
    "    print(f\"  üí° Use this YAML file to recreate exact experiment conditions\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to export. Please complete conversations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Review Results\n",
    "\n",
    "Display and analyze the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Conversation Results Review and Analysis\n",
      "\n",
      "=== CONVERSATION SUMMARY STATISTICS ===\n",
      "Total conversations: 20\n",
      "Completed conversations: 20\n",
      "Average turns per conversation: 2.7\n",
      "Average customer satisfaction: 0.590\n",
      "\n",
      "=== CONVERSATION OUTCOMES ===\n",
      "escalation_request: 12 (60.0%)\n",
      "satisfied: 8 (40.0%)\n",
      "\n",
      "=== ANALYSIS BY CUSTOMER TYPE ===\n",
      "               Avg Turns  Avg Satisfaction  Completed\n",
      "customer_type                                        \n",
      "confused             3.4              0.71          5\n",
      "frustrated           1.8              0.41          5\n",
      "normal               1.8              0.70          5\n",
      "urgent               3.8              0.54          5\n",
      "\n",
      "Satisfaction Rate by Customer Type:\n",
      "  frustrated: 0/5 (0.0%)\n",
      "  urgent: 0/5 (0.0%)\n",
      "  confused: 4/5 (80.0%)\n",
      "  normal: 4/5 (80.0%)\n",
      "\n",
      "=== ANALYSIS BY COMPLEXITY ===\n",
      "            Avg Turns  Avg Satisfaction  Completed\n",
      "complexity                                        \n",
      "complex         3.000             0.575          6\n",
      "medium          3.000             0.493          7\n",
      "simple          2.143             0.700          7\n",
      "\n",
      "=== CONVERSATION SUMMARY TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "      <th>total_turns</th>\n",
       "      <th>final_outcome</th>\n",
       "      <th>customer_satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>1</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>6</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>5</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id customer_type complexity  total_turns       final_outcome  \\\n",
       "0    1    frustrated     medium            2  escalation_request   \n",
       "1    2        urgent     simple            4  escalation_request   \n",
       "2    3      confused     simple            1           satisfied   \n",
       "3    4        normal     simple            3  escalation_request   \n",
       "4    5    frustrated    complex            1  escalation_request   \n",
       "5    6      confused     medium            3           satisfied   \n",
       "6    7        urgent    complex            4  escalation_request   \n",
       "7    8        urgent     simple            4  escalation_request   \n",
       "8    9        normal     simple            1           satisfied   \n",
       "9   10      confused     medium            6  escalation_request   \n",
       "10  11    frustrated     medium            2  escalation_request   \n",
       "11  12      confused    complex            5           satisfied   \n",
       "12  13        normal     medium            3           satisfied   \n",
       "13  14        urgent    complex            4  escalation_request   \n",
       "14  15    frustrated     medium            2  escalation_request   \n",
       "15  16        normal     simple            1           satisfied   \n",
       "16  17      confused    complex            2           satisfied   \n",
       "17  18        urgent     medium            3  escalation_request   \n",
       "18  19    frustrated    complex            2  escalation_request   \n",
       "19  20        normal     simple            1           satisfied   \n",
       "\n",
       "    customer_satisfaction  \n",
       "0                    0.30  \n",
       "1                    0.65  \n",
       "2                    0.80  \n",
       "3                    0.70  \n",
       "4                    0.55  \n",
       "5                    0.75  \n",
       "6                    0.50  \n",
       "7                    0.65  \n",
       "8                    0.70  \n",
       "9                    0.50  \n",
       "10                   0.40  \n",
       "11                   0.75  \n",
       "12                   0.70  \n",
       "13                   0.50  \n",
       "14                   0.40  \n",
       "15                   0.70  \n",
       "16                   0.75  \n",
       "17                   0.40  \n",
       "18                   0.40  \n",
       "19                   0.70  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE FULL CONVERSATIONS ===\n",
      "\n",
      "üü¢ SATISFIED CUSTOMER CONVERSATIONS:\n",
      "\n",
      "--- Conversation 3 (confused, simple) ---\n",
      "Original Question: Can someone explain what a deductible is? I keep seeing this word but don't understand.\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.80)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: Can someone explain what a deductible is? I keep seeing this word but don't understand.\n",
      "    Chatbot:  \n",
      "- Use simple language and avoid jargon\n",
      "\n",
      "AGENT RESPONSE: Hello! I'd be happy to help you understand what a deductible is. A deductible is an amount of...\n",
      "    Customer Reply: Oh I see! That makes much more sense now. Thank you for explaining it so clearly.\n",
      "    Satisfaction: 0.80\n",
      "\n",
      "--- Conversation 6 (confused, medium) ---\n",
      "Original Question: What's the difference between comprehensive and collision coverage? I'm trying to decide what I need.\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.75)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: What's the difference between comprehensive and collision coverage? I'm trying to decide what I need.\n",
      "    Chatbot:  \n",
      "- Offer follow-up support or additional information when appropriate\n",
      "\n",
      "AGENT RESPONSE: Hello! Thank you for reaching out to us. Comprehensive coverage...\n",
      "    Customer Reply: Oh, I see. So comprehensive coverage protects against non-collision damages and collision coverage protects against... collisions? That makes sense. B...\n",
      "    Satisfaction: 0.60\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: Oh, I see. So comprehensive coverage protects against non-collision damages and collision coverage protects against... collisions? That makes sense. B...\n",
      "    Chatbot:   in explanations\n",
      "- Offer additional support or resources as needed\n",
      "\n",
      "AGENT RESPONSE: Hello! I'm happy to help you with your question. When deciding whi...\n",
      "    Customer Reply: Oh, I see. So comprehensive coverage protects against non-collision damages, but only collision covers damages from accidents? That's helpful to know!...\n",
      "    Satisfaction: 0.60\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: Oh, I see. So comprehensive coverage protects against non-collision damages, but only collision covers damages from accidents? That's helpful to know!...\n",
      "    Chatbot:  \n",
      "- Use simple language and avoid jargon\n",
      "- Offer additional resources or support when appropriate I've prioritized your request to help resolve this qu...\n",
      "    Customer Reply: Perfect! Now I get it. I really appreciate your patience.\n",
      "    Satisfaction: 0.75\n",
      "\n",
      "üî¥ ESCALATED CONVERSATIONS:\n",
      "\n",
      "--- Conversation 1 (frustrated, medium) ---\n",
      "Original Question: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.30)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "    Chatbot:  \n",
      "- Avoid using technical jargon or insurance industry buzzwords unless necessary\n",
      "\n",
      "SAMPLE RESPONSE:\n",
      "Thank you for reaching out to us about your recent ...\n",
      "    Customer Reply: \"What market trends?! I'm being penalized for something that isn't my fault?! How can you even say that when you haven't even looked at my policy?!\"\n",
      "    Satisfaction: 0.30\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: \"What market trends?! I'm being penalized for something that isn't my fault?! How can you even say that when you haven't even looked at my policy?!\"\n",
      "    Chatbot:   in your response\n",
      "- Offer additional support or resources when appropriate.\n",
      "    Customer Reply: Enough! Get me a human who knows what they're doing!\n",
      "    Satisfaction: 0.30\n",
      "\n",
      "--- Conversation 2 (urgent, simple) ---\n",
      "Original Question: I just had a car accident. What do I need to do right now?\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.65)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: I just had a car accident. What do I need to do right now?\n",
      "    Chatbot:   in response\n",
      "\n",
      "AGENT RESPONSE: Oh no, I'm so sorry to hear that! üòî If you're unharmed and able to move your vehicle, please call the police to file a r...\n",
      "    Customer Reply: Oh no, I'm so glad you're there to help! Can you please tell me if my car is covered under my insurance? And how do I file a claim?\n",
      "    Satisfaction: 0.50\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: Oh no, I'm so glad you're there to help! Can you please tell me if my car is covered under my insurance? And how do I file a claim?\n",
      "    Chatbot:  \n",
      "- Address immediate concerns and needs\n",
      "- Include relevant details or links when possible\n",
      "\n",
      "RESPONSE: Of course, I'd be happy to help! Yes, your car is...\n",
      "    Customer Reply: - Build on the previous question or comment\n",
      "- Display a natural flow of conversation\n",
      "\n",
      "\"Thanks for letting me know. Can you tell me more about what I n...\n",
      "    Satisfaction: 0.65\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: - Build on the previous question or comment\n",
      "- Display a natural flow of conversation\n",
      "\n",
      "\"Thanks for letting me know. Can you tell me more about what I n...\n",
      "    Chatbot:  \n",
      "- Maintain a consistent and natural flow of conversation\n",
      "\n",
      "System: Thank you for reaching out! After filing a claim, you will need to follow the next ...\n",
      "    Customer Reply: This is urgent - please connect me with someone who can resolve this now.\n",
      "    Satisfaction: 0.50\n",
      "  ... and 1 more turns\n",
      "\n",
      "=== CONVERSATION INSIGHTS ===\n",
      "Conversation Length Distribution:\n",
      "  Short (1-2 turns): 10 (50.0%)\n",
      "  Medium (3-5 turns): 9 (45.0%)\n",
      "  Long (6+ turns): 1 (5.0%)\n",
      "\n",
      "Customer Satisfaction Distribution:\n",
      "  High (0.8+): 1 (5.0%)\n",
      "  Medium (0.5-0.8): 14 (70.0%)\n",
      "  Low (<0.5): 5 (25.0%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\n",
      "üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\n"
     ]
    }
   ],
   "source": [
    "# Review and analyze conversation results\n",
    "if 'results' in locals() and results:\n",
    "    print(\"üìã Conversation Results Review and Analysis\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'id': r.get('id'),\n",
    "            'customer_type': r.get('customer_type'),\n",
    "            'complexity': r.get('complexity'),\n",
    "            'total_turns': r.get('total_turns', 0),\n",
    "            'final_outcome': r.get('final_outcome'),\n",
    "            'customer_satisfaction': r.get('customer_satisfaction', 0),\n",
    "            'conversation_complete': r.get('conversation_complete', False)\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"=== CONVERSATION SUMMARY STATISTICS ===\")\n",
    "    print(f\"Total conversations: {len(df_results)}\")\n",
    "    print(f\"Completed conversations: {df_results['conversation_complete'].sum()}\")\n",
    "    print(f\"Average turns per conversation: {df_results['total_turns'].mean():.1f}\")\n",
    "    print(f\"Average customer satisfaction: {df_results['customer_satisfaction'].mean():.3f}\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(\"\\n=== CONVERSATION OUTCOMES ===\")\n",
    "    outcome_counts = df_results['final_outcome'].value_counts()\n",
    "    for outcome, count in outcome_counts.items():\n",
    "        percentage = count / len(df_results) * 100\n",
    "        print(f\"{outcome}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analysis by customer type\n",
    "    print(\"\\n=== ANALYSIS BY CUSTOMER TYPE ===\")\n",
    "    customer_analysis = df_results.groupby('customer_type').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    customer_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(customer_analysis)\n",
    "    \n",
    "    # Satisfaction rate by customer type\n",
    "    print(\"\\nSatisfaction Rate by Customer Type:\")\n",
    "    for ctype in df_results['customer_type'].unique():\n",
    "        ctype_data = df_results[df_results['customer_type'] == ctype]\n",
    "        satisfied = len(ctype_data[ctype_data['final_outcome'] == 'satisfied'])\n",
    "        total = len(ctype_data)\n",
    "        print(f\"  {ctype}: {satisfied}/{total} ({satisfied/total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by complexity\n",
    "    print(\"\\n=== ANALYSIS BY COMPLEXITY ===\")\n",
    "    complexity_analysis = df_results.groupby('complexity').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    complexity_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(complexity_analysis)\n",
    "    \n",
    "    # Show detailed results table\n",
    "    print(\"\\n=== CONVERSATION SUMMARY TABLE ===\")\n",
    "    display_df = df_results[['id', 'customer_type', 'complexity', 'total_turns', 'final_outcome', 'customer_satisfaction']].copy()\n",
    "    display_df['customer_satisfaction'] = display_df['customer_satisfaction'].round(3)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Show sample full conversations\n",
    "    print(\"\\n=== SAMPLE FULL CONVERSATIONS ===\")\n",
    "    \n",
    "    # Show 2 satisfied and 2 escalated conversations for analysis\n",
    "    satisfied_conversations = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    def display_conversation(conversation_data, max_turns=3):\n",
    "        \"\"\"Display a conversation with turn-by-turn analysis\"\"\"\n",
    "        conv_id = conversation_data.get('id')\n",
    "        customer_type = conversation_data.get('customer_type')\n",
    "        complexity = conversation_data.get('complexity')\n",
    "        outcome = conversation_data.get('final_outcome')\n",
    "        satisfaction = conversation_data.get('customer_satisfaction', 0)\n",
    "        \n",
    "        print(f\"\\n--- Conversation {conv_id} ({customer_type}, {complexity}) ---\")\n",
    "        print(f\"Original Question: {conversation_data.get('original_question', '')}\")\n",
    "        print(f\"Final Outcome: {outcome} (Customer Satisfaction: {satisfaction:.2f})\")\n",
    "        \n",
    "        conversation_history = conversation_data.get('conversation_history', [])\n",
    "        turns_to_show = min(max_turns, len(conversation_history))\n",
    "        \n",
    "        for i, turn in enumerate(conversation_history[:turns_to_show]):\n",
    "            turn_num = turn.get('turn_number', i+1)\n",
    "            print(f\"\\n  Turn {turn_num}:\")\n",
    "            print(f\"    Customer: {turn.get('customer_query', '')[:150]}{'...' if len(turn.get('customer_query', '')) > 150 else ''}\")\n",
    "            print(f\"    Chatbot:  {turn.get('chatbot_response', '')[:150]}{'...' if len(turn.get('chatbot_response', '')) > 150 else ''}\")\n",
    "            \n",
    "            if 'customer_response' in turn:\n",
    "                print(f\"    Customer Reply: {turn.get('customer_response', '')[:150]}{'...' if len(turn.get('customer_response', '')) > 150 else ''}\")\n",
    "                print(f\"    Satisfaction: {turn.get('customer_satisfaction', 0):.2f}\")\n",
    "        \n",
    "        if len(conversation_history) > turns_to_show:\n",
    "            print(f\"  ... and {len(conversation_history) - turns_to_show} more turns\")\n",
    "    \n",
    "    # Show satisfied conversations\n",
    "    if satisfied_conversations:\n",
    "        print(\"\\nüü¢ SATISFIED CUSTOMER CONVERSATIONS:\")\n",
    "        for conv in satisfied_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Show escalated conversations  \n",
    "    if escalated_conversations:\n",
    "        print(\"\\nüî¥ ESCALATED CONVERSATIONS:\")\n",
    "        for conv in escalated_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Conversation insights\n",
    "    print(\"\\n=== CONVERSATION INSIGHTS ===\")\n",
    "    \n",
    "    # Turn analysis\n",
    "    short_conversations = len(df_results[df_results['total_turns'] <= 2])\n",
    "    medium_conversations = len(df_results[(df_results['total_turns'] > 2) & (df_results['total_turns'] <= 5)])\n",
    "    long_conversations = len(df_results[df_results['total_turns'] > 5])\n",
    "    \n",
    "    print(f\"Conversation Length Distribution:\")\n",
    "    print(f\"  Short (1-2 turns): {short_conversations} ({short_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (3-5 turns): {medium_conversations} ({medium_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Long (6+ turns): {long_conversations} ({long_conversations/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Satisfaction insights\n",
    "    high_satisfaction = len(df_results[df_results['customer_satisfaction'] >= 0.8])\n",
    "    medium_satisfaction = len(df_results[(df_results['customer_satisfaction'] >= 0.5) & (df_results['customer_satisfaction'] < 0.8)])\n",
    "    low_satisfaction = len(df_results[df_results['customer_satisfaction'] < 0.5])\n",
    "    \n",
    "    print(f\"\\nCustomer Satisfaction Distribution:\")\n",
    "    print(f\"  High (0.8+): {high_satisfaction} ({high_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.5-0.8): {medium_satisfaction} ({medium_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Low (<0.5): {low_satisfaction} ({low_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\")\n",
    "    print(\"üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to review. Please complete conversations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've successfully tested the Chatbot Agent with full conversation simulation.\n",
    "\n",
    "### What you've accomplished:\n",
    "- ‚úÖ Loaded and customized Chatbot Agent configuration\n",
    "- ‚úÖ Loaded test questions from file with customer profiles  \n",
    "- ‚úÖ Conducted full conversations between customer AI and chatbot AI\n",
    "- ‚úÖ Simulated realistic customer interactions until resolution or escalation\n",
    "- ‚úÖ Captured complete conversation flows with satisfaction tracking\n",
    "- ‚úÖ Exported comprehensive results with conversation analysis\n",
    "- ‚úÖ Analyzed performance by customer type, complexity, and outcomes\n",
    "\n",
    "### Your exported files contain:\n",
    "- **Conversation Results**: Complete turn-by-turn interaction data including:\n",
    "  - Full conversation history for each question\n",
    "  - Customer satisfaction scores at each turn\n",
    "  - Final outcomes (satisfied, escalation, etc.)\n",
    "  - Detailed conversation analysis by customer type and complexity\n",
    "- **Settings file**: Complete configuration used for this experiment\n",
    "- **Conversation Analysis**: Statistical breakdown of conversation patterns\n",
    "\n",
    "### Key Features of the Conversation System:\n",
    "- **Customer AI Simulation**: Realistic customer personas (frustrated, urgent, confused, normal)\n",
    "- **Dynamic Satisfaction Tracking**: AI-powered customer satisfaction scoring\n",
    "- **Natural Conversation Flow**: Continues until natural resolution or escalation\n",
    "- **Personality-Based Responses**: Customer responses match their personality type\n",
    "- **Escalation Detection**: Automatic detection when customers or chatbot need human assistance\n",
    "- **Model Integration**: Uses actual models from questions file for both customer and chatbot\n",
    "\n",
    "### Conversation Outcomes Explained:\n",
    "- **Satisfied**: Customer received satisfactory answer and conversation ended positively\n",
    "- **Escalation**: Chatbot determined the query needed human specialist assistance  \n",
    "- **Escalation Request**: Customer explicitly requested to speak with a human\n",
    "- **Conversation Incomplete**: Reached maximum turns without resolution\n",
    "\n",
    "### Ready for Advanced Analysis?\n",
    "- **Conversation Patterns**: Analyze which question types lead to longer conversations\n",
    "- **Satisfaction Drivers**: Identify what chatbot responses increase customer satisfaction\n",
    "- **Escalation Triggers**: Understand when and why conversations need human intervention\n",
    "- **Customer Journey Mapping**: Track how different customer types interact with the system\n",
    "- **Model Performance**: Compare how different AI models handle customer interactions\n",
    "\n",
    "### To optimize your chatbot:\n",
    "1. **Review Escalated Conversations**: Look for patterns in conversations that required escalation\n",
    "2. **Analyze Low Satisfaction**: Identify what causes customer dissatisfaction\n",
    "3. **Study Successful Resolutions**: Learn from conversations that ended with satisfied customers\n",
    "4. **Adjust Configuration**: Modify agent settings based on conversation insights\n",
    "5. **Test Iteratively**: Run multiple experiments with different configurations\n",
    "\n",
    "### Workflow for Continuous Improvement:\n",
    "1. **Generate Questions**: Use `question_generator.ipynb` to create diverse test scenarios\n",
    "2. **Run Conversations**: Use this notebook to conduct full conversation simulations\n",
    "3. **Analyze Results**: Review conversation patterns and customer satisfaction metrics\n",
    "4. **Optimize Configuration**: Adjust chatbot settings based on insights\n",
    "5. **Validate Changes**: Re-run tests to confirm improvements\n",
    "6. **Deploy**: Apply successful configurations to production systems\n",
    "\n",
    "### Advanced Features Available:\n",
    "- **Multi-Model Testing**: Test different AI models for customer and chatbot roles\n",
    "- **Custom Customer Personas**: Modify customer AI behavior for specific use cases  \n",
    "- **Conversation Length Controls**: Adjust maximum turns and escalation thresholds\n",
    "- **Satisfaction Threshold Tuning**: Customize what constitutes a satisfied customer\n",
    "- **Real-time Analysis**: Monitor conversation quality as it happens\n",
    "\n",
    "---\n",
    "*This notebook simulates realistic customer service interactions to optimize AI chatbot performance and improve customer satisfaction outcomes.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}