{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Chatbot Agent with pre-generated questions.\n",
    "It's designed for users with little programming experience.\n",
    "\n",
    "## Features:\n",
    "- Load and edit agent configuration settings\n",
    "- Load test questions from JSON files\n",
    "- Process questions through the Chatbot Agent\n",
    "- Review and analyze results\n",
    "- Export results with timestamps\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load test questions from file (generated using question_generator.ipynb)\n",
    "4. Review questions before processing\n",
    "5. Run the agent and review results\n",
    "\n",
    "## Question Generation:\n",
    "Use the separate `question_generator.ipynb` notebook to create test questions, then load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Ready to start testing the Chatbot Agent.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports (this helps with relative imports)\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.chatbot_agent import ChatbotAgentNode\n",
    "from src.core.config.agent_config_manager import AgentConfigManager\n",
    "from src.integrations.llm_providers import LLMProviderFactory\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start testing the Chatbot Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration Settings\n",
    "\n",
    "The following cell loads the current configuration for the Chatbot Agent.\n",
    "You can edit these values to customize the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration files loaded successfully!\n",
      "Agent name: chatbot_agent\n",
      "Agent version: 1.0.0\n",
      "Preferred model: anthropic_general_budget\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from files\n",
    "config_base_path = Path('/workspace/config')\n",
    "agent_config_path = config_base_path / 'agents' / 'chatbot_agent'\n",
    "\n",
    "def load_config_files():\n",
    "    \"\"\"Load all configuration files for the Chatbot Agent\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Load agent config\n",
    "    with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "        configs['agent'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load prompts\n",
    "    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "        configs['prompts'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load models\n",
    "    with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "        configs['models'] = yaml.safe_load(f)\n",
    "    \n",
    "    # Load shared models for reference\n",
    "    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n",
    "        configs['shared_models'] = yaml.safe_load(f)\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Load configurations\n",
    "configs = load_config_files()\n",
    "\n",
    "print(\"üìÅ Configuration files loaded successfully!\")\n",
    "print(f\"Agent name: {configs['agent']['agent']['name']}\")\n",
    "print(f\"Agent version: {configs['agent']['agent']['version']}\")\n",
    "print(f\"Preferred model: {configs['agent']['models']['preferred']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Editable Configuration Settings\n",
    "\n",
    "Edit these settings to customize how the Chatbot Agent behaves.\n",
    "These variables map directly to the configuration files and can be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration variables loaded and ready for editing!\n",
      "\n",
      "Current literal values (you can copy these to edit the variables above):\n",
      "agent_preferred_model = 'anthropic_general_budget'\n",
      "agent_fallback_models = ['local_general_budget', 'local_general_standard']\n",
      "agent_temperature = 0.7\n",
      "agent_max_tokens = 2000\n",
      "agent_timeout = 30\n",
      "agent_response_style = 'clear_and_professional'\n",
      "agent_context_integration = True\n",
      "agent_personalization = True\n",
      "agent_confidence_threshold = 0.7\n",
      "agent_auto_escalation = True\n",
      "agent_escalation_triggers = ['low_confidence', 'user_dissatisfaction', 'repeat_query']\n",
      "agent_prompt_style = 'Warm, professional customer service tone'\n",
      "agent_include_empathy = True\n",
      "agent_customer_focus = True\n",
      "\n",
      "üí° Copy any of these lines above to edit the variables, then re-run this cell to apply changes.\n",
      "üìù After editing, the new values will be used when processing questions.\n"
     ]
    }
   ],
   "source": [
    "# EDITABLE CONFIGURATION VARIABLES\n",
    "# These can be modified to customize the agent behavior\n",
    "\n",
    "# === MODEL SETTINGS ===\n",
    "agent_preferred_model = configs['agent']['models']['preferred']  # Which model to use first\n",
    "agent_fallback_models = configs['agent']['models']['fallback']  # Backup models if primary fails\n",
    "agent_temperature = configs['agent']['settings']['temperature']  # Creativity level (0.0-1.0)\n",
    "agent_max_tokens = configs['agent']['settings']['max_tokens']  # Maximum response length\n",
    "agent_timeout = configs['agent']['settings']['timeout']  # Timeout in seconds\n",
    "\n",
    "# === BEHAVIOR SETTINGS ===\n",
    "agent_response_style = configs['agent']['behavior']['response_style']  # How the agent responds\n",
    "agent_context_integration = configs['agent']['behavior']['context_integration']  # Use conversation history\n",
    "agent_personalization = configs['agent']['behavior']['personalization']  # Personalize responses\n",
    "\n",
    "# === ESCALATION SETTINGS ===\n",
    "agent_confidence_threshold = configs['agent']['escalation']['confidence_threshold']  # When to escalate (0.0-1.0)\n",
    "agent_auto_escalation = configs['agent']['escalation']['enable_auto_escalation']  # Enable automatic escalation\n",
    "agent_escalation_triggers = configs['agent']['escalation']['escalation_triggers']  # What triggers escalation\n",
    "\n",
    "# === PROMPT STYLE ===\n",
    "agent_prompt_style = configs['prompts']['response_guidelines']['style']  # Communication style\n",
    "agent_include_empathy = configs['prompts']['response_guidelines']['include_empathy']  # Show empathy\n",
    "agent_customer_focus = configs['prompts']['response_guidelines']['customer_focus']  # Focus on customer needs\n",
    "\n",
    "# Display current literal values for easy editing\n",
    "print(\"‚öôÔ∏è Configuration variables loaded and ready for editing!\")\n",
    "print(\"\\nCurrent literal values (you can copy these to edit the variables above):\")\n",
    "print(f\"agent_preferred_model = '{agent_preferred_model}'\")\n",
    "print(f\"agent_fallback_models = {agent_fallback_models}\")\n",
    "print(f\"agent_temperature = {agent_temperature}\")\n",
    "print(f\"agent_max_tokens = {agent_max_tokens}\")\n",
    "print(f\"agent_timeout = {agent_timeout}\")\n",
    "print(f\"agent_response_style = '{agent_response_style}'\")\n",
    "print(f\"agent_context_integration = {agent_context_integration}\")\n",
    "print(f\"agent_personalization = {agent_personalization}\")\n",
    "print(f\"agent_confidence_threshold = {agent_confidence_threshold}\")\n",
    "print(f\"agent_auto_escalation = {agent_auto_escalation}\")\n",
    "print(f\"agent_escalation_triggers = {agent_escalation_triggers}\")\n",
    "print(f\"agent_prompt_style = '{agent_prompt_style}'\")\n",
    "print(f\"agent_include_empathy = {agent_include_empathy}\")\n",
    "print(f\"agent_customer_focus = {agent_customer_focus}\")\n",
    "\n",
    "print(\"\\nüí° Copy any of these lines above to edit the variables, then re-run this cell to apply changes.\")\n",
    "print(\"üìù After editing, the new values will be used when processing questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Questions\n",
    "\n",
    "Load test questions from a JSON file. Use question_generator.ipynb to create new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Load Test Questions\n",
      "\n",
      "üí° How to get test questions:\n",
      "1. Use question_generator.ipynb to create test questions\n",
      "2. Upload the generated JSON file below\n",
      "3. Supported formats: Full export or questions-only JSON\n",
      "\n",
      "üìÑ Expected JSON format:\n",
      "- Full export: {'metadata': {...}, 'questions': [...]}\n",
      "- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\n",
      "\n",
      "üìÅ Upload your questions file:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c64a18f27a4e15ada855ec212e8e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.json', description='Upload questions file:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File upload widget for loading questions\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.json',\n",
    "    multiple=False,\n",
    "    description='Upload questions file:'\n",
    ")\n",
    "\n",
    "# Instructions for file format\n",
    "print(\"üìù Load Test Questions\")\n",
    "print(\"\\nüí° How to get test questions:\")\n",
    "print(\"1. Use question_generator.ipynb to create test questions\")\n",
    "print(\"2. Upload the generated JSON file below\")\n",
    "print(\"3. Supported formats: Full export or questions-only JSON\")\n",
    "print(\"\\nüìÑ Expected JSON format:\")\n",
    "print(\"- Full export: {'metadata': {...}, 'questions': [...]}\")\n",
    "print(\"- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\")\n",
    "print(\"\\nüìÅ Upload your questions file:\")\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Using tuple format\n",
      "üîç File content type: <class 'memoryview'>\n",
      "üìÅ Loading questions from: test_questions_20250718_1744.json\n",
      "üìÑ Loaded file with metadata:\n",
      "  Generation model: anthropic_general_standard\n",
      "  Generation timestamp: 20250718_1744\n",
      "  Question count: 20\n",
      "‚úÖ Loaded 20 test questions\n",
      "\n",
      "üìã Preview of loaded questions:\n",
      "  1. Why did my premium increase by $200? This is ridiculous - I haven't had any claims! [frustrated]\n",
      "  2. I just had a car accident. What do I need to do right now? [urgent]\n",
      "  3. Can someone explain what a deductible is? I keep seeing this word but don't understand. [confused]\n",
      "  ... and 17 more questions\n",
      "\n",
      "üìä Question Distribution:\n",
      "  Customer types: {'frustrated': np.int64(5), 'urgent': np.int64(5), 'confused': np.int64(5), 'normal': np.int64(5)}\n",
      "  Complexities: {'medium': np.int64(7), 'simple': np.int64(7), 'complex': np.int64(6)}\n"
     ]
    }
   ],
   "source": [
    "# Load test questions from uploaded file\n",
    "test_questions = []\n",
    "\n",
    "def load_questions_from_file(file_content, filename):\n",
    "    \"\"\"Load questions from uploaded JSON file\"\"\"\n",
    "    try:\n",
    "        # Handle different content types\n",
    "        if isinstance(file_content, memoryview):\n",
    "            # Convert memoryview to bytes\n",
    "            content_bytes = file_content.tobytes()\n",
    "        elif hasattr(file_content, 'decode'):\n",
    "            # Already bytes\n",
    "            content_bytes = file_content\n",
    "        else:\n",
    "            # Convert to bytes if it's a string or other type\n",
    "            content_bytes = str(file_content).encode('utf-8')\n",
    "        \n",
    "        # Decode to string and parse JSON\n",
    "        data = json.loads(content_bytes.decode('utf-8'))\n",
    "        \n",
    "        # Handle different JSON formats\n",
    "        if isinstance(data, dict):\n",
    "            # Full export format with metadata\n",
    "            if 'questions' in data:\n",
    "                questions = data['questions']\n",
    "                metadata = data.get('metadata', {})\n",
    "                print(f\"üìÑ Loaded file with metadata:\")\n",
    "                print(f\"  Generation model: {metadata.get('generation_model', 'unknown')}\")\n",
    "                print(f\"  Generation timestamp: {metadata.get('generation_timestamp', 'unknown')}\")\n",
    "                print(f\"  Question count: {metadata.get('question_count', len(questions))}\")\n",
    "                return questions\n",
    "            else:\n",
    "                # Single question object\n",
    "                return [data]\n",
    "        elif isinstance(data, list):\n",
    "            # Questions-only format\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected data format: {type(data)}\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        print(f\"   File content type: {type(file_content)}\")\n",
    "        return []\n",
    "\n",
    "def validate_questions(questions):\n",
    "    \"\"\"Validate and normalize question format\"\"\"\n",
    "    validated_questions = []\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        if isinstance(q, dict):\n",
    "            # Ensure required fields exist\n",
    "            validated_q = {\n",
    "                \"id\": q.get(\"id\", i + 1),\n",
    "                \"question\": q.get(\"question\", f\"Question {i + 1}\"),\n",
    "                \"customer_type\": q.get(\"customer_type\", \"normal\"),\n",
    "                \"complexity\": q.get(\"complexity\", \"medium\")\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "        else:\n",
    "            # Convert string to dict if needed\n",
    "            validated_q = {\n",
    "                \"id\": i + 1,\n",
    "                \"question\": str(q),\n",
    "                \"customer_type\": \"normal\",\n",
    "                \"complexity\": \"medium\"\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "    \n",
    "    return validated_questions\n",
    "\n",
    "# Process uploaded file\n",
    "if file_upload.value:\n",
    "    uploaded_file = None\n",
    "    filename = None\n",
    "    file_content = None\n",
    "    \n",
    "    # Handle different file upload widget formats\n",
    "    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using tuple format\")\n",
    "        uploaded_file = file_upload.value[0]\n",
    "        filename = uploaded_file['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using dict format\")\n",
    "        uploaded_file = list(file_upload.value.values())[0]\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Unable to read uploaded file format\")\n",
    "        print(f\"   Type: {type(file_upload.value)}\")\n",
    "        print(f\"   Length: {len(file_upload.value) if hasattr(file_upload.value, '__len__') else 'No length'}\")\n",
    "        print(f\"   Content: {file_upload.value}\")\n",
    "    \n",
    "    if uploaded_file and filename and file_content is not None:\n",
    "        print(f\"üìÅ Loading questions from: {filename}\")\n",
    "        \n",
    "        raw_questions = load_questions_from_file(file_content, filename)\n",
    "        \n",
    "        if raw_questions:\n",
    "            test_questions = validate_questions(raw_questions)\n",
    "            print(f\"‚úÖ Loaded {len(test_questions)} test questions\")\n",
    "            \n",
    "            # Display first few questions as preview\n",
    "            print(\"\\nüìã Preview of loaded questions:\")\n",
    "            for i, q in enumerate(test_questions[:3]):\n",
    "                print(f\"  {i+1}. {q['question']} [{q['customer_type']}]\")\n",
    "            if len(test_questions) > 3:\n",
    "                print(f\"  ... and {len(test_questions) - 3} more questions\")\n",
    "                \n",
    "            # Show distribution\n",
    "            df_preview = pd.DataFrame(test_questions)\n",
    "            print(\"\\nüìä Question Distribution:\")\n",
    "            print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n",
    "            print(f\"  Complexities: {dict(df_preview['complexity'].value_counts())}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå No questions loaded from file\")\n",
    "    else:\n",
    "        print(\"‚ùå Error accessing uploaded file\")\n",
    "        print(f\"   uploaded_file: {uploaded_file is not None}\")\n",
    "        print(f\"   filename: {filename}\")\n",
    "        print(f\"   file_content: {file_content is not None}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please upload a JSON file with test questions.\")\n",
    "    print(\"üí° Use question_generator.ipynb to create test questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review and Edit Questions\n",
    "\n",
    "Review the loaded questions and make any edits before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Question Editor - You can modify questions before processing\n",
      "Edit the questions in the table below, then run the next cell to process them.\n",
      "\n",
      "Current questions (you can edit the JSON below if needed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Why did my premium increase by $200? This is r...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I just had a car accident. What do I need to d...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Can someone explain what a deductible is? I ke...</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How do I add my teenage daughter to my auto po...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been a customer for 15 years and you deni...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What's the difference between comprehensive an...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>My house just flooded - I need emergency assis...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Can I get proof of insurance sent to my phone?...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How do I update my billing information?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>What exactly is an 'act of God' and why isn't ...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>I've been on hold for 45 minutes and keep gett...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>My neighbor's tree fell on my garage during th...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Can I schedule a review of my current coverage...</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Someone broke into my car last night. I need t...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Why am I paying more than my friend for the sa...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>What documents do I need to submit for my wate...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>I don't understand all these terms in my polic...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>My windshield cracked on the highway. Is this ...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>I've been trying to file a claim online for ho...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>When does my policy expire and how do I renew it?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question customer_type  \\\n",
       "0    1  Why did my premium increase by $200? This is r...    frustrated   \n",
       "1    2  I just had a car accident. What do I need to d...        urgent   \n",
       "2    3  Can someone explain what a deductible is? I ke...      confused   \n",
       "3    4  How do I add my teenage daughter to my auto po...        normal   \n",
       "4    5  I've been a customer for 15 years and you deni...    frustrated   \n",
       "5    6  What's the difference between comprehensive an...      confused   \n",
       "6    7  My house just flooded - I need emergency assis...        urgent   \n",
       "7    8  Can I get proof of insurance sent to my phone?...        urgent   \n",
       "8    9            How do I update my billing information?        normal   \n",
       "9   10  What exactly is an 'act of God' and why isn't ...      confused   \n",
       "10  11  I've been on hold for 45 minutes and keep gett...    frustrated   \n",
       "11  12  My neighbor's tree fell on my garage during th...      confused   \n",
       "12  13  Can I schedule a review of my current coverage...        normal   \n",
       "13  14  Someone broke into my car last night. I need t...        urgent   \n",
       "14  15  Why am I paying more than my friend for the sa...    frustrated   \n",
       "15  16  What documents do I need to submit for my wate...        normal   \n",
       "16  17  I don't understand all these terms in my polic...      confused   \n",
       "17  18  My windshield cracked on the highway. Is this ...        urgent   \n",
       "18  19  I've been trying to file a claim online for ho...    frustrated   \n",
       "19  20  When does my policy expire and how do I renew it?        normal   \n",
       "\n",
       "   complexity  \n",
       "0      medium  \n",
       "1      simple  \n",
       "2      simple  \n",
       "3      simple  \n",
       "4     complex  \n",
       "5      medium  \n",
       "6     complex  \n",
       "7      simple  \n",
       "8      simple  \n",
       "9      medium  \n",
       "10     medium  \n",
       "11    complex  \n",
       "12     medium  \n",
       "13    complex  \n",
       "14     medium  \n",
       "15     simple  \n",
       "16    complex  \n",
       "17     medium  \n",
       "18    complex  \n",
       "19     simple  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced: Edit questions as JSON (optional):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215fdf83b26a4849bfce173ebca33324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='[\\n  {\\n    \"id\": 1,\\n    \"question\": \"Why did my premium increase by $200? This is ridiculous‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc657931ee7d44499cd37dd2765ddc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update from JSON', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive question editor\n",
    "if test_questions:\n",
    "    print(\"üìù Question Editor - You can modify questions before processing\")\n",
    "    print(\"Edit the questions in the table below, then run the next cell to process them.\\n\")\n",
    "    \n",
    "    # Convert to DataFrame for easy editing\n",
    "    df = pd.DataFrame(test_questions)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "    if 'customer_type' not in df.columns:\n",
    "        df['customer_type'] = 'normal'\n",
    "    if 'complexity' not in df.columns:\n",
    "        df['complexity'] = 'medium'\n",
    "    \n",
    "    # Display editable table\n",
    "    print(\"Current questions (you can edit the JSON below if needed):\")\n",
    "    display(df)\n",
    "    \n",
    "    # Show JSON for manual editing if needed\n",
    "    questions_json = widgets.Textarea(\n",
    "        value=json.dumps(test_questions, indent=2),\n",
    "        description=\"Questions JSON:\",\n",
    "        layout=widgets.Layout(width='100%', height='200px')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAdvanced: Edit questions as JSON (optional):\")\n",
    "    display(questions_json)\n",
    "    \n",
    "    def update_questions_from_json():\n",
    "        \"\"\"Update questions from the JSON editor\"\"\"\n",
    "        global test_questions\n",
    "        try:\n",
    "            test_questions = json.loads(questions_json.value)\n",
    "            print(\"‚úÖ Questions updated from JSON editor\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing JSON: {e}\")\n",
    "    \n",
    "    # Button to update from JSON\n",
    "    update_btn = widgets.Button(description=\"Update from JSON\")\n",
    "    update_btn.on_click(lambda b: update_questions_from_json())\n",
    "    display(update_btn)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please upload a questions file in the previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Questions Through Chatbot Agent\n",
    "\n",
    "Run the questions through the Chatbot Agent and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Processing 20 questions through Chatbot Agent...\n",
      "Using model: anthropic_general_budget\n",
      "Temperature: 0.7\n",
      "\n",
      "==================================================\n",
      "‚úÖ 20:44:01.711 [INFO    ] factory         | Attempting to create provider: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:44:01.714 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:44:01.717 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:44:01.718 [INFO    ] factory         | Successfully created provider: anthropic_general_budget\n",
      "‚úÖ 20:44:01.719 [INFO    ] chatbot_agent   | Chatbot Agent LLM provider initialized | operation=initialize_llm_provider model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ Chatbot Agent initialized successfully\n",
      "\n",
      "üîÑ Processing question 1: Why did my premium increase by $200? This is ridiculous - I ...\n",
      "‚úÖ 20:44:06.110 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 2: I just had a car accident. What do I need to do right now?...\n",
      "‚úÖ 20:44:10.278 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 3: Can someone explain what a deductible is? I keep seeing this...\n",
      "‚úÖ 20:44:14.127 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 4: How do I add my teenage daughter to my auto policy?...\n",
      "‚úÖ 20:44:20.148 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 5: I've been a customer for 15 years and you denied my claim? I...\n",
      "‚úÖ 20:44:22.676 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 6: What's the difference between comprehensive and collision co...\n",
      "‚úÖ 20:44:27.414 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 7: My house just flooded - I need emergency assistance. Does my...\n",
      "‚úÖ 20:44:31.533 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 8: Can I get proof of insurance sent to my phone? I got pulled ...\n",
      "‚úÖ 20:44:35.713 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 9: How do I update my billing information?...\n",
      "‚úÖ 20:44:38.952 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 10: What exactly is an 'act of God' and why isn't my damage cove...\n",
      "‚úÖ 20:44:44.260 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 11: I've been on hold for 45 minutes and keep getting disconnect...\n",
      "‚úÖ 20:44:50.146 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚ö†Ô∏è Escalation needed (explicit_transfer) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 12: My neighbor's tree fell on my garage during the storm. Which...\n",
      "‚úÖ 20:44:55.053 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 13: Can I schedule a review of my current coverage? I think I mi...\n",
      "‚úÖ 20:44:58.440 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 14: Someone broke into my car last night. I need to file a claim...\n",
      "‚úÖ 20:45:03.390 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 15: Why am I paying more than my friend for the same coverage? W...\n",
      "‚úÖ 20:45:08.729 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 16: What documents do I need to submit for my water damage claim...\n",
      "‚úÖ 20:45:12.985 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 17: I don't understand all these terms in my policy renewal - ca...\n",
      "‚úÖ 20:45:15.300 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 18: My windshield cracked on the highway. Is this covered and ho...\n",
      "‚úÖ 20:45:19.079 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 19: I've been trying to file a claim online for hours but keep g...\n",
      "‚úÖ 20:45:25.072 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (information_request) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 20: When does my policy expire and how do I renew it?...\n",
      "‚úÖ 20:45:27.708 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "==================================================\n",
      "‚úÖ Processing complete! Processed 20 questions.\n",
      "\n",
      "üìä Enhanced Summary:\n",
      "  Total questions: 20\n",
      "  Need more input: 19 (95.0%)\n",
      "    Reasons: {'clarifying_questions': 18, 'information_request': 1}\n",
      "  Need escalation: 1 (5.0%)\n",
      "    Reasons: {'explicit_transfer': 1}\n",
      "  Average confidence: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Process questions through the Chatbot Agent\n",
    "if not test_questions:\n",
    "    print(\"‚ö†Ô∏è No questions to process. Please load or generate questions first.\")\n",
    "else:\n",
    "    print(f\"ü§ñ Processing {len(test_questions)} questions through Chatbot Agent...\")\n",
    "    print(f\"Using model: {agent_preferred_model}\")\n",
    "    print(f\"Temperature: {agent_temperature}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    def analyze_chatbot_response(response: str, confidence: float) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze chatbot response to determine escalation and input needs\n",
    "        \"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Escalation indicators - phrases that suggest referring to human/department\n",
    "        escalation_phrases = [\n",
    "            \"let me transfer you to\",\n",
    "            \"i'll connect you with\",\n",
    "            \"please contact our\",\n",
    "            \"speak with a specialist\",\n",
    "            \"escalate this to\",\n",
    "            \"connect you with the\",\n",
    "            \"transfer to our\",\n",
    "            \"speak with someone from\",\n",
    "            \"you'll need to contact\",\n",
    "            \"reach out to our\",\n",
    "            \"i recommend speaking with\",\n",
    "            \"forward this to\"\n",
    "        ]\n",
    "        \n",
    "        # More input indicators - phrases requesting additional information\n",
    "        input_request_phrases = [\n",
    "            \"could you provide\",\n",
    "            \"can you tell me more about\",\n",
    "            \"what is your\",\n",
    "            \"to better assist you\",\n",
    "            \"i need more information\",\n",
    "            \"could you clarify\",\n",
    "            \"can you specify\",\n",
    "            \"what type of\",\n",
    "            \"please let me know\",\n",
    "            \"can you share\",\n",
    "            \"what specific\",\n",
    "            \"which policy\"\n",
    "                    ]\n",
    "        \n",
    "        # Question indicators (asking user for info to continue conversation)\n",
    "        question_patterns = [\n",
    "            \"could you\",\n",
    "            \"can you\",\n",
    "            \"would you mind\",\n",
    "            \"please provide\",\n",
    "            \"do you have\",\n",
    "            \"what is\",\n",
    "            \"which\",\n",
    "            \"when did\",\n",
    "            \"?\"\n",
    "        ]\n",
    "        \n",
    "        # Check for escalation intent\n",
    "        escalation_detected = any(phrase in response_lower for phrase in escalation_phrases)\n",
    "        \n",
    "        # Check for information requests\n",
    "        input_needed = any(phrase in response_lower for phrase in input_request_phrases)\n",
    "        \n",
    "        # Check if response is primarily asking questions to continue conversation\n",
    "        question_count = response.count('?')\n",
    "        is_asking_questions = question_count > 0 and any(pattern in response_lower for pattern in question_patterns)\n",
    "        \n",
    "        # Confidence-based escalation (very low confidence = likely needs human help)\n",
    "        low_confidence_escalation = confidence < 0.6\n",
    "        \n",
    "        # Final determination with priority logic\n",
    "        needs_escalation = escalation_detected or low_confidence_escalation\n",
    "        # Only flag needs_more_input if NOT escalating (escalation takes priority)\n",
    "        needs_more_input = (input_needed or is_asking_questions) and not needs_escalation\n",
    "        \n",
    "        return {\n",
    "            'needs_escalation': needs_escalation,\n",
    "            'needs_more_input': needs_more_input,\n",
    "            'escalation_reason': 'explicit_transfer' if escalation_detected else 'low_confidence' if low_confidence_escalation else None,\n",
    "            'input_reason': 'information_request' if input_needed else 'clarifying_questions' if is_asking_questions else None,\n",
    "            'question_count': question_count,\n",
    "            'confidence_score': confidence,\n",
    "            'analysis_details': {\n",
    "                'escalation_detected': escalation_detected,\n",
    "                'input_needed': input_needed,\n",
    "                'is_asking_questions': is_asking_questions,\n",
    "                'low_confidence': low_confidence_escalation\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create a simple mock context provider for testing\n",
    "    class MockContextProvider:\n",
    "        \"\"\"Simple mock context provider for testing\"\"\"\n",
    "        \n",
    "        def get_context_summary(self, user_id: str, session_id: str) -> dict:\n",
    "            \"\"\"Return a simple mock context summary\"\"\"\n",
    "            return {\n",
    "                'entries_count': 0,\n",
    "                'type_breakdown': {},\n",
    "                'recent_queries': [],\n",
    "                'escalation_count': 0,\n",
    "                'last_activity': None\n",
    "            }\n",
    "        \n",
    "        def save_context_entry(self, entry) -> bool:\n",
    "            \"\"\"Mock save method\"\"\"\n",
    "            return True\n",
    "        \n",
    "        def get_recent_context(self, user_id: str, session_id: str, limit: int = 10) -> list:\n",
    "            \"\"\"Mock recent context method\"\"\"\n",
    "            return []\n",
    "    \n",
    "    # Initialize the Chatbot Agent\n",
    "    try:\n",
    "        # Create config manager with the correct config directory\n",
    "        config_manager = AgentConfigManager(config_dir='/workspace/config')\n",
    "        \n",
    "        # Create a mock context provider for testing\n",
    "        context_provider = MockContextProvider()\n",
    "        \n",
    "        # Initialize Chatbot Agent\n",
    "        chatbot_agent = ChatbotAgentNode(config_manager, context_provider)\n",
    "        \n",
    "        print(\"‚úÖ Chatbot Agent initialized successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Chatbot Agent: {e}\")\n",
    "        print(\"Continuing with mock responses for demonstration...\")\n",
    "        chatbot_agent = None\n",
    "    \n",
    "    # Process each question\n",
    "    results = []\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        question_id = question_data.get('id', i + 1)\n",
    "        question_text = question_data.get('question', '')\n",
    "        customer_type = question_data.get('customer_type', 'normal')\n",
    "        complexity = question_data.get('complexity', 'medium')\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing question {question_id}: {question_text[:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            if chatbot_agent:\n",
    "                # Create state for the Chatbot Agent\n",
    "                from datetime import datetime\n",
    "                state = {\n",
    "                    'query': question_text,\n",
    "                    'user_id': 'test_user',\n",
    "                    'session_id': f'test_session_{i}',\n",
    "                    'query_id': f'query_{question_id}',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'messages': []\n",
    "                }\n",
    "                \n",
    "                # Process through agent using __call__ method\n",
    "                response_state = chatbot_agent(state)\n",
    "                \n",
    "                answer = response_state.get('ai_response', 'No response generated')\n",
    "                confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n",
    "                \n",
    "                # Use improved flag analysis\n",
    "                flag_analysis = analyze_chatbot_response(answer, confidence)\n",
    "                needs_escalation = flag_analysis['needs_escalation']\n",
    "                needs_more_input = flag_analysis['needs_more_input']\n",
    "                \n",
    "            else:\n",
    "                # Mock processing for demonstration\n",
    "                import random\n",
    "                answer = f\"Thank you for your question about {question_text[:30]}... I'd be happy to help you with that. [This is a mock response for demonstration]\"\n",
    "                confidence = random.uniform(0.6, 0.95)\n",
    "                \n",
    "                # Use improved flag analysis even for mock responses\n",
    "                flag_analysis = analyze_chatbot_response(answer, confidence)\n",
    "                needs_escalation = flag_analysis['needs_escalation']\n",
    "                needs_more_input = flag_analysis['needs_more_input']\n",
    "            \n",
    "            # Create result entry with enhanced analysis\n",
    "            result = {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'ai_answer': answer,\n",
    "                'confidence_score': confidence,\n",
    "                'needs_escalation': needs_escalation,\n",
    "                'needs_more_input': needs_more_input,\n",
    "                'escalation_reason': flag_analysis.get('escalation_reason'),\n",
    "                'input_reason': flag_analysis.get('input_reason'),\n",
    "                'question_count': flag_analysis.get('question_count', 0),\n",
    "                'analysis_details': flag_analysis.get('analysis_details', {}),\n",
    "                'processing_time': datetime.now().isoformat(),\n",
    "                'model_used': agent_preferred_model,\n",
    "                'temperature': agent_temperature\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Show progress with enhanced status\n",
    "            if needs_more_input:\n",
    "                reason = flag_analysis.get('input_reason', 'unknown')\n",
    "                status = f\"üîÑ Needs more input ({reason})\"\n",
    "            elif needs_escalation:\n",
    "                reason = flag_analysis.get('escalation_reason', 'unknown')\n",
    "                status = f\"‚ö†Ô∏è Escalation needed ({reason})\"\n",
    "            else:\n",
    "                status = \"‚úÖ Complete\"\n",
    "            \n",
    "            print(f\"   {status} (confidence: {confidence:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing question: {e}\")\n",
    "            # Add error result\n",
    "            result = {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'ai_answer': f\"Error processing question: {e}\",\n",
    "                'confidence_score': 0.0,\n",
    "                'needs_escalation': True,\n",
    "                'needs_more_input': False,\n",
    "                'escalation_reason': 'processing_error',\n",
    "                'input_reason': None,\n",
    "                'question_count': 0,\n",
    "                'analysis_details': {'error': True},\n",
    "                'processing_time': datetime.now().isoformat(),\n",
    "                'model_used': agent_preferred_model,\n",
    "                'temperature': agent_temperature,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"‚úÖ Processing complete! Processed {len(results)} questions.\")\n",
    "    \n",
    "    # Enhanced summary statistics\n",
    "    total_questions = len(results)\n",
    "    needs_more_input_count = sum(1 for r in results if r['needs_more_input'])\n",
    "    needs_escalation_count = sum(1 for r in results if r['needs_escalation'])\n",
    "    avg_confidence = sum(r['confidence_score'] for r in results) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "    # Breakdown by escalation reason\n",
    "    escalation_reasons = {}\n",
    "    input_reasons = {}\n",
    "    for r in results:\n",
    "        if r['needs_escalation'] and r['escalation_reason']:\n",
    "            escalation_reasons[r['escalation_reason']] = escalation_reasons.get(r['escalation_reason'], 0) + 1\n",
    "        if r['needs_more_input'] and r['input_reason']:\n",
    "            input_reasons[r['input_reason']] = input_reasons.get(r['input_reason'], 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Enhanced Summary:\")\n",
    "    print(f\"  Total questions: {total_questions}\")\n",
    "    print(f\"  Need more input: {needs_more_input_count} ({needs_more_input_count/total_questions*100:.1f}%)\")\n",
    "    if input_reasons:\n",
    "        print(f\"    Reasons: {input_reasons}\")\n",
    "    print(f\"  Need escalation: {needs_escalation_count} ({needs_escalation_count/total_questions*100:.1f}%)\")\n",
    "    if escalation_reasons:\n",
    "        print(f\"    Reasons: {escalation_reasons}\")\n",
    "    print(f\"  Average confidence: {avg_confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Complete Conversations with Customer AI\n",
    "\n",
    "Continue conversations between customer AI and chatbot AI until natural resolution or escalation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Starting full conversations for 20 questions...\n",
      "This will simulate realistic customer-chatbot interactions until resolution.\n",
      "\n",
      "üó£Ô∏è Starting full conversation for question 1\n",
      "   Customer: frustrated, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:50:07.385 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:50:07.386 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:50:13.781 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:15.650 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:50:19.520 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 1: escalation_request (2 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 2\n",
      "   Customer: urgent, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:50:19.606 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:50:19.607 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:50:23.781 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:25.398 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:50:29.942 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:31.586 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:50:34.202 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 3 turns\n",
      "Question 2: satisfied (3 turns, satisfaction: 0.85)\n",
      "üó£Ô∏è Starting full conversation for question 3\n",
      "   Customer: confused, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:50:34.297 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:50:34.298 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:50:37.786 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:40.464 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:50:45.745 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:47.245 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:50:51.316 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 20:50:54.023 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 4 turns\n",
      "Question 3: satisfied (4 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 4\n",
      "   Customer: normal, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:50:54.108 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:50:54.109 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:50:57.243 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:50:58.541 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:51:02.338 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 2 turns\n",
      "Question 4: satisfied (2 turns, satisfaction: 0.85)\n",
      "üó£Ô∏è Starting full conversation for question 5\n",
      "   Customer: frustrated, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:51:02.423 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:51:02.424 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:51:05.686 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:51:08.230 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:51:15.259 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 5: escalation_request (2 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 6\n",
      "   Customer: confused, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:51:15.348 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:51:15.349 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:51:19.568 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:51:21.512 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:51:25.196 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:51:26.726 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:51:31.429 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:51:33.351 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 20:51:36.828 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 4 turns\n",
      "Question 6: satisfied (4 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 7\n",
      "   Customer: urgent, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:51:36.921 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:51:36.922 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:51:41.236 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:51:42.630 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:51:46.905 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:51:51.749 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 20:51:54.174 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   üîÑ Chatbot escalating: explicit_transfer\n",
      "   üìã Conversation complete: escalation in 4 turns\n",
      "Question 7: escalation (4 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 8\n",
      "   Customer: urgent, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:51:54.259 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:51:54.260 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:51:58.103 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 8: satisfied (1 turns, satisfaction: 0.85)\n",
      "üó£Ô∏è Starting full conversation for question 9\n",
      "   Customer: normal, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:51:58.188 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:51:58.189 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:00.420 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:52:01.744 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:52:05.537 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:52:07.066 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:52:10.209 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 9: escalation_request (3 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 10\n",
      "   Customer: confused, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:52:10.293 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:52:10.294 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:15.391 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 10: satisfied (1 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 11\n",
      "   Customer: frustrated, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:52:15.476 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:52:15.477 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:22.460 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:52:24.542 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:52:27.301 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:52:28.874 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 11: escalation_request (3 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 12\n",
      "   Customer: confused, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:52:28.959 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:52:28.960 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:33.500 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 12: satisfied (1 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 13\n",
      "   Customer: normal, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:52:33.586 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:52:33.587 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:37.096 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 13: satisfied (1 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 14\n",
      "   Customer: urgent, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:52:37.183 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:52:37.183 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:52:42.613 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:52:44.557 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:52:51.781 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:52:54.861 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:53:01.057 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 20:53:03.353 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 4 turns\n",
      "Question 14: escalation_request (4 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 15\n",
      "   Customer: frustrated, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:03.436 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:03.437 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:07.842 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:53:10.028 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:53:14.161 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 15: escalation_request (2 turns, satisfaction: 0.45)\n",
      "üó£Ô∏è Starting full conversation for question 16\n",
      "   Customer: normal, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:14.243 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:14.244 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:18.831 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:53:22.678 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 16: satisfied (1 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 17\n",
      "   Customer: confused, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:22.767 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:22.768 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:25.010 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:53:26.363 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:53:28.158 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:53:30.252 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 20:53:35.238 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 4 turns\n",
      "Question 17: satisfied (4 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 18\n",
      "   Customer: urgent, Complexity: medium, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:35.322 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:35.323 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:40.369 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 18: satisfied (1 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 19\n",
      "   Customer: frustrated, Complexity: complex, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:40.451 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:40.452 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:43.391 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:53:45.726 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:53:50.453 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:53:54.536 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 19: escalation_request (3 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 20\n",
      "   Customer: normal, Complexity: simple, Model: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 20:53:54.628 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 20:53:54.629 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 20:53:57.517 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:53:58.707 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 20:54:01.871 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 20:54:03.175 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 20:54:06.722 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 20: escalation_request (3 turns, satisfaction: 0.60)\n",
      "\n",
      "‚úÖ All conversations completed!\n",
      "\n",
      "üìä Conversation Summary:\n",
      "  Completed conversations: 20/20\n",
      "  Customer satisfaction: 11 (55.0%)\n",
      "  Escalations: 9 (45.0%)\n",
      "  Average turns per conversation: 2.5\n",
      "  Average customer satisfaction: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Complete conversations with customer AI simulation\n",
    "if 'results' in locals() and results:\n",
    "    import random  # Add missing import\n",
    "    \n",
    "    def create_customer_ai_simulator(customer_type, complexity, model_name='anthropic_general_budget'):\n",
    "        \"\"\"Create a customer AI simulator based on customer profile\"\"\"\n",
    "        \n",
    "        # Define customer personas\n",
    "        customer_personas = {\n",
    "            'frustrated': {\n",
    "                'style': 'Impatient, demanding, may use caps or exclamation points. Wants quick resolution.',\n",
    "                'follow_up_tendency': 'high',\n",
    "                'satisfaction_threshold': 0.8,\n",
    "                'escalation_patience': 2  # Will demand escalation after 2 responses if not satisfied\n",
    "            },\n",
    "            'urgent': {\n",
    "                'style': 'Time-sensitive, focused on immediate action. Professional but hurried.',\n",
    "                'follow_up_tendency': 'high', \n",
    "                'satisfaction_threshold': 0.75,\n",
    "                'escalation_patience': 3\n",
    "            },\n",
    "            'confused': {\n",
    "                'style': 'Asks many clarifying questions, needs simple explanations. Polite but persistent.',\n",
    "                'follow_up_tendency': 'very_high',\n",
    "                'satisfaction_threshold': 0.7,\n",
    "                'escalation_patience': 4\n",
    "            },\n",
    "            'normal': {\n",
    "                'style': 'Professional, patient, reasonable expectations.',\n",
    "                'follow_up_tendency': 'medium',\n",
    "                'satisfaction_threshold': 0.65,\n",
    "                'escalation_patience': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Define complexity behaviors\n",
    "        complexity_behaviors = {\n",
    "            'simple': {\n",
    "                'satisfaction_boost': 0.1,  # Easier to satisfy\n",
    "                'max_turns': 3,\n",
    "                'question_depth': 'surface-level'\n",
    "            },\n",
    "            'medium': {\n",
    "                'satisfaction_boost': 0.0,\n",
    "                'max_turns': 5,\n",
    "                'question_depth': 'moderate detail'\n",
    "            },\n",
    "            'complex': {\n",
    "                'satisfaction_boost': -0.1,  # Harder to satisfy\n",
    "                'max_turns': 7,\n",
    "                'question_depth': 'detailed technical'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        persona = customer_personas.get(customer_type, customer_personas['normal'])\n",
    "        behavior = complexity_behaviors.get(complexity, complexity_behaviors['medium'])\n",
    "        \n",
    "        class CustomerAISimulator:\n",
    "            def __init__(self, customer_type, complexity):\n",
    "                self.customer_type = customer_type  # Store customer type as instance variable\n",
    "                self.complexity = complexity  # Store complexity as instance variable\n",
    "                self.persona = persona\n",
    "                self.behavior = behavior\n",
    "                self.turn_count = 0\n",
    "                self.satisfaction_level = 0.0\n",
    "                self.conversation_history = []\n",
    "                \n",
    "                # Create LLM for customer simulation\n",
    "                try:\n",
    "                    provider_factory = LLMProviderFactory()\n",
    "                    self.llm = provider_factory.create_provider(model_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create customer AI simulator LLM: {e}\")\n",
    "                    self.llm = None\n",
    "            \n",
    "            def generate_response(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate customer response to chatbot\"\"\"\n",
    "                self.turn_count += 1\n",
    "                \n",
    "                # Analyze satisfaction with chatbot response\n",
    "                satisfaction_score = self._analyze_satisfaction(chatbot_response)\n",
    "                self.satisfaction_level = satisfaction_score\n",
    "                \n",
    "                # Check if conversation should end\n",
    "                is_satisfied = satisfaction_score >= (self.persona['satisfaction_threshold'] + self.behavior['satisfaction_boost'])\n",
    "                is_escalating = self.turn_count >= self.persona['escalation_patience'] and satisfaction_score < 0.6\n",
    "                is_max_turns = self.turn_count >= self.behavior['max_turns']\n",
    "                \n",
    "                if is_satisfied:\n",
    "                    return self._generate_satisfaction_response()\n",
    "                elif is_escalating:\n",
    "                    return self._generate_escalation_request()\n",
    "                elif is_max_turns:\n",
    "                    return self._generate_final_response()\n",
    "                else:\n",
    "                    return self._generate_follow_up_question(chatbot_response, original_question)\n",
    "            \n",
    "            def _analyze_satisfaction(self, response):\n",
    "                \"\"\"Analyze how satisfied the customer would be with the response\"\"\"\n",
    "                response_lower = response.lower()\n",
    "                \n",
    "                # Positive indicators\n",
    "                positive_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['specifically', 'here\\'s how', 'i can help', 'let me explain']):\n",
    "                    positive_score += 0.2\n",
    "                if any(phrase in response_lower for phrase in ['immediately', 'right away', 'quickly']):\n",
    "                    positive_score += 0.15\n",
    "                if len(response) > 100:  # Detailed response\n",
    "                    positive_score += 0.1\n",
    "                \n",
    "                # Negative indicators  \n",
    "                negative_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['i need more information', 'could you provide', 'what type']):\n",
    "                    negative_score += 0.3  # Asking for more info is frustrating\n",
    "                if any(phrase in response_lower for phrase in ['unfortunately', 'however', 'but']):\n",
    "                    negative_score += 0.1\n",
    "                if response.count('?') > 2:  # Too many questions back\n",
    "                    negative_score += 0.2\n",
    "                \n",
    "                # Base satisfaction varies by customer type - use self.customer_type\n",
    "                base_satisfaction = {\n",
    "                    'frustrated': 0.3,\n",
    "                    'urgent': 0.4, \n",
    "                    'confused': 0.5,\n",
    "                    'normal': 0.6\n",
    "                }.get(self.customer_type, 0.5)\n",
    "                \n",
    "                return max(0.0, min(1.0, base_satisfaction + positive_score - negative_score))\n",
    "            \n",
    "            def _generate_satisfaction_response(self):\n",
    "                \"\"\"Generate a satisfied customer response\"\"\"\n",
    "                satisfied_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"Okay, that makes sense. Thanks for clearing that up.\",\n",
    "                        \"Finally! Thank you for the explanation.\",\n",
    "                        \"Alright, I understand now. That helps.\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"Perfect, that's exactly what I needed to know. Thank you!\",\n",
    "                        \"Great, I'll do that right away. Thanks for the quick help!\",\n",
    "                        \"Excellent, that answers my question. Much appreciated.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"Oh I see! That makes much more sense now. Thank you for explaining it so clearly.\",\n",
    "                        \"Thank you! That explanation really helped me understand.\",\n",
    "                        \"Perfect! Now I get it. I really appreciate your patience.\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"Thank you for the helpful information. That resolves my question.\",\n",
    "                        \"Great, that's exactly what I needed to know. Thanks!\",\n",
    "                        \"Perfect, I understand now. Thank you for your assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(satisfied_responses.get(self.customer_type, satisfied_responses['normal']))\n",
    "            \n",
    "            def _generate_escalation_request(self):\n",
    "                \"\"\"Generate escalation request\"\"\"\n",
    "                escalation_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"This isn't working. I need to speak to someone who can actually help me!\",\n",
    "                        \"I'm getting nowhere with this. Transfer me to a supervisor NOW!\",\n",
    "                        \"Enough! Get me a human who knows what they're doing!\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"I need this resolved immediately. Can you transfer me to someone who can handle this urgently?\",\n",
    "                        \"Time is critical here. I need to speak with a specialist right away.\",\n",
    "                        \"This is urgent - please connect me with someone who can resolve this now.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"I'm still really confused. Could you please connect me with someone who can walk me through this step by step?\",\n",
    "                        \"I don't think I'm understanding this correctly. Can I speak with someone who can explain this more simply?\",\n",
    "                        \"I'm getting more confused. Could you transfer me to someone who specializes in helping customers like me?\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"I think I need to speak with a specialist about this. Could you please transfer me?\",\n",
    "                        \"This seems like it might require human expertise. Can you connect me with the right department?\",\n",
    "                        \"I'd like to speak with someone who can provide more detailed assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(escalation_responses.get(self.customer_type, escalation_responses['normal']))\n",
    "            \n",
    "            def _generate_final_response(self):\n",
    "                \"\"\"Generate final response when max turns reached\"\"\"\n",
    "                final_responses = {\n",
    "                    'frustrated': \"Look, I've been going in circles here. Just transfer me to someone else.\",\n",
    "                    'urgent': \"I've spent too much time on this already. I need to speak with a human.\",\n",
    "                    'confused': \"I'm still not clear on this. I think I need to talk to someone in person.\",\n",
    "                    'normal': \"I think this might be beyond what we can resolve here. Could you transfer me to the appropriate department?\"\n",
    "                }\n",
    "                \n",
    "                return final_responses.get(self.customer_type, final_responses['normal'])\n",
    "            \n",
    "            def _generate_follow_up_question(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate intelligent follow-up question using AI if available\"\"\"\n",
    "                \n",
    "                if self.llm:\n",
    "                    # Use AI to generate contextual follow-up - FIX: Use correct method signature\n",
    "                    prompt = f'''You are a {self.customer_type} customer with a {self.complexity} question about insurance. \n",
    "                    \n",
    "Your personality: {self.persona['style']}\n",
    "Question complexity: {self.behavior['question_depth']}\n",
    "Turn {self.turn_count} of conversation.\n",
    "\n",
    "Original question: {original_question}\n",
    "Chatbot's response: {chatbot_response}\n",
    "\n",
    "Generate a follow-up response that a {self.customer_type} customer would realistically ask. Be specific to the chatbot's response and maintain the personality. Make it {self.behavior['question_depth']} in nature.\n",
    "\n",
    "Respond as the customer would (2-3 sentences max):'''\n",
    "                    \n",
    "                    try:\n",
    "                        # FIX: Use correct method signature (prompt, system_prompt)\n",
    "                        response = self.llm.generate_response(prompt)\n",
    "                        \n",
    "                        if response and len(response.strip()) > 10:\n",
    "                            return response.strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: AI follow-up generation failed: {e}\")\n",
    "                \n",
    "                # Fallback to template-based responses\n",
    "                return self._generate_template_follow_up(chatbot_response, original_question)\n",
    "            \n",
    "            def _generate_template_follow_up(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate template-based follow-up questions\"\"\"\n",
    "                \n",
    "                # Analyze what the chatbot asked for\n",
    "                response_lower = chatbot_response.lower()\n",
    "                \n",
    "                if 'policy number' in response_lower:\n",
    "                    return f\"My policy number is POL-{random.randint(100000, 999999)}. Now what?\"\n",
    "                elif 'claim number' in response_lower:\n",
    "                    return f\"It's claim #{random.randint(10000, 99999)}. What's the next step?\"\n",
    "                elif 'type of coverage' in response_lower or 'which coverage' in response_lower:\n",
    "                    coverage_types = ['comprehensive', 'collision', 'liability', 'homeowners', 'auto']\n",
    "                    return f\"I have {random.choice(coverage_types)} coverage. Does that help?\"\n",
    "                elif 'when did' in response_lower or 'what date' in response_lower:\n",
    "                    return \"This happened yesterday around 3 PM. What do I do now?\"\n",
    "                elif 'how much' in response_lower or 'what amount' in response_lower:\n",
    "                    return f\"It's about ${random.randint(500, 5000)} in damage. What's next?\"\n",
    "                elif 'documents' in response_lower or 'paperwork' in response_lower:\n",
    "                    return \"I have photos and a police report. How do I submit them?\"\n",
    "                else:\n",
    "                    # Generic follow-ups based on customer type\n",
    "                    generic_followups = {\n",
    "                        'frustrated': \"That doesn't really answer my question. Can you be more specific?\",\n",
    "                        'urgent': \"Okay, but what do I do RIGHT NOW? This is time-sensitive!\",\n",
    "                        'confused': \"I'm still not sure I understand. Can you explain it differently?\",\n",
    "                        'normal': \"Could you provide more specific steps on what I should do next?\"\n",
    "                    }\n",
    "                    return generic_followups.get(self.customer_type, generic_followups['normal'])\n",
    "        \n",
    "        return CustomerAISimulator(customer_type, complexity)  # Pass parameters to constructor\n",
    "    \n",
    "    def conduct_full_conversation(question_data, max_conversation_turns=8):\n",
    "        \"\"\"Conduct a full conversation between customer AI and chatbot until resolution\"\"\"\n",
    "        \n",
    "        question_id = question_data.get('id', 1)\n",
    "        question_text = question_data.get('question', '')\n",
    "        customer_type = question_data.get('customer_type', 'normal')\n",
    "        complexity = question_data.get('complexity', 'medium')\n",
    "        \n",
    "        # Get the model from questions file or use default\n",
    "        model_to_use = question_data.get('model', agent_preferred_model)\n",
    "        \n",
    "        print(f\"üó£Ô∏è Starting full conversation for question {question_id}\")\n",
    "        print(f\"   Customer: {customer_type}, Complexity: {complexity}, Model: {model_to_use}\")\n",
    "        \n",
    "        # Create customer AI simulator\n",
    "        try:\n",
    "            customer_ai = create_customer_ai_simulator(customer_type, complexity, model_to_use)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error creating customer AI: {e}\")\n",
    "            return {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'error': f\"Failed to create customer AI: {e}\",\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "        \n",
    "        # Initialize conversation\n",
    "        conversation_history = []\n",
    "        current_query = question_text\n",
    "        turn_number = 1\n",
    "        \n",
    "        # Conversation loop\n",
    "        while turn_number <= max_conversation_turns:\n",
    "            print(f\"   Turn {turn_number}: Processing...\")\n",
    "            \n",
    "            try:\n",
    "                if chatbot_agent:\n",
    "                    # Create state for chatbot\n",
    "                    state = {\n",
    "                        'query': current_query,\n",
    "                        'user_id': 'test_user',\n",
    "                        'session_id': f'conversation_session_{question_id}',\n",
    "                        'query_id': f'query_{question_id}_turn_{turn_number}',\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'messages': []  # Start with empty messages for each turn\n",
    "                    }\n",
    "                    \n",
    "                    # Get chatbot response\n",
    "                    response_state = chatbot_agent(state)\n",
    "                    chatbot_response = response_state.get('ai_response', 'No response generated')\n",
    "                    confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n",
    "                    \n",
    "                else:\n",
    "                    # Mock chatbot response\n",
    "                    chatbot_response = f\"Thank you for your question. Let me help you with that... [Mock response turn {turn_number}]\"\n",
    "                    confidence = 0.8\n",
    "                \n",
    "                # Add to conversation history\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'chatbot_response': chatbot_response,\n",
    "                    'confidence': confidence,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                \n",
    "                # Analyze chatbot response for escalation\n",
    "                response_analysis = analyze_chatbot_response(chatbot_response, confidence)\n",
    "                \n",
    "                # Check if chatbot is escalating\n",
    "                if response_analysis['needs_escalation']:\n",
    "                    print(f\"   üîÑ Chatbot escalating: {response_analysis['escalation_reason']}\")\n",
    "                    conversation_turn['chatbot_action'] = 'escalation'\n",
    "                    conversation_turn['escalation_reason'] = response_analysis['escalation_reason']\n",
    "                    break\n",
    "                \n",
    "                # Generate customer response using AI simulator\n",
    "                try:\n",
    "                    customer_response = customer_ai.generate_response(chatbot_response, question_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error generating customer response: {e}\")\n",
    "                    customer_response = \"I'm having trouble understanding this. Can you help me differently?\"\n",
    "                \n",
    "                # Add customer response to turn\n",
    "                conversation_turn['customer_response'] = customer_response\n",
    "                conversation_turn['customer_satisfaction'] = customer_ai.satisfaction_level\n",
    "                \n",
    "                # Check if customer is satisfied (conversation ends)\n",
    "                if any(phrase in customer_response.lower() for phrase in ['thank you', 'that helps', 'perfect', 'great', 'excellent', 'makes sense']):\n",
    "                    if customer_ai.satisfaction_level >= customer_ai.persona['satisfaction_threshold']:\n",
    "                        print(f\"   ‚úÖ Customer satisfied!\")\n",
    "                        conversation_turn['customer_action'] = 'satisfied'\n",
    "                        break\n",
    "                \n",
    "                # Check if customer is requesting escalation\n",
    "                if any(phrase in customer_response.lower() for phrase in ['transfer', 'supervisor', 'specialist', 'human', 'someone else']):\n",
    "                    print(f\"   ‚¨ÜÔ∏è Customer requesting escalation\")\n",
    "                    conversation_turn['customer_action'] = 'escalation_request'\n",
    "                    break\n",
    "                \n",
    "                # Prepare for next turn\n",
    "                current_query = customer_response\n",
    "                turn_number += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error in turn {turn_number}: {e}\")\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                break\n",
    "        \n",
    "        # Determine final outcome\n",
    "        final_turn = conversation_history[-1] if conversation_history else {}\n",
    "        final_outcome = final_turn.get('customer_action', final_turn.get('chatbot_action', 'conversation_incomplete'))\n",
    "        \n",
    "        conversation_result = {\n",
    "            'id': question_id,\n",
    "            'original_question': question_text,\n",
    "            'customer_type': customer_type,\n",
    "            'complexity': complexity,\n",
    "            'model_used': model_to_use,\n",
    "            'conversation_history': conversation_history,\n",
    "            'total_turns': len(conversation_history),\n",
    "            'final_outcome': final_outcome,\n",
    "            'customer_satisfaction': customer_ai.satisfaction_level if customer_ai else 0.0,\n",
    "            'processing_time': datetime.now().isoformat(),\n",
    "            'conversation_complete': final_outcome in ['satisfied', 'escalation', 'escalation_request']\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìã Conversation complete: {final_outcome} in {len(conversation_history)} turns\")\n",
    "        return conversation_result\n",
    "    \n",
    "    # Process conversations for all questions\n",
    "    print(f\"ü§ñ Starting full conversations for {len(test_questions)} questions...\")\n",
    "    print(f\"This will simulate realistic customer-chatbot interactions until resolution.\\n\")\n",
    "    \n",
    "    conversation_results = []\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        try:\n",
    "            conversation_result = conduct_full_conversation(question_data)\n",
    "            conversation_results.append(conversation_result)\n",
    "            \n",
    "            # Show progress\n",
    "            outcome = conversation_result['final_outcome']\n",
    "            turns = conversation_result['total_turns']\n",
    "            satisfaction = conversation_result.get('customer_satisfaction', 0)\n",
    "            \n",
    "            print(f\"Question {question_data['id']}: {outcome} ({turns} turns, satisfaction: {satisfaction:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing conversation {question_data['id']}: {e}\")\n",
    "            # Add error result\n",
    "            error_result = {\n",
    "                'id': question_data['id'],\n",
    "                'original_question': question_data.get('question', ''),\n",
    "                'customer_type': question_data.get('customer_type', 'normal'),\n",
    "                'complexity': question_data.get('complexity', 'medium'),\n",
    "                'error': str(e),\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "            conversation_results.append(error_result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ All conversations completed!\")\n",
    "    \n",
    "    # Update results with conversation data\n",
    "    results = conversation_results\n",
    "    \n",
    "    # Summary statistics\n",
    "    completed_conversations = [r for r in results if r.get('conversation_complete', False)]\n",
    "    satisfied_customers = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    avg_turns = sum(r.get('total_turns', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    avg_satisfaction = sum(r.get('customer_satisfaction', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    \n",
    "    print(f\"\\nüìä Conversation Summary:\")\n",
    "    print(f\"  Completed conversations: {len(completed_conversations)}/{len(results)}\")\n",
    "    print(f\"  Customer satisfaction: {len(satisfied_customers)} ({len(satisfied_customers)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Escalations: {len(escalated_conversations)} ({len(escalated_conversations)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Average turns per conversation: {avg_turns:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {avg_satisfaction:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please load questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results and Settings\n",
    "\n",
    "Save the results and configuration settings to files with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Conversation results exported to: /workspace/notebooks/experiment_runs/chatbot_agent_output_20250719_2054.json\n",
      "‚öôÔ∏è Settings exported to: /workspace/notebooks/experiment_runs/chatbot_agent_settings_20250719_2054.json\n",
      "\n",
      "üìä Export Summary:\n",
      "  Timestamp: 20250719_2054\n",
      "  Results file: chatbot_agent_output_20250719_2054.json\n",
      "  Settings file: chatbot_agent_settings_20250719_2054.json\n",
      "  Total conversations: 20\n",
      "  Completed conversations: 20\n",
      "  Satisfaction rate: 55.0%\n",
      "  Escalation rate: 45.0%\n",
      "  Average turns per conversation: 2.5\n",
      "  Average customer satisfaction: 0.67\n",
      "  Files saved to: /workspace/notebooks/experiment_runs\n"
     ]
    }
   ],
   "source": [
    "# Export conversation results and settings\n",
    "if 'results' in locals() and results:\n",
    "    # Create timestamp for filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    # Create experiment_runs directory if it doesn't exist\n",
    "    experiment_dir = Path('/workspace/notebooks/experiment_runs')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export results with full conversation data\n",
    "    results_filename = f\"chatbot_agent_output_{timestamp}.json\"\n",
    "    results_path = experiment_dir / results_filename\n",
    "    \n",
    "    # Create comprehensive export with conversation analysis\n",
    "    export_data = {\n",
    "        'experiment_metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'agent_type': 'chatbot_agent',\n",
    "            'experiment_type': 'full_conversation_simulation',\n",
    "            'total_questions': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'generation_model': agent_preferred_model,\n",
    "            'generation_timestamp': timestamp\n",
    "        },\n",
    "        'conversation_results': results,\n",
    "        'summary_statistics': {\n",
    "            'total_conversations': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'satisfied_customers': len([r for r in results if r.get('final_outcome') == 'satisfied']),\n",
    "            'escalated_conversations': len([r for r in results if 'escalation' in r.get('final_outcome', '')]),\n",
    "            'error_conversations': len([r for r in results if r.get('final_outcome') == 'error']),\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in results) / len(results) if results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in results) / len(results) if results else 0,\n",
    "            'satisfaction_rate': len([r for r in results if r.get('final_outcome') == 'satisfied']) / len(results) * 100 if results else 0,\n",
    "            'escalation_rate': len([r for r in results if 'escalation' in r.get('final_outcome', '')]) / len(results) * 100 if results else 0\n",
    "        },\n",
    "        'conversation_analysis': {\n",
    "            'by_customer_type': {},\n",
    "            'by_complexity': {},\n",
    "            'by_outcome': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Analyze by customer type\n",
    "    customer_types = list(set(r.get('customer_type', 'unknown') for r in results))\n",
    "    for ctype in customer_types:\n",
    "        ctype_results = [r for r in results if r.get('customer_type') == ctype]\n",
    "        export_data['conversation_analysis']['by_customer_type'][ctype] = {\n",
    "            'count': len(ctype_results),\n",
    "            'satisfaction_rate': len([r for r in ctype_results if r.get('final_outcome') == 'satisfied']) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'escalation_rate': len([r for r in ctype_results if 'escalation' in r.get('final_outcome', '')]) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by complexity\n",
    "    complexities = list(set(r.get('complexity', 'unknown') for r in results))\n",
    "    for complexity in complexities:\n",
    "        complexity_results = [r for r in results if r.get('complexity') == complexity]\n",
    "        export_data['conversation_analysis']['by_complexity'][complexity] = {\n",
    "            'count': len(complexity_results),\n",
    "            'satisfaction_rate': len([r for r in complexity_results if r.get('final_outcome') == 'satisfied']) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'escalation_rate': len([r for r in complexity_results if 'escalation' in r.get('final_outcome', '')]) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by outcome\n",
    "    outcomes = list(set(r.get('final_outcome', 'unknown') for r in results))\n",
    "    for outcome in outcomes:\n",
    "        outcome_results = [r for r in results if r.get('final_outcome') == outcome]\n",
    "        export_data['conversation_analysis']['by_outcome'][outcome] = {\n",
    "            'count': len(outcome_results),\n",
    "            'percentage': len(outcome_results) / len(results) * 100 if results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0\n",
    "        }\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Conversation results exported to: {results_path}\")\n",
    "    \n",
    "    # Export settings\n",
    "    settings_filename = f\"chatbot_agent_settings_{timestamp}.json\"\n",
    "    settings_path = experiment_dir / settings_filename\n",
    "    \n",
    "    # Get uploaded filename safely\n",
    "    uploaded_filename = 'unknown'\n",
    "    if file_upload.value:\n",
    "        if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = file_upload.value[0]['name']\n",
    "        elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = list(file_upload.value.values())[0]['metadata']['name']\n",
    "    \n",
    "    settings_export = {\n",
    "        'experiment_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'agent_type': 'chatbot_agent',\n",
    "            'experiment_type': 'full_conversation_simulation',\n",
    "            'total_questions': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'input_file': uploaded_filename\n",
    "        },\n",
    "        'model_settings': {\n",
    "            'preferred_model': agent_preferred_model,\n",
    "            'fallback_models': agent_fallback_models,\n",
    "            'temperature': agent_temperature,\n",
    "            'max_tokens': agent_max_tokens,\n",
    "            'timeout': agent_timeout\n",
    "        },\n",
    "        'behavior_settings': {\n",
    "            'response_style': agent_response_style,\n",
    "            'context_integration': agent_context_integration,\n",
    "            'personalization': agent_personalization,\n",
    "            'prompt_style': agent_prompt_style,\n",
    "            'include_empathy': agent_include_empathy,\n",
    "            'customer_focus': agent_customer_focus\n",
    "        },\n",
    "        'escalation_settings': {\n",
    "            'confidence_threshold': agent_confidence_threshold,\n",
    "            'auto_escalation': agent_auto_escalation,\n",
    "            'escalation_triggers': agent_escalation_triggers\n",
    "        },\n",
    "        'conversation_settings': {\n",
    "            'max_conversation_turns': 8,\n",
    "            'customer_ai_enabled': True,\n",
    "            'realistic_simulation': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(settings_path, 'w') as f:\n",
    "        json.dump(settings_export, f, indent=2)\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Settings exported to: {settings_path}\")\n",
    "    \n",
    "    # Show enhanced summary\n",
    "    stats = export_data['summary_statistics']\n",
    "    print(f\"\\nüìä Export Summary:\")\n",
    "    print(f\"  Timestamp: {timestamp}\")\n",
    "    print(f\"  Results file: {results_filename}\")\n",
    "    print(f\"  Settings file: {settings_filename}\")\n",
    "    print(f\"  Total conversations: {stats['total_conversations']}\")\n",
    "    print(f\"  Completed conversations: {stats['completed_conversations']}\")\n",
    "    print(f\"  Satisfaction rate: {stats['satisfaction_rate']:.1f}%\")\n",
    "    print(f\"  Escalation rate: {stats['escalation_rate']:.1f}%\")\n",
    "    print(f\"  Average turns per conversation: {stats['average_turns']:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {stats['average_satisfaction']:.2f}\")\n",
    "    print(f\"  Files saved to: {experiment_dir.absolute()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to export. Please complete conversations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Review Results\n",
    "\n",
    "Display and analyze the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Conversation Results Review and Analysis\n",
      "\n",
      "=== CONVERSATION SUMMARY STATISTICS ===\n",
      "Total conversations: 20\n",
      "Completed conversations: 20\n",
      "Average turns per conversation: 2.5\n",
      "Average customer satisfaction: 0.675\n",
      "\n",
      "=== CONVERSATION OUTCOMES ===\n",
      "satisfied: 11 (55.0%)\n",
      "escalation_request: 8 (40.0%)\n",
      "escalation: 1 (5.0%)\n",
      "\n",
      "=== ANALYSIS BY CUSTOMER TYPE ===\n",
      "               Avg Turns  Avg Satisfaction  Completed\n",
      "customer_type                                        \n",
      "confused             2.8              0.78          5\n",
      "frustrated           2.4              0.50          5\n",
      "normal               2.0              0.71          5\n",
      "urgent               2.6              0.71          5\n",
      "\n",
      "Satisfaction Rate by Customer Type:\n",
      "  frustrated: 0/5 (0.0%)\n",
      "  urgent: 3/5 (60.0%)\n",
      "  confused: 5/5 (100.0%)\n",
      "  normal: 3/5 (60.0%)\n",
      "\n",
      "=== ANALYSIS BY COMPLEXITY ===\n",
      "            Avg Turns  Avg Satisfaction  Completed\n",
      "complexity                                        \n",
      "complex         3.000             0.600          6\n",
      "medium          2.000             0.650          7\n",
      "simple          2.429             0.764          7\n",
      "\n",
      "=== CONVERSATION SUMMARY TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "      <th>total_turns</th>\n",
       "      <th>final_outcome</th>\n",
       "      <th>customer_satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "      <td>4</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>2</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>4</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>4</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>4</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id customer_type complexity  total_turns       final_outcome  \\\n",
       "0    1    frustrated     medium            2  escalation_request   \n",
       "1    2        urgent     simple            3           satisfied   \n",
       "2    3      confused     simple            4           satisfied   \n",
       "3    4        normal     simple            2           satisfied   \n",
       "4    5    frustrated    complex            2  escalation_request   \n",
       "5    6      confused     medium            4           satisfied   \n",
       "6    7        urgent    complex            4          escalation   \n",
       "7    8        urgent     simple            1           satisfied   \n",
       "8    9        normal     simple            3  escalation_request   \n",
       "9   10      confused     medium            1           satisfied   \n",
       "10  11    frustrated     medium            3  escalation_request   \n",
       "11  12      confused    complex            1           satisfied   \n",
       "12  13        normal     medium            1           satisfied   \n",
       "13  14        urgent    complex            4  escalation_request   \n",
       "14  15    frustrated     medium            2  escalation_request   \n",
       "15  16        normal     simple            1           satisfied   \n",
       "16  17      confused    complex            4           satisfied   \n",
       "17  18        urgent     medium            1           satisfied   \n",
       "18  19    frustrated    complex            3  escalation_request   \n",
       "19  20        normal     simple            3  escalation_request   \n",
       "\n",
       "    customer_satisfaction  \n",
       "0                    0.55  \n",
       "1                    0.85  \n",
       "2                    0.80  \n",
       "3                    0.85  \n",
       "4                    0.40  \n",
       "5                    0.75  \n",
       "6                    0.55  \n",
       "7                    0.85  \n",
       "8                    0.70  \n",
       "9                    0.80  \n",
       "10                   0.55  \n",
       "11                   0.80  \n",
       "12                   0.70  \n",
       "13                   0.55  \n",
       "14                   0.45  \n",
       "15                   0.70  \n",
       "16                   0.75  \n",
       "17                   0.75  \n",
       "18                   0.55  \n",
       "19                   0.60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE FULL CONVERSATIONS ===\n",
      "\n",
      "üü¢ SATISFIED CUSTOMER CONVERSATIONS:\n",
      "\n",
      "--- Conversation 2 (urgent, simple) ---\n",
      "Original Question: I just had a car accident. What do I need to do right now?\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.85)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: I just had a car accident. What do I need to do right now?\n",
      "    Chatbot:  I'm sorry to hear about your car accident. Here are the immediate steps you should take:\n",
      "\n",
      "1. Safety First\n",
      "- Check yourself and others for injuries\n",
      "- I...\n",
      "    Customer Reply: I'm okay, but how quickly can I start my insurance claim? I don't want this to drag out and I need my car fixed ASAP. Can I start the claim process ri...\n",
      "    Satisfaction: 0.65\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: I'm okay, but how quickly can I start my insurance claim? I don't want this to drag out and I need my car fixed ASAP. Can I start the claim process ri...\n",
      "    Chatbot:  I understand you want to start your insurance claim quickly, and I'm here to help you through this process right away.\n",
      "\n",
      "You can absolutely initiate yo...\n",
      "    Customer Reply: Okay, I need to file this claim fast. Which phone number should I call right now - the 1-800 number you mentioned? I'm at the accident scene and want ...\n",
      "    Satisfaction: 0.65\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: Okay, I need to file this claim fast. Which phone number should I call right now - the 1-800 number you mentioned? I'm at the accident scene and want ...\n",
      "    Chatbot:  I understand the urgency of your situation. For immediate claims assistance, please call our 24/7 claims hotline at 1-800-CLAIMS (1-800-252-467). Our ...\n",
      "    Customer Reply: Excellent, that answers my question. Much appreciated.\n",
      "    Satisfaction: 0.85\n",
      "\n",
      "--- Conversation 3 (confused, simple) ---\n",
      "Original Question: Can someone explain what a deductible is? I keep seeing this word but don't understand.\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.80)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: Can someone explain what a deductible is? I keep seeing this word but don't understand.\n",
      "    Chatbot:  I'd be happy to explain what a deductible is in simple terms.\n",
      "\n",
      "A deductible is the amount of money you agree to pay out of pocket before your insuranc...\n",
      "    Customer Reply: Oh, okay... so if I have a $500 deductible and something happens, I have to pay $500 first? That sounds like a lot of money. Can you tell me if everyo...\n",
      "    Satisfaction: 0.50\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: Oh, okay... so if I have a $500 deductible and something happens, I have to pay $500 first? That sounds like a lot of money. Can you tell me if everyo...\n",
      "    Chatbot:  Let me help clarify how deductibles work.\n",
      "\n",
      "Deductibles can vary based on your specific insurance policy and personal preferences. While $500 might see...\n",
      "    Customer Reply: Oh, okay... so when you say \"out-of-pocket expenses,\" what exactly does that mean? And can you give me a simple example of how this actually works in ...\n",
      "    Satisfaction: 0.60\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: Oh, okay... so when you say \"out-of-pocket expenses,\" what exactly does that mean? And can you give me a simple example of how this actually works in ...\n",
      "    Chatbot:  Out-of-pocket expenses refer to the costs you pay directly from your own money before insurance coverage kicks in. Let me break this down with a simpl...\n",
      "    Customer Reply: I'm still not clear on this. I think I need to talk to someone in person.\n",
      "    Satisfaction: 0.60\n",
      "  ... and 1 more turns\n",
      "\n",
      "üî¥ ESCALATED CONVERSATIONS:\n",
      "\n",
      "--- Conversation 1 (frustrated, medium) ---\n",
      "Original Question: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.55)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "    Chatbot:  Hello! I understand your frustration about the premium increase. Let me help you understand potential reasons for this change:\n",
      "\n",
      "Premium increases can ...\n",
      "    Customer Reply: WHAT DO YOU MEAN \"SCHEDULE A CONSULTATION\"?! I just want to know RIGHT NOW why my rates went up. I don't have time to sit on the phone for hours - can...\n",
      "    Satisfaction: 0.40\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: WHAT DO YOU MEAN \"SCHEDULE A CONSULTATION\"?! I just want to know RIGHT NOW why my rates went up. I don't have time to sit on the phone for hours - can...\n",
      "    Chatbot:  I understand your frustration. Let me help you quickly clarify why your rates may have changed. Insurance rates can be adjusted due to several factors...\n",
      "    Customer Reply: I'm getting nowhere with this. Transfer me to a supervisor NOW!\n",
      "    Satisfaction: 0.55\n",
      "\n",
      "--- Conversation 5 (frustrated, complex) ---\n",
      "Original Question: I've been a customer for 15 years and you denied my claim? I want to speak to a supervisor immediately!\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.40)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: I've been a customer for 15 years and you denied my claim? I want to speak to a supervisor immediately!\n",
      "    Chatbot:  I apologize for your frustration. I understand this is a serious matter, especially given your long-standing relationship with our company. While I ca...\n",
      "    Customer Reply: I'm talking about my homeowner's policy claim for water damage from a burst pipe in my kitchen! Claim #WD-2023-0742. The adjuster said \"exclusionary c...\n",
      "    Satisfaction: 0.25\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: I'm talking about my homeowner's policy claim for water damage from a burst pipe in my kitchen! Claim #WD-2023-0742. The adjuster said \"exclusionary c...\n",
      "    Chatbot:  Hello, I understand you're frustrated about your water damage claim #WD-2023-0742. I apologize for the confusion and disappointment you're experiencin...\n",
      "    Customer Reply: I'm getting nowhere with this. Transfer me to a supervisor NOW!\n",
      "    Satisfaction: 0.40\n",
      "\n",
      "=== CONVERSATION INSIGHTS ===\n",
      "Conversation Length Distribution:\n",
      "  Short (1-2 turns): 10 (50.0%)\n",
      "  Medium (3-5 turns): 10 (50.0%)\n",
      "  Long (6+ turns): 0 (0.0%)\n",
      "\n",
      "Customer Satisfaction Distribution:\n",
      "  High (0.8+): 6 (30.0%)\n",
      "  Medium (0.5-0.8): 12 (60.0%)\n",
      "  Low (<0.5): 2 (10.0%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\n",
      "üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\n"
     ]
    }
   ],
   "source": [
    "# Review and analyze conversation results\n",
    "if 'results' in locals() and results:\n",
    "    print(\"üìã Conversation Results Review and Analysis\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'id': r.get('id'),\n",
    "            'customer_type': r.get('customer_type'),\n",
    "            'complexity': r.get('complexity'),\n",
    "            'total_turns': r.get('total_turns', 0),\n",
    "            'final_outcome': r.get('final_outcome'),\n",
    "            'customer_satisfaction': r.get('customer_satisfaction', 0),\n",
    "            'conversation_complete': r.get('conversation_complete', False)\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"=== CONVERSATION SUMMARY STATISTICS ===\")\n",
    "    print(f\"Total conversations: {len(df_results)}\")\n",
    "    print(f\"Completed conversations: {df_results['conversation_complete'].sum()}\")\n",
    "    print(f\"Average turns per conversation: {df_results['total_turns'].mean():.1f}\")\n",
    "    print(f\"Average customer satisfaction: {df_results['customer_satisfaction'].mean():.3f}\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(\"\\n=== CONVERSATION OUTCOMES ===\")\n",
    "    outcome_counts = df_results['final_outcome'].value_counts()\n",
    "    for outcome, count in outcome_counts.items():\n",
    "        percentage = count / len(df_results) * 100\n",
    "        print(f\"{outcome}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analysis by customer type\n",
    "    print(\"\\n=== ANALYSIS BY CUSTOMER TYPE ===\")\n",
    "    customer_analysis = df_results.groupby('customer_type').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    customer_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(customer_analysis)\n",
    "    \n",
    "    # Satisfaction rate by customer type\n",
    "    print(\"\\nSatisfaction Rate by Customer Type:\")\n",
    "    for ctype in df_results['customer_type'].unique():\n",
    "        ctype_data = df_results[df_results['customer_type'] == ctype]\n",
    "        satisfied = len(ctype_data[ctype_data['final_outcome'] == 'satisfied'])\n",
    "        total = len(ctype_data)\n",
    "        print(f\"  {ctype}: {satisfied}/{total} ({satisfied/total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by complexity\n",
    "    print(\"\\n=== ANALYSIS BY COMPLEXITY ===\")\n",
    "    complexity_analysis = df_results.groupby('complexity').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    complexity_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(complexity_analysis)\n",
    "    \n",
    "    # Show detailed results table\n",
    "    print(\"\\n=== CONVERSATION SUMMARY TABLE ===\")\n",
    "    display_df = df_results[['id', 'customer_type', 'complexity', 'total_turns', 'final_outcome', 'customer_satisfaction']].copy()\n",
    "    display_df['customer_satisfaction'] = display_df['customer_satisfaction'].round(3)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Show sample full conversations\n",
    "    print(\"\\n=== SAMPLE FULL CONVERSATIONS ===\")\n",
    "    \n",
    "    # Show 2 satisfied and 2 escalated conversations for analysis\n",
    "    satisfied_conversations = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    def display_conversation(conversation_data, max_turns=3):\n",
    "        \"\"\"Display a conversation with turn-by-turn analysis\"\"\"\n",
    "        conv_id = conversation_data.get('id')\n",
    "        customer_type = conversation_data.get('customer_type')\n",
    "        complexity = conversation_data.get('complexity')\n",
    "        outcome = conversation_data.get('final_outcome')\n",
    "        satisfaction = conversation_data.get('customer_satisfaction', 0)\n",
    "        \n",
    "        print(f\"\\n--- Conversation {conv_id} ({customer_type}, {complexity}) ---\")\n",
    "        print(f\"Original Question: {conversation_data.get('original_question', '')}\")\n",
    "        print(f\"Final Outcome: {outcome} (Customer Satisfaction: {satisfaction:.2f})\")\n",
    "        \n",
    "        conversation_history = conversation_data.get('conversation_history', [])\n",
    "        turns_to_show = min(max_turns, len(conversation_history))\n",
    "        \n",
    "        for i, turn in enumerate(conversation_history[:turns_to_show]):\n",
    "            turn_num = turn.get('turn_number', i+1)\n",
    "            print(f\"\\n  Turn {turn_num}:\")\n",
    "            print(f\"    Customer: {turn.get('customer_query', '')[:150]}{'...' if len(turn.get('customer_query', '')) > 150 else ''}\")\n",
    "            print(f\"    Chatbot:  {turn.get('chatbot_response', '')[:150]}{'...' if len(turn.get('chatbot_response', '')) > 150 else ''}\")\n",
    "            \n",
    "            if 'customer_response' in turn:\n",
    "                print(f\"    Customer Reply: {turn.get('customer_response', '')[:150]}{'...' if len(turn.get('customer_response', '')) > 150 else ''}\")\n",
    "                print(f\"    Satisfaction: {turn.get('customer_satisfaction', 0):.2f}\")\n",
    "        \n",
    "        if len(conversation_history) > turns_to_show:\n",
    "            print(f\"  ... and {len(conversation_history) - turns_to_show} more turns\")\n",
    "    \n",
    "    # Show satisfied conversations\n",
    "    if satisfied_conversations:\n",
    "        print(\"\\nüü¢ SATISFIED CUSTOMER CONVERSATIONS:\")\n",
    "        for conv in satisfied_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Show escalated conversations  \n",
    "    if escalated_conversations:\n",
    "        print(\"\\nüî¥ ESCALATED CONVERSATIONS:\")\n",
    "        for conv in escalated_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Conversation insights\n",
    "    print(\"\\n=== CONVERSATION INSIGHTS ===\")\n",
    "    \n",
    "    # Turn analysis\n",
    "    short_conversations = len(df_results[df_results['total_turns'] <= 2])\n",
    "    medium_conversations = len(df_results[(df_results['total_turns'] > 2) & (df_results['total_turns'] <= 5)])\n",
    "    long_conversations = len(df_results[df_results['total_turns'] > 5])\n",
    "    \n",
    "    print(f\"Conversation Length Distribution:\")\n",
    "    print(f\"  Short (1-2 turns): {short_conversations} ({short_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (3-5 turns): {medium_conversations} ({medium_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Long (6+ turns): {long_conversations} ({long_conversations/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Satisfaction insights\n",
    "    high_satisfaction = len(df_results[df_results['customer_satisfaction'] >= 0.8])\n",
    "    medium_satisfaction = len(df_results[(df_results['customer_satisfaction'] >= 0.5) & (df_results['customer_satisfaction'] < 0.8)])\n",
    "    low_satisfaction = len(df_results[df_results['customer_satisfaction'] < 0.5])\n",
    "    \n",
    "    print(f\"\\nCustomer Satisfaction Distribution:\")\n",
    "    print(f\"  High (0.8+): {high_satisfaction} ({high_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.5-0.8): {medium_satisfaction} ({medium_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Low (<0.5): {low_satisfaction} ({low_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\")\n",
    "    print(\"üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to review. Please complete conversations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've successfully tested the Chatbot Agent with full conversation simulation.\n",
    "\n",
    "### What you've accomplished:\n",
    "- ‚úÖ Loaded and customized Chatbot Agent configuration\n",
    "- ‚úÖ Loaded test questions from file with customer profiles  \n",
    "- ‚úÖ Conducted full conversations between customer AI and chatbot AI\n",
    "- ‚úÖ Simulated realistic customer interactions until resolution or escalation\n",
    "- ‚úÖ Captured complete conversation flows with satisfaction tracking\n",
    "- ‚úÖ Exported comprehensive results with conversation analysis\n",
    "- ‚úÖ Analyzed performance by customer type, complexity, and outcomes\n",
    "\n",
    "### Your exported files contain:\n",
    "- **Conversation Results**: Complete turn-by-turn interaction data including:\n",
    "  - Full conversation history for each question\n",
    "  - Customer satisfaction scores at each turn\n",
    "  - Final outcomes (satisfied, escalation, etc.)\n",
    "  - Detailed conversation analysis by customer type and complexity\n",
    "- **Settings file**: Complete configuration used for this experiment\n",
    "- **Conversation Analysis**: Statistical breakdown of conversation patterns\n",
    "\n",
    "### Key Features of the Conversation System:\n",
    "- **Customer AI Simulation**: Realistic customer personas (frustrated, urgent, confused, normal)\n",
    "- **Dynamic Satisfaction Tracking**: AI-powered customer satisfaction scoring\n",
    "- **Natural Conversation Flow**: Continues until natural resolution or escalation\n",
    "- **Personality-Based Responses**: Customer responses match their personality type\n",
    "- **Escalation Detection**: Automatic detection when customers or chatbot need human assistance\n",
    "- **Model Integration**: Uses actual models from questions file for both customer and chatbot\n",
    "\n",
    "### Conversation Outcomes Explained:\n",
    "- **Satisfied**: Customer received satisfactory answer and conversation ended positively\n",
    "- **Escalation**: Chatbot determined the query needed human specialist assistance  \n",
    "- **Escalation Request**: Customer explicitly requested to speak with a human\n",
    "- **Conversation Incomplete**: Reached maximum turns without resolution\n",
    "\n",
    "### Ready for Advanced Analysis?\n",
    "- **Conversation Patterns**: Analyze which question types lead to longer conversations\n",
    "- **Satisfaction Drivers**: Identify what chatbot responses increase customer satisfaction\n",
    "- **Escalation Triggers**: Understand when and why conversations need human intervention\n",
    "- **Customer Journey Mapping**: Track how different customer types interact with the system\n",
    "- **Model Performance**: Compare how different AI models handle customer interactions\n",
    "\n",
    "### To optimize your chatbot:\n",
    "1. **Review Escalated Conversations**: Look for patterns in conversations that required escalation\n",
    "2. **Analyze Low Satisfaction**: Identify what causes customer dissatisfaction\n",
    "3. **Study Successful Resolutions**: Learn from conversations that ended with satisfied customers\n",
    "4. **Adjust Configuration**: Modify agent settings based on conversation insights\n",
    "5. **Test Iteratively**: Run multiple experiments with different configurations\n",
    "\n",
    "### Workflow for Continuous Improvement:\n",
    "1. **Generate Questions**: Use `question_generator.ipynb` to create diverse test scenarios\n",
    "2. **Run Conversations**: Use this notebook to conduct full conversation simulations\n",
    "3. **Analyze Results**: Review conversation patterns and customer satisfaction metrics\n",
    "4. **Optimize Configuration**: Adjust chatbot settings based on insights\n",
    "5. **Validate Changes**: Re-run tests to confirm improvements\n",
    "6. **Deploy**: Apply successful configurations to production systems\n",
    "\n",
    "### Advanced Features Available:\n",
    "- **Multi-Model Testing**: Test different AI models for customer and chatbot roles\n",
    "- **Custom Customer Personas**: Modify customer AI behavior for specific use cases  \n",
    "- **Conversation Length Controls**: Adjust maximum turns and escalation thresholds\n",
    "- **Satisfaction Threshold Tuning**: Customize what constitutes a satisfied customer\n",
    "- **Real-time Analysis**: Monitor conversation quality as it happens\n",
    "\n",
    "---\n",
    "*This notebook simulates realistic customer service interactions to optimize AI chatbot performance and improve customer satisfaction outcomes.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
