{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Quality Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Quality Agent with chatbot interactions.\n",
    "It reads output from chatbot_tester.ipynb and evaluates the quality of each chatbot response.\n",
    "\n",
    "## Features:\n",
    "- Load and edit Quality Agent configuration settings\n",
    "- Import chatbot conversation results from chatbot_tester.ipynb\n",
    "- Process each conversation turn through the Quality Agent\n",
    "- Generate quality assessments, scores, and improvement suggestions\n",
    "- Analyze quality patterns across different customer types and scenarios\n",
    "- Export enhanced results with quality metrics\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load chatbot conversation results from chatbot_tester exports\n",
    "4. Review conversations before quality assessment\n",
    "5. Run quality analysis and review detailed results\n",
    "\n",
    "## Input Requirements:\n",
    "Load conversation results exported from chatbot_tester.ipynb containing:\n",
    "- Customer queries and chatbot responses\n",
    "- Conversation metadata and turn-by-turn interactions\n",
    "- Customer types, complexity levels, and satisfaction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Ready to start testing the Quality Agent.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.quality_agent import QualityAgentNode\n",
    "from src.core.config import ConfigManager\n",
    "from src.core.context_manager import SQLiteContextProvider\n",
    "from src.interfaces.core.state_schema import HybridSystemState\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start testing the Quality Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration Settings\n",
    "\n",
    "The following cell loads the current configuration for the Quality Agent.\n",
    "You can edit these values to customize the agent's quality assessment behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration files loaded and temporary copies created with comments preserved!\n",
      "Agent name: quality_agent\n",
      "Agent version: 1.0.0\n",
      "Preferred model: anthropic_general_budget\n",
      "\n",
      "üéØ Quality Assessment Thresholds:\n",
      "  Adequate score: 7.0\n",
      "  Adjustment needed: 5.0\n",
      "\n",
      "üíæ Temporary config files created at:\n",
      "  agent_config: /tmp/quality_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/quality_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/quality_agent_configs/models.yaml\n",
      "\n",
      "üí° These temp files retain original comments and can be edited directly in Step 2.\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from files and create temporary editable copies\n",
    "config_base_path = Path('/workspace/config')\n",
    "agent_config_path = config_base_path / 'agents' / 'quality_agent'\n",
    "temp_config_dir = Path('/tmp/quality_agent_configs')\n",
    "\n",
    "def load_and_create_temp_configs():\n",
    "    \"\"\"Load all configuration files and create temporary editable copies with comments preserved\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Create YAML instance for comment preservation\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True\n",
    "    yaml.default_flow_style = False\n",
    "    \n",
    "    # Create temp directory\n",
    "    temp_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load original files for parsing (to access data)\n",
    "    with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "        configs['agent'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "        configs['prompts'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "        configs['models'] = yaml.load(f)\n",
    "    \n",
    "    # Load shared models for reference\n",
    "    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n",
    "        configs['shared_models'] = yaml.load(f)\n",
    "    \n",
    "    # Create temp file paths\n",
    "    temp_agent_path = temp_config_dir / 'config.yaml'\n",
    "    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n",
    "    temp_models_path = temp_config_dir / 'models.yaml'\n",
    "    \n",
    "    # Copy original files to temp directory to preserve comments and formatting\n",
    "    import shutil\n",
    "    shutil.copy2(agent_config_path / 'config.yaml', temp_agent_path)\n",
    "    shutil.copy2(agent_config_path / 'prompts.yaml', temp_prompts_path)  \n",
    "    shutil.copy2(agent_config_path / 'models.yaml', temp_models_path)\n",
    "    \n",
    "    return configs, {\n",
    "        'agent_config': temp_agent_path,\n",
    "        'prompts_config': temp_prompts_path,\n",
    "        'models_config': temp_models_path\n",
    "    }\n",
    "\n",
    "# Load configurations and create temp files\n",
    "configs, temp_file_paths = load_and_create_temp_configs()\n",
    "\n",
    "print(\"üìÅ Configuration files loaded and temporary copies created with comments preserved!\")\n",
    "print(f\"Agent name: {configs['agent']['agent']['name']}\")\n",
    "print(f\"Agent version: {configs['agent']['agent']['version']}\")\n",
    "\n",
    "# Get preferred model from models config\n",
    "preferred_model = \"Unknown\"\n",
    "if 'primary_model' in configs['models']:\n",
    "    preferred_model = configs['models']['primary_model']\n",
    "elif 'preferred' in configs['models']:\n",
    "    preferred_model = configs['models']['preferred']\n",
    "\n",
    "print(f\"Preferred model: {preferred_model}\")\n",
    "\n",
    "# Display key quality thresholds\n",
    "quality_thresholds = configs['agent']['settings']['quality_thresholds']\n",
    "print(f\"\\nüéØ Quality Assessment Thresholds:\")\n",
    "print(f\"  Adequate score: {quality_thresholds['adequate_score']}\")\n",
    "print(f\"  Adjustment needed: {quality_thresholds['adjustment_score']}\")\n",
    "\n",
    "print(f\"\\nüíæ Temporary config files created at:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "print(f\"\\nüí° These temp files retain original comments and can be edited directly in Step 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Editable Configuration Settings\n",
    "\n",
    "Edit these settings to customize how the Quality Agent assesses chatbot responses.\n",
    "These variables map directly to the configuration files and can be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Quality Agent Configuration File Editor\n",
      "Edit the YAML configuration files below and use the Save buttons to apply changes.\n",
      "Changes are saved to temporary files and will be used in quality assessment.\n",
      "\n",
      "üìÑ 1. Quality Agent Configuration (config.yaml)\n",
      "Contains: quality thresholds, assessment settings, escalation rules\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5980703ce6184c2695d6e6ee499bf6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='# Quality Agent Configuration\\n# Responsibility: Review chatbot answers and decide if adequate‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6031752875e648bea9c87c8ec2d1ae6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Agent Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 2. Quality Assessment Prompts Configuration (prompts.yaml)\n",
      "Contains: system prompts, quality assessment criteria, adjustment templates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d280a57cd84313be03febd556b84df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='# Quality Agent Prompts\\n\\nsystem: |\\n  You are a Quality Assessment Agent responsible for eva‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3af8b56a634c23b51de6158ac1a963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Prompts Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 3. Quality Assessment Models Configuration (models.yaml)\n",
      "Contains: preferred models for quality assessment, response adjustment\n",
      "\n",
      "üîç Available Model Aliases for Quality Assessment:\n",
      "Use these aliases in your models configuration below:\n",
      "\n",
      "üì° ANTHROPIC Provider:\n",
      "  ‚Ä¢ anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 - Anthropic Claude 3.5 Haiku - fast and efficient\n",
      "  ‚Ä¢ anthropic_general_standard ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_reasoning_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_coding_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_flagship ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "\n",
      "üì° OPENAI Provider:\n",
      "  ‚Ä¢ openai_general_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "  ‚Ä¢ openai_general_budget ‚Üí gpt-3.5-turbo - OpenAI GPT-3.5 Turbo - fast and cost-effective\n",
      "  ‚Ä¢ openai_coding_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "\n",
      "üì° LLAMA Provider:\n",
      "  ‚Ä¢ local_general_standard ‚Üí llama-7b - Llama 7B model - good balance of speed and quality\n",
      "  ‚Ä¢ local_general_premium ‚Üí llama-13b - Llama 13B model - higher quality, slower inference\n",
      "  ‚Ä¢ local_coding_standard ‚Üí codellama-7b - CodeLlama 7B - optimized for code generation\n",
      "\n",
      "üì° MISTRAL Provider:\n",
      "  ‚Ä¢ local_general_budget ‚Üí mistral-7b - Mistral 7B Instruct - excellent instruction following\n",
      "\n",
      "üìã Current Models Configuration:\n",
      "  Primary model: anthropic_general_budget\n",
      "\n",
      "üí° Quality Assessment Model Recommendations:\n",
      "  ‚Ä¢ Lower temperature models (0.1-0.3) for consistent quality scoring\n",
      "  ‚Ä¢ Reasoning models for complex quality analysis\n",
      "  ‚Ä¢ Standard models sufficient for basic quality checks\n",
      "  ‚Ä¢ Budget models acceptable for rule-based assessments\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62403dbdf8024652843f5e089c91dacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='primary_model: \"anthropic_general_budget\"\\n\\nmodel_overrides:\\n  temperature: 0.3          # L‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896cdeb427e54e018eee41649de6a277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Models Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Save All Changes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230e9bae6ef342388c7e1caeba8d0034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Save All Configs', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Temp config files location:\n",
      "  agent_config: /tmp/quality_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/quality_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/quality_agent_configs/models.yaml\n",
      "\n",
      "üí° Quality Assessment Tips:\n",
      "  ‚Ä¢ Lower adequate_score = more responses marked as adequate\n",
      "  ‚Ä¢ Higher adjustment_score = more attempts at response improvement\n",
      "  ‚Ä¢ Lower temperature = more consistent quality scoring\n",
      "  ‚Ä¢ Edit quality assessment prompts to customize evaluation criteria\n",
      "  ‚Ä¢ Model preferences allow different models for different quality tasks\n"
     ]
    }
   ],
   "source": [
    "# Display and edit configuration files in separate windows\n",
    "\n",
    "def load_config_file_contents():\n",
    "    \"\"\"Load current config file contents from temp files\"\"\"\n",
    "    with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "        agent_config_content = f.read()\n",
    "    with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "        prompts_config_content = f.read()\n",
    "    with open(temp_file_paths['models_config'], 'r') as f:\n",
    "        models_config_content = f.read()\n",
    "    \n",
    "    return agent_config_content, prompts_config_content, models_config_content\n",
    "\n",
    "# Load current config file contents\n",
    "agent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n",
    "\n",
    "print(\"‚öôÔ∏è Quality Agent Configuration File Editor\")\n",
    "print(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\n",
    "print(\"Changes are saved to temporary files and will be used in quality assessment.\\n\")\n",
    "\n",
    "# Create text areas for each config file\n",
    "print(\"üìÑ 1. Quality Agent Configuration (config.yaml)\")\n",
    "print(\"Contains: quality thresholds, assessment settings, escalation rules\")\n",
    "\n",
    "agent_config_editor = widgets.Textarea(\n",
    "    value=agent_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_agent_config(button):\n",
    "    \"\"\"Save agent config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(agent_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['agent_config'], 'w') as f:\n",
    "            f.write(agent_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Quality Agent config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n",
    "\n",
    "agent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\n",
    "agent_save_btn.on_click(save_agent_config)\n",
    "\n",
    "display(agent_config_editor)\n",
    "display(agent_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 2. Quality Assessment Prompts Configuration (prompts.yaml)\")\n",
    "print(\"Contains: system prompts, quality assessment criteria, adjustment templates\")\n",
    "\n",
    "prompts_config_editor = widgets.Textarea(\n",
    "    value=prompts_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='300px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_prompts_config(button):\n",
    "    \"\"\"Save prompts config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(prompts_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['prompts_config'], 'w') as f:\n",
    "            f.write(prompts_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Prompts config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n",
    "\n",
    "prompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\n",
    "prompts_save_btn.on_click(save_prompts_config)\n",
    "\n",
    "display(prompts_config_editor)\n",
    "display(prompts_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 3. Quality Assessment Models Configuration (models.yaml)\")\n",
    "print(\"Contains: preferred models for quality assessment, response adjustment\")\n",
    "\n",
    "# Show available model aliases from shared models config\n",
    "def display_available_models():\n",
    "    \"\"\"Display available model aliases and their actual models\"\"\"\n",
    "    try:\n",
    "        shared_models = configs['shared_models']\n",
    "        \n",
    "        # Extract model aliases and models sections\n",
    "        model_aliases = shared_models.get('model_aliases', {})\n",
    "        models = shared_models.get('models', {})\n",
    "        \n",
    "        if not model_aliases:\n",
    "            print(\"‚ùå No model aliases found in shared configuration\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nüîç Available Model Aliases for Quality Assessment:\")\n",
    "        print(\"Use these aliases in your models configuration below:\\n\")\n",
    "        \n",
    "        # Group by provider for better organization\n",
    "        providers = {}\n",
    "        for alias, actual_model_name in model_aliases.items():\n",
    "            # Get model details from models section\n",
    "            model_details = models.get(actual_model_name, {})\n",
    "            provider = model_details.get('type', 'unknown')\n",
    "            description = model_details.get('description', '')\n",
    "            \n",
    "            if provider not in providers:\n",
    "                providers[provider] = []\n",
    "            providers[provider].append({\n",
    "                'alias': alias,\n",
    "                'model_name': actual_model_name,\n",
    "                'description': description\n",
    "            })\n",
    "        \n",
    "        # Display by provider with quality assessment recommendations\n",
    "        for provider, provider_models in providers.items():\n",
    "            print(f\"üì° {provider.upper()} Provider:\")\n",
    "            for model in provider_models:\n",
    "                desc = f\" - {model['description']}\" if model['description'] else \"\"\n",
    "                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n",
    "            print()\n",
    "        \n",
    "        # Show current configuration\n",
    "        try:\n",
    "            yaml = YAML()\n",
    "            yaml.preserve_quotes = True\n",
    "            current_models_config = yaml.load(models_config_content)\n",
    "            current_preferred = current_models_config.get('primary_model', current_models_config.get('preferred', 'unknown'))\n",
    "            \n",
    "            # Show model preferences for different tasks\n",
    "            model_preferences = current_models_config.get('model_preferences', {})\n",
    "        except:\n",
    "            current_preferred = 'unknown'\n",
    "            model_preferences = {}\n",
    "        \n",
    "        print(f\"üìã Current Models Configuration:\")\n",
    "        print(f\"  Primary model: {current_preferred}\")\n",
    "        if model_preferences:\n",
    "            print(f\"  Task-specific preferences:\")\n",
    "            for task, prefs in model_preferences.items():\n",
    "                print(f\"    {task}: {prefs.get('primary', 'unknown')}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üí° Quality Assessment Model Recommendations:\")\n",
    "        print(\"  ‚Ä¢ Lower temperature models (0.1-0.3) for consistent quality scoring\")\n",
    "        print(\"  ‚Ä¢ Reasoning models for complex quality analysis\")\n",
    "        print(\"  ‚Ä¢ Standard models sufficient for basic quality checks\")\n",
    "        print(\"  ‚Ä¢ Budget models acceptable for rule-based assessments\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading available models: {e}\")\n",
    "        print(\"Continuing with models configuration editor...\\n\")\n",
    "\n",
    "# Display available models before showing the editor\n",
    "display_available_models()\n",
    "\n",
    "models_config_editor = widgets.Textarea(\n",
    "    value=models_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='200px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_models_config(button):\n",
    "    \"\"\"Save models config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        parsed_config = yaml.load(models_config_editor.value)\n",
    "        \n",
    "        # Additional validation for model aliases\n",
    "        if isinstance(parsed_config, dict):\n",
    "            preferred = parsed_config.get('primary_model') or parsed_config.get('preferred')\n",
    "            \n",
    "            # Get available aliases\n",
    "            model_aliases = configs['shared_models'].get('model_aliases', {})\n",
    "            \n",
    "            # Check if preferred model exists\n",
    "            if preferred and preferred not in model_aliases:\n",
    "                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n",
    "            \n",
    "            # Check model preferences\n",
    "            model_preferences = parsed_config.get('model_preferences', {})\n",
    "            for task, prefs in model_preferences.items():\n",
    "                task_primary = prefs.get('primary')\n",
    "                if task_primary and task_primary not in model_aliases:\n",
    "                    print(f\"‚ö†Ô∏è Warning: {task} primary model '{task_primary}' not found in available model aliases\")\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['models_config'], 'w') as f:\n",
    "            f.write(models_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Models config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in models config: {e}\")\n",
    "\n",
    "models_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\n",
    "models_save_btn.on_click(save_models_config)\n",
    "\n",
    "display(models_config_editor)\n",
    "display(models_save_btn)\n",
    "\n",
    "# Save All button for convenience\n",
    "def save_all_configs(button):\n",
    "    \"\"\"Save all config changes at once\"\"\"\n",
    "    save_agent_config(None)\n",
    "    save_prompts_config(None)\n",
    "    save_models_config(None)\n",
    "\n",
    "print(\"\\nüíæ Save All Changes\")\n",
    "save_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\n",
    "save_all_btn.on_click(save_all_configs)\n",
    "display(save_all_btn)\n",
    "\n",
    "print(f\"\\nüíæ Temp config files location:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "\n",
    "print(\"\\nüí° Quality Assessment Tips:\")\n",
    "print(\"  ‚Ä¢ Lower adequate_score = more responses marked as adequate\")\n",
    "print(\"  ‚Ä¢ Higher adjustment_score = more attempts at response improvement\")\n",
    "print(\"  ‚Ä¢ Lower temperature = more consistent quality scoring\")\n",
    "print(\"  ‚Ä¢ Edit quality assessment prompts to customize evaluation criteria\")\n",
    "print(\"  ‚Ä¢ Model preferences allow different models for different quality tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5: How Quality Assessment Works\n",
    "\n",
    "The Quality Agent uses a sophisticated multi-layer approach to assess chatbot response quality, combining LLM-based evaluation with rule-based fallbacks and configurable thresholds. Understanding this process helps optimize the assessment parameters for your specific use case.\n",
    "\n",
    "### Quality Assessment Architecture\n",
    "\n",
    "The quality assessment process follows these key steps:\n",
    "\n",
    "1. **Initial Response Validation**: Checks if a chatbot response exists to assess\n",
    "2. **LLM-Based Quality Evaluation**: Uses configured LLM with specialized prompts for detailed assessment\n",
    "3. **Context-Based Adjustments**: Applies customer context and conversation history to refine scores\n",
    "4. **Decision Making**: Compares scores against configurable thresholds to determine actions\n",
    "5. **Response Improvement**: Generates adjusted responses when needed\n",
    "\n",
    "### YAML Configuration Components\n",
    "\n",
    "#### 1. Agent Configuration (`config.yaml`)\n",
    "\n",
    "**Quality Thresholds** (`src/nodes/quality_agent.py:114-117`):\n",
    "```yaml\n",
    "quality_thresholds:\n",
    "  adequate_score: 7.0      # Responses ‚â• 7.0 are considered adequate\n",
    "  adjustment_score: 5.0    # Responses 5.0-6.9 need adjustment, <5.0 need human\n",
    "```\n",
    "\n",
    "**Assessment Settings** (`src/nodes/quality_agent.py:121-128`):\n",
    "```yaml\n",
    "assessment:\n",
    "  use_llm_evaluation: true    # Enable LLM-based quality assessment\n",
    "  confidence_threshold: 0.7   # Minimum confidence for decisions\n",
    "  context_weight: 0.3         # Weight of customer context in final score\n",
    "```\n",
    "\n",
    "**Response Adjustment** (`src/nodes/quality_agent.py:90-94`):\n",
    "```yaml\n",
    "adjustment:\n",
    "  max_attempts: 2             # Maximum improvement attempts\n",
    "  improvement_threshold: 1.5  # Minimum score improvement required\n",
    "```\n",
    "\n",
    "#### 2. Prompts Configuration (`prompts.yaml`)\n",
    "\n",
    "**System Prompt** (`src/nodes/quality_agent.py:161`):\n",
    "- Establishes the agent's role and evaluation criteria\n",
    "- Defines focus areas: accuracy, completeness, clarity, tone, usefulness\n",
    "- Sets objective assessment standards\n",
    "\n",
    "**Quality Assessment Prompt** (`src/nodes/quality_agent.py:160-166`):\n",
    "- Structures the evaluation request with customer query and chatbot response\n",
    "- Defines 1-10 scoring scale with specific criteria\n",
    "- Identifies red flags: incorrect info, incomplete answers, inappropriate tone\n",
    "\n",
    "**Response Adjustment Prompt** (`src/nodes/quality_agent.py:91-93`):\n",
    "- Guides improvement of inadequate responses\n",
    "- Maintains accuracy while addressing quality issues\n",
    "- Ensures professional, complete, and helpful responses\n",
    "\n",
    "#### 3. Models Configuration (`models.yaml`)\n",
    "\n",
    "**Temperature Settings** (`src/nodes/quality_agent.py:168-171`):\n",
    "```yaml\n",
    "model_overrides:\n",
    "  temperature: 0.3          # Base temperature for consistent assessment\n",
    "  per_model:\n",
    "    quality_assessment:\n",
    "      temperature: 0.2      # Very low for objective, consistent scoring\n",
    "    response_adjustment:\n",
    "      temperature: 0.5      # Higher for creative response improvement\n",
    "```\n",
    "\n",
    "### Assessment Logic Flow\n",
    "\n",
    "#### Step 1: LLM Quality Assessment (`src/nodes/quality_agent.py:157-191`)\n",
    "\n",
    "1. **Prompt Formation**: Combines system prompt with quality assessment template\n",
    "2. **Structured Evaluation**: LLM evaluates on 5 key dimensions:\n",
    "   - Accuracy of information\n",
    "   - Completeness of response  \n",
    "   - Clarity and structure\n",
    "   - Appropriate tone\n",
    "   - Customer usefulness\n",
    "3. **Score Extraction**: Parses numerical score (1-10) and detailed reasoning\n",
    "4. **Fallback Handling**: Uses rule-based assessment if LLM fails\n",
    "\n",
    "#### Step 2: Context-Based Adjustments (`src/nodes/quality_agent.py:127-128`)\n",
    "\n",
    "The system adjusts raw LLM scores based on:\n",
    "- Customer type (frustrated, urgent, confused, normal)\n",
    "- Conversation complexity level\n",
    "- Previous interaction history\n",
    "- Customer satisfaction indicators\n",
    "\n",
    "#### Step 3: Decision Making (`src/nodes/quality_agent.py:145-155`)\n",
    "\n",
    "```python\n",
    "if final_score >= adequate_score:          # ‚â• 7.0\n",
    "    decision = ADEQUATE\n",
    "elif final_score >= adjustment_score:      # 5.0-6.9  \n",
    "    decision = NEEDS_ADJUSTMENT\n",
    "else:                                      # < 5.0\n",
    "    decision = HUMAN_INTERVENTION\n",
    "```\n",
    "\n",
    "#### Step 4: Response Improvement (`src/nodes/quality_agent.py:90-94`)\n",
    "\n",
    "When `NEEDS_ADJUSTMENT` is determined:\n",
    "1. Uses response adjustment prompt with original query and quality issues\n",
    "2. Generates improved response maintaining accuracy and helpfulness\n",
    "3. Applies higher temperature (0.5) for creative improvement\n",
    "\n",
    "### Configuration Best Practices\n",
    "\n",
    "**For Stricter Quality Control**:\n",
    "- Increase `adequate_score` to 8.0+ \n",
    "- Decrease `adjustment_score` to 4.0\n",
    "- Lower temperature to 0.1 for consistent scoring\n",
    "\n",
    "**For Lenient Quality Assessment**:\n",
    "- Decrease `adequate_score` to 6.0\n",
    "- Increase `adjustment_score` to 6.0\n",
    "- Higher temperature (0.4) for varied assessments\n",
    "\n",
    "**For Response Improvement Focus**:\n",
    "- Set wider gap between thresholds (adequate: 8.0, adjustment: 4.0)\n",
    "- Enable multiple adjustment attempts\n",
    "- Use creative temperature (0.6-0.7) for adjustments\n",
    "\n",
    "### Quality Metrics and Confidence\n",
    "\n",
    "**Confidence Calculation** (`src/nodes/quality_agent.py:151`):\n",
    "```python\n",
    "confidence = min(0.95, max(0.5, final_score / 10.0))\n",
    "```\n",
    "- Higher scores = higher confidence\n",
    "- Minimum confidence: 0.5, Maximum: 0.95\n",
    "- Directly proportional to quality score\n",
    "\n",
    "**Assessment Output Structure**:\n",
    "```json\n",
    "{\n",
    "  \"decision\": \"adequate|needs_adjustment|human_intervention\",\n",
    "  \"overall_score\": 7.2,\n",
    "  \"confidence\": 0.72,\n",
    "  \"reasoning\": \"Detailed LLM assessment explanation...\",\n",
    "  \"adjustment_needed\": false,\n",
    "  \"adjusted_response\": \"Improved response if needed...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This comprehensive system ensures consistent, configurable quality assessment that can be tuned for different customer service standards and business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Load Chatbot Conversation Results\n",
    "\n",
    "Load conversation results from chatbot_tester.ipynb exports for quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Load Chatbot Conversation Results\n",
      "\n",
      "üí° How to get conversation data:\n",
      "1. Run chatbot_tester.ipynb and export conversation results\n",
      "2. Upload the exported JSON file below\n",
      "3. The file should contain conversation turns with queries and responses\n",
      "\n",
      "üìÑ Expected JSON format:\n",
      "- Array of conversations with conversation_history containing:\n",
      "- Each turn: {'customer_query': '...', 'chatbot_response': '...', 'turn_number': N}\n",
      "- Metadata: customer_type, complexity, conversation details\n",
      "\n",
      "üìÅ Upload your conversation results file:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c5919137dc4c669695fd322f20bb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.json', description='Upload conversation results:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File upload widget for loading conversation results\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.json',\n",
    "    multiple=False,\n",
    "    description='Upload conversation results:'\n",
    ")\n",
    "\n",
    "# Instructions for file format\n",
    "print(\"üìù Load Chatbot Conversation Results\")\n",
    "print(\"\\nüí° How to get conversation data:\")\n",
    "print(\"1. Run chatbot_tester.ipynb and export conversation results\")\n",
    "print(\"2. Upload the exported JSON file below\")\n",
    "print(\"3. The file should contain conversation turns with queries and responses\")\n",
    "print(\"\\nüìÑ Expected JSON format:\")\n",
    "print(\"- Array of conversations with conversation_history containing:\")\n",
    "print(\"- Each turn: {'customer_query': '...', 'chatbot_response': '...', 'turn_number': N}\")\n",
    "print(\"- Metadata: customer_type, complexity, conversation details\")\n",
    "print(\"\\nüìÅ Upload your conversation results file:\")\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading conversations from: chatbot_agent_output.json\n",
      "‚úÖ Loaded 50 conversation turns from 20 conversations\n",
      "\n",
      "üìä Conversation Turn Distribution:\n",
      "  Total turns: 50\n",
      "  Unique conversations: 20\n",
      "  Customer types: {'frustrated': np.int64(15), 'urgent': np.int64(15), 'confused': np.int64(14), 'normal': np.int64(6)}\n",
      "  Complexity levels: {'medium': np.int64(20), 'complex': np.int64(18), 'simple': np.int64(12)}\n",
      "\n",
      "üìã Sample Conversation Turns:\n",
      "  Turn 1.1 [frustrated]:\n",
      "    Query: Why did my premium increase by $200? This is ridiculous - I haven't had any clai...\n",
      "    Response:   Sorry to hear that your premium increased by $200, [Customer Name]. I can unde...\n",
      "  Turn 1.2 [frustrated]:\n",
      "    Query: \"ARE YOU KIDDING ME?! I DON'T HAVE TIME FOR THIS NONSENSE! I JUST WANT TO KNOW W...\n",
      "    Response:   Oh no, I'm so sorry to hear that you're dealing with an unexpected premium inc...\n",
      "  Turn 1.3 [frustrated]:\n",
      "    Query: This isn't working. I need to speak to someone who can actually help me!\n",
      "    Response:   Hello there! üòä I apologize for any inconvenience you're experiencing. I'm here...\n",
      "  ... and 47 more turns\n"
     ]
    }
   ],
   "source": [
    "# Load conversation results from uploaded file\n",
    "conversation_data = []\n",
    "\n",
    "def load_conversations_from_file(file_content, filename):\n",
    "    \"\"\"Load conversations from uploaded JSON file\"\"\"\n",
    "    try:\n",
    "        # Handle different content types\n",
    "        if isinstance(file_content, memoryview):\n",
    "            content_bytes = file_content.tobytes()\n",
    "        elif hasattr(file_content, 'decode'):\n",
    "            content_bytes = file_content\n",
    "        else:\n",
    "            content_bytes = str(file_content).encode('utf-8')\n",
    "        \n",
    "        # Decode to string and parse JSON\n",
    "        data = json.loads(content_bytes.decode('utf-8'))\n",
    "        \n",
    "        # Handle different JSON formats\n",
    "        if isinstance(data, dict):\n",
    "            # Check for different export formats from chatbot_tester\n",
    "            if 'conversation_results' in data:  # This is the correct key!\n",
    "                conversations = data['conversation_results']\n",
    "            elif 'conversations' in data:\n",
    "                conversations = data['conversations']\n",
    "            elif 'results' in data:\n",
    "                conversations = data['results']\n",
    "            else:\n",
    "                # Single conversation object\n",
    "                conversations = [data]\n",
    "        elif isinstance(data, list):\n",
    "            conversations = data\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected data format: {type(data)}\")\n",
    "            return []\n",
    "            \n",
    "        return conversations\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_conversation_turns(conversations):\n",
    "    \"\"\"Extract individual turns from conversations for quality assessment\"\"\"\n",
    "    turns = []\n",
    "    \n",
    "    for conv_idx, conversation in enumerate(conversations):\n",
    "        conv_id = conversation.get('id', conv_idx + 1)\n",
    "        customer_type = conversation.get('customer_type', 'unknown')\n",
    "        complexity = conversation.get('complexity', 'medium')\n",
    "        \n",
    "        # Get conversation history\n",
    "        history = conversation.get('conversation_history', [])\n",
    "        \n",
    "        # If no conversation_history, try to extract from other fields\n",
    "        if not history:\n",
    "            # Check for single turn conversation\n",
    "            query = conversation.get('query') or conversation.get('customer_query') or conversation.get('original_question')\n",
    "            response = conversation.get('ai_response') or conversation.get('chatbot_response') or conversation.get('response')\n",
    "            \n",
    "            if query and response:\n",
    "                history = [{\n",
    "                    'turn_number': 1,\n",
    "                    'customer_query': query,\n",
    "                    'chatbot_response': response\n",
    "                }]\n",
    "        \n",
    "        # Extract turns from conversation history\n",
    "        for turn_idx, turn in enumerate(history):\n",
    "            turn_data = {\n",
    "                'conversation_id': conv_id,\n",
    "                'turn_number': turn.get('turn_number', turn_idx + 1),\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'customer_query': turn.get('customer_query', ''),\n",
    "                'chatbot_response': turn.get('chatbot_response', ''),\n",
    "                'original_customer_satisfaction': turn.get('customer_satisfaction'),\n",
    "                'conversation_metadata': {\n",
    "                    'total_turns': len(history),\n",
    "                    'final_outcome': conversation.get('final_outcome'),\n",
    "                    'original_question': conversation.get('original_question', ''),\n",
    "                }\n",
    "            }\n",
    "            turns.append(turn_data)\n",
    "    \n",
    "    return turns\n",
    "\n",
    "# Process uploaded file\n",
    "if file_upload.value:\n",
    "    uploaded_file = None\n",
    "    filename = None\n",
    "    file_content = None\n",
    "    \n",
    "    # Handle different file upload widget formats\n",
    "    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "        uploaded_file = file_upload.value[0]\n",
    "        filename = uploaded_file['name']\n",
    "        file_content = uploaded_file['content']\n",
    "    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "        uploaded_file = list(file_upload.value.values())[0]\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "        file_content = uploaded_file['content']\n",
    "    else:\n",
    "        print(f\"‚ùå Unable to read uploaded file format\")\n",
    "    \n",
    "    if uploaded_file and filename and file_content is not None:\n",
    "        print(f\"üìÅ Loading conversations from: {filename}\")\n",
    "        \n",
    "        raw_conversations = load_conversations_from_file(file_content, filename)\n",
    "        \n",
    "        if raw_conversations:\n",
    "            conversation_data = extract_conversation_turns(raw_conversations)\n",
    "            print(f\"‚úÖ Loaded {len(conversation_data)} conversation turns from {len(raw_conversations)} conversations\")\n",
    "            \n",
    "            # Display conversation statistics only if we have data\n",
    "            if conversation_data:\n",
    "                df_preview = pd.DataFrame(conversation_data)\n",
    "                print(f\"\\nüìä Conversation Turn Distribution:\")\n",
    "                print(f\"  Total turns: {len(conversation_data)}\")\n",
    "                print(f\"  Unique conversations: {df_preview['conversation_id'].nunique()}\")\n",
    "                print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n",
    "                print(f\"  Complexity levels: {dict(df_preview['complexity'].value_counts())}\")\n",
    "                \n",
    "                # Show sample turns\n",
    "                print(f\"\\nüìã Sample Conversation Turns:\")\n",
    "                for i, turn in enumerate(conversation_data[:3]):\n",
    "                    query_preview = turn['customer_query'][:80] + \"...\" if len(turn['customer_query']) > 80 else turn['customer_query']\n",
    "                    response_preview = turn['chatbot_response'][:80] + \"...\" if len(turn['chatbot_response']) > 80 else turn['chatbot_response']\n",
    "                    print(f\"  Turn {turn['conversation_id']}.{turn['turn_number']} [{turn['customer_type']}]:\")\n",
    "                    print(f\"    Query: {query_preview}\")\n",
    "                    print(f\"    Response: {response_preview}\")\n",
    "                if len(conversation_data) > 3:\n",
    "                    print(f\"  ... and {len(conversation_data) - 3} more turns\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No conversation turns found in the loaded data\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå No conversations loaded from file\")\n",
    "    else:\n",
    "        print(\"‚ùå Error accessing uploaded file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please upload a JSON file with conversation results from chatbot_tester.ipynb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Quality Agent\n",
    "\n",
    "Create the Quality Agent instance with the configured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing Quality Agent with configured settings...\n",
      "‚úÖ 19:23:44.134 [INFO    ] context_manager | SQLite context provider initialized | operation=__init__\n",
      "‚úÖ 19:23:44.159 [INFO    ] factory         | Attempting to create provider: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 19:23:44.162 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 19:23:44.165 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 19:23:44.166 [INFO    ] factory         | Successfully created provider: anthropic_general_budget\n",
      "‚úÖ 19:23:44.167 [INFO    ] quality_agent   | Quality Agent LLM provider initialized | operation=initialize_llm_provider model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ Quality Agent initialized successfully!\n",
      "  Agent name: quality_agent\n",
      "  Agent version: 1.0.0\n",
      "  LLM provider: claude-3-5-haiku-20241022\n",
      "\n",
      "üìã Quality Agent Configuration:\n",
      "  Name: quality_agent\n",
      "  Description: Review chatbot answers and decide if adequate, needs adjustment, or requires human intervention\n",
      "  Version: 1.0.0\n",
      "  Quality thresholds:\n",
      "    adequate_score: 7.0\n",
      "    adjustment_score: 5.0\n",
      "  Assessment settings:\n",
      "    Use LLM evaluation: True\n",
      "    Confidence threshold: 0.7\n",
      "    Context weight: 0.3\n",
      "\n",
      "üöÄ Quality Agent is ready for conversation assessment!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Quality Agent with temporary configuration\n",
    "def initialize_quality_agent():\n",
    "    \"\"\"Initialize Quality Agent using temporary configuration files\"\"\"\n",
    "    try:\n",
    "        # Create a proper config structure for the temp config\n",
    "        # We need to set up the directory structure that AgentConfigManager expects\n",
    "        temp_config_root = temp_config_dir.parent / 'quality_agent_test_config'\n",
    "        temp_config_root.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create the expected directory structure\n",
    "        temp_shared_dir = temp_config_root / 'shared'\n",
    "        temp_agents_dir = temp_config_root / 'agents' / 'quality_agent'\n",
    "        temp_shared_dir.mkdir(parents=True, exist_ok=True)\n",
    "        temp_agents_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Copy shared config files from the main config\n",
    "        import shutil\n",
    "        main_shared_dir = Path('/workspace/config/shared')\n",
    "        if main_shared_dir.exists():\n",
    "            for config_file in main_shared_dir.glob('*.yaml'):\n",
    "                shutil.copy2(config_file, temp_shared_dir)\n",
    "        \n",
    "        # Copy our temporary agent config files to the expected location\n",
    "        shutil.copy2(temp_file_paths['agent_config'], temp_agents_dir / 'config.yaml')\n",
    "        shutil.copy2(temp_file_paths['prompts_config'], temp_agents_dir / 'prompts.yaml')\n",
    "        shutil.copy2(temp_file_paths['models_config'], temp_agents_dir / 'models.yaml')\n",
    "        \n",
    "        # Create ConfigManager pointing to temp config root\n",
    "        config_manager = ConfigManager(config_dir=temp_config_root)\n",
    "        \n",
    "        # Initialize context provider (using in-memory for testing)\n",
    "        from src.core.context_manager import SQLiteContextProvider\n",
    "        context_provider = SQLiteContextProvider(db_path=\":memory:\")\n",
    "        \n",
    "        # Create Quality Agent\n",
    "        quality_agent = QualityAgentNode(\n",
    "            config_manager=config_manager,\n",
    "            context_provider=context_provider\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Quality Agent initialized successfully!\")\n",
    "        print(f\"  Agent name: {quality_agent.agent_config.name if quality_agent.agent_config else 'Unknown'}\")\n",
    "        print(f\"  Agent version: {quality_agent.agent_config.version if quality_agent.agent_config else 'Unknown'}\")\n",
    "        print(f\"  LLM provider: {quality_agent.llm_provider.model_name if quality_agent.llm_provider else 'None'}\")\n",
    "        \n",
    "        return quality_agent, context_provider\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Quality Agent: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "quality_agent = None\n",
    "context_provider = None\n",
    "\n",
    "print(\"üîß Initializing Quality Agent with configured settings...\")\n",
    "\n",
    "# Initialize the quality agent\n",
    "quality_agent, context_provider = initialize_quality_agent()\n",
    "\n",
    "if quality_agent:\n",
    "    # Display agent configuration\n",
    "    if quality_agent.agent_config:\n",
    "        agent_info = quality_agent.agent_config.__dict__\n",
    "        settings = quality_agent.agent_config.settings\n",
    "        print(f\"\\nüìã Quality Agent Configuration:\")\n",
    "        print(f\"  Name: {agent_info.get('name', 'Unknown')}\")\n",
    "        print(f\"  Description: {agent_info.get('description', 'Unknown')}\")\n",
    "        print(f\"  Version: {agent_info.get('version', 'Unknown')}\")\n",
    "        \n",
    "        print(f\"  Quality thresholds:\")\n",
    "        quality_thresholds = settings.get('quality_thresholds', {})\n",
    "        for threshold_name, threshold_value in quality_thresholds.items():\n",
    "            print(f\"    {threshold_name}: {threshold_value}\")\n",
    "        \n",
    "        assessment_settings = settings.get('assessment', {})\n",
    "        print(f\"  Assessment settings:\")\n",
    "        print(f\"    Use LLM evaluation: {assessment_settings.get('use_llm_evaluation', True)}\")\n",
    "        print(f\"    Confidence threshold: {assessment_settings.get('confidence_threshold', 0.7)}\")\n",
    "        print(f\"    Context weight: {assessment_settings.get('context_weight', 0.3)}\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Quality Agent is ready for conversation assessment!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Quality Agent initialization failed. Please check configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 5: Run Quality Assessment\n",
    "\n",
    "Process each conversation turn through the Quality Agent to generate quality scores and assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting quality assessment process...\n",
      "üîç Running quality assessment on 50 conversation turns...\n",
      "Processing turn 1/50 (Conv 1.1)...‚úÖ 19:24:00.107 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:00.108 [ERROR   ] context_manager | Failed to get context summary | user_id=user_1 session_id=session_1 operation=get_context_summary\n",
      "‚úÖ 19:24:00.109 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_1_1 user_id=user_1 session_id=session_1\n",
      "Processing turn 2/50 (Conv 1.2)...‚úÖ 19:24:09.488 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:09.489 [ERROR   ] context_manager | Failed to get context summary | user_id=user_1 session_id=session_1 operation=get_context_summary\n",
      "‚úÖ 19:24:09.490 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_1_2 user_id=user_1 session_id=session_1\n",
      "Processing turn 3/50 (Conv 1.3)...‚úÖ 19:24:16.290 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:16.292 [ERROR   ] context_manager | Failed to get context summary | user_id=user_1 session_id=session_1 operation=get_context_summary\n",
      "‚úÖ 19:24:16.292 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_1_3 user_id=user_1 session_id=session_1\n",
      "Processing turn 4/50 (Conv 1.4)...‚úÖ 19:24:23.747 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:23.748 [ERROR   ] context_manager | Failed to get context summary | user_id=user_1 session_id=session_1 operation=get_context_summary\n",
      "‚úÖ 19:24:23.749 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_1_4 user_id=user_1 session_id=session_1\n",
      "Processing turn 5/50 (Conv 2.1)...‚úÖ 19:24:31.393 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:31.395 [ERROR   ] context_manager | Failed to get context summary | user_id=user_2 session_id=session_2 operation=get_context_summary\n",
      "‚úÖ 19:24:31.396 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_2_1 user_id=user_2 session_id=session_2\n",
      "Processing turn 6/50 (Conv 2.2)...‚úÖ 19:24:38.222 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:38.224 [ERROR   ] context_manager | Failed to get context summary | user_id=user_2 session_id=session_2 operation=get_context_summary\n",
      "‚úÖ 19:24:38.224 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_2_2 user_id=user_2 session_id=session_2\n",
      "Processing turn 7/50 (Conv 2.3)...‚úÖ 19:24:45.535 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:45.536 [ERROR   ] context_manager | Failed to get context summary | user_id=user_2 session_id=session_2 operation=get_context_summary\n",
      "‚úÖ 19:24:45.537 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_2_3 user_id=user_2 session_id=session_2\n",
      "Processing turn 8/50 (Conv 3.1)...‚úÖ 19:24:52.327 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:24:52.328 [ERROR   ] context_manager | Failed to get context summary | user_id=user_3 session_id=session_3 operation=get_context_summary\n",
      "‚úÖ 19:24:52.329 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_3_1 user_id=user_3 session_id=session_3\n",
      "Processing turn 9/50 (Conv 3.2)...‚úÖ 19:25:00.192 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:00.193 [ERROR   ] context_manager | Failed to get context summary | user_id=user_3 session_id=session_3 operation=get_context_summary\n",
      "‚úÖ 19:25:00.194 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_3_2 user_id=user_3 session_id=session_3\n",
      "Processing turn 10/50 (Conv 4.1)...‚úÖ 19:25:07.950 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:07.952 [ERROR   ] context_manager | Failed to get context summary | user_id=user_4 session_id=session_4 operation=get_context_summary\n",
      "‚úÖ 19:25:07.953 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_4_1 user_id=user_4 session_id=session_4\n",
      "Processing turn 11/50 (Conv 5.1)...‚úÖ 19:25:16.684 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:16.685 [ERROR   ] context_manager | Failed to get context summary | user_id=user_5 session_id=session_5 operation=get_context_summary\n",
      "‚úÖ 19:25:16.686 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_5_1 user_id=user_5 session_id=session_5\n",
      "Processing turn 12/50 (Conv 5.2)...‚úÖ 19:25:23.923 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:23.925 [ERROR   ] context_manager | Failed to get context summary | user_id=user_5 session_id=session_5 operation=get_context_summary\n",
      "‚úÖ 19:25:32.026 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:25:32.028 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_5_2 user_id=user_5 session_id=session_5\n",
      "Processing turn 13/50 (Conv 5.3)...‚úÖ 19:25:39.760 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:39.762 [ERROR   ] context_manager | Failed to get context summary | user_id=user_5 session_id=session_5 operation=get_context_summary\n",
      "‚úÖ 19:25:46.485 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:25:46.486 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_5_3 user_id=user_5 session_id=session_5\n",
      "Processing turn 14/50 (Conv 5.4)...‚úÖ 19:25:54.291 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:25:54.292 [ERROR   ] context_manager | Failed to get context summary | user_id=user_5 session_id=session_5 operation=get_context_summary\n",
      "‚úÖ 19:25:54.293 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_5_4 user_id=user_5 session_id=session_5\n",
      "Processing turn 15/50 (Conv 5.5)...‚úÖ 19:26:01.397 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:01.398 [ERROR   ] context_manager | Failed to get context summary | user_id=user_5 session_id=session_5 operation=get_context_summary\n",
      "‚úÖ 19:26:01.407 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_5_5 user_id=user_5 session_id=session_5\n",
      "Processing turn 16/50 (Conv 6.1)...‚úÖ 19:26:08.324 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:08.325 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:08.326 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_1 user_id=user_6 session_id=session_6\n",
      "Processing turn 17/50 (Conv 6.2)...‚úÖ 19:26:15.990 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:15.991 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:15.992 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_2 user_id=user_6 session_id=session_6\n",
      "Processing turn 18/50 (Conv 6.3)...‚úÖ 19:26:24.267 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:24.269 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:24.269 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_3 user_id=user_6 session_id=session_6\n",
      "Processing turn 19/50 (Conv 6.4)...‚úÖ 19:26:32.495 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:32.496 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:32.497 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_4 user_id=user_6 session_id=session_6\n",
      "Processing turn 20/50 (Conv 6.5)...‚úÖ 19:26:38.907 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:38.908 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:38.909 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_5 user_id=user_6 session_id=session_6\n",
      "Processing turn 21/50 (Conv 6.6)...‚úÖ 19:26:45.878 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:45.879 [ERROR   ] context_manager | Failed to get context summary | user_id=user_6 session_id=session_6 operation=get_context_summary\n",
      "‚úÖ 19:26:45.880 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_6_6 user_id=user_6 session_id=session_6\n",
      "Processing turn 22/50 (Conv 7.1)...‚úÖ 19:26:53.072 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:26:53.073 [ERROR   ] context_manager | Failed to get context summary | user_id=user_7 session_id=session_7 operation=get_context_summary\n",
      "‚úÖ 19:26:53.082 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_7_1 user_id=user_7 session_id=session_7\n",
      "Processing turn 23/50 (Conv 7.2)...‚úÖ 19:27:01.845 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:27:01.846 [ERROR   ] context_manager | Failed to get context summary | user_id=user_7 session_id=session_7 operation=get_context_summary\n",
      "‚úÖ 19:27:01.847 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_7_2 user_id=user_7 session_id=session_7\n",
      "Processing turn 24/50 (Conv 7.3)...‚úÖ 19:27:09.647 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:27:09.648 [ERROR   ] context_manager | Failed to get context summary | user_id=user_7 session_id=session_7 operation=get_context_summary\n",
      "‚úÖ 19:27:09.649 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_7_3 user_id=user_7 session_id=session_7\n",
      "Processing turn 25/50 (Conv 8.1)...‚úÖ 19:27:17.391 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:27:17.392 [ERROR   ] context_manager | Failed to get context summary | user_id=user_8 session_id=session_8 operation=get_context_summary\n",
      "‚úÖ 19:27:24.345 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:27:24.346 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_8_1 user_id=user_8 session_id=session_8\n",
      "Processing turn 26/50 (Conv 8.2)...‚úÖ 19:27:34.056 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:27:34.057 [ERROR   ] context_manager | Failed to get context summary | user_id=user_8 session_id=session_8 operation=get_context_summary\n",
      "‚úÖ 19:27:40.670 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:27:40.672 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_8_2 user_id=user_8 session_id=session_8\n",
      "Processing turn 27/50 (Conv 8.3)...‚úÖ 19:27:48.791 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:27:48.793 [ERROR   ] context_manager | Failed to get context summary | user_id=user_8 session_id=session_8 operation=get_context_summary\n",
      "‚úÖ 19:27:54.711 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:27:54.712 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_8_3 user_id=user_8 session_id=session_8\n",
      "Processing turn 28/50 (Conv 9.1)...‚úÖ 19:28:01.522 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:01.523 [ERROR   ] context_manager | Failed to get context summary | user_id=user_9 session_id=session_9 operation=get_context_summary\n",
      "‚úÖ 19:28:01.524 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_9_1 user_id=user_9 session_id=session_9\n",
      "Processing turn 29/50 (Conv 10.1)...‚úÖ 19:28:09.699 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:09.700 [ERROR   ] context_manager | Failed to get context summary | user_id=user_10 session_id=session_10 operation=get_context_summary\n",
      "‚úÖ 19:28:09.701 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_10_1 user_id=user_10 session_id=session_10\n",
      "Processing turn 30/50 (Conv 11.1)...‚úÖ 19:28:18.337 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:18.338 [ERROR   ] context_manager | Failed to get context summary | user_id=user_11 session_id=session_11 operation=get_context_summary\n",
      "‚úÖ 19:28:18.339 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_11_1 user_id=user_11 session_id=session_11\n",
      "Processing turn 31/50 (Conv 11.2)...‚úÖ 19:28:25.381 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:25.382 [ERROR   ] context_manager | Failed to get context summary | user_id=user_11 session_id=session_11 operation=get_context_summary\n",
      "‚úÖ 19:28:33.851 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:28:33.852 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_11_2 user_id=user_11 session_id=session_11\n",
      "Processing turn 32/50 (Conv 12.1)...‚úÖ 19:28:40.840 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:40.841 [ERROR   ] context_manager | Failed to get context summary | user_id=user_12 session_id=session_12 operation=get_context_summary\n",
      "‚úÖ 19:28:40.851 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_12_1 user_id=user_12 session_id=session_12\n",
      "Processing turn 33/50 (Conv 12.2)...‚úÖ 19:28:48.316 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:48.317 [ERROR   ] context_manager | Failed to get context summary | user_id=user_12 session_id=session_12 operation=get_context_summary\n",
      "‚úÖ 19:28:48.318 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_12_2 user_id=user_12 session_id=session_12\n",
      "Processing turn 34/50 (Conv 12.3)...‚úÖ 19:28:54.760 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:28:54.762 [ERROR   ] context_manager | Failed to get context summary | user_id=user_12 session_id=session_12 operation=get_context_summary\n",
      "‚úÖ 19:28:54.763 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_12_3 user_id=user_12 session_id=session_12\n",
      "Processing turn 35/50 (Conv 12.4)...‚úÖ 19:29:02.267 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:02.268 [ERROR   ] context_manager | Failed to get context summary | user_id=user_12 session_id=session_12 operation=get_context_summary\n",
      "‚úÖ 19:29:10.563 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:29:10.564 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_12_4 user_id=user_12 session_id=session_12\n",
      "Processing turn 36/50 (Conv 13.1)...‚úÖ 19:29:17.290 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:17.291 [ERROR   ] context_manager | Failed to get context summary | user_id=user_13 session_id=session_13 operation=get_context_summary\n",
      "‚úÖ 19:29:17.292 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_13_1 user_id=user_13 session_id=session_13\n",
      "Processing turn 37/50 (Conv 13.2)...‚úÖ 19:29:25.558 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:25.559 [ERROR   ] context_manager | Failed to get context summary | user_id=user_13 session_id=session_13 operation=get_context_summary\n",
      "‚úÖ 19:29:25.560 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_13_2 user_id=user_13 session_id=session_13\n",
      "Processing turn 38/50 (Conv 14.1)...‚úÖ 19:29:33.361 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:33.363 [ERROR   ] context_manager | Failed to get context summary | user_id=user_14 session_id=session_14 operation=get_context_summary\n",
      "‚úÖ 19:29:33.364 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_14_1 user_id=user_14 session_id=session_14\n",
      "Processing turn 39/50 (Conv 14.2)...‚úÖ 19:29:41.917 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:41.919 [ERROR   ] context_manager | Failed to get context summary | user_id=user_14 session_id=session_14 operation=get_context_summary\n",
      "‚úÖ 19:29:41.920 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_14_2 user_id=user_14 session_id=session_14\n",
      "Processing turn 40/50 (Conv 14.3)...‚úÖ 19:29:49.658 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:49.660 [ERROR   ] context_manager | Failed to get context summary | user_id=user_14 session_id=session_14 operation=get_context_summary\n",
      "‚úÖ 19:29:49.660 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_14_3 user_id=user_14 session_id=session_14\n",
      "Processing turn 41/50 (Conv 15.1)...‚úÖ 19:29:56.115 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:29:56.116 [ERROR   ] context_manager | Failed to get context summary | user_id=user_15 session_id=session_15 operation=get_context_summary\n",
      "‚úÖ 19:29:56.117 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_15_1 user_id=user_15 session_id=session_15\n",
      "Processing turn 42/50 (Conv 15.2)...‚úÖ 19:30:03.249 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:03.251 [ERROR   ] context_manager | Failed to get context summary | user_id=user_15 session_id=session_15 operation=get_context_summary\n",
      "‚úÖ 19:30:03.252 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_15_2 user_id=user_15 session_id=session_15\n",
      "Processing turn 43/50 (Conv 16.1)...‚úÖ 19:30:10.987 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:10.988 [ERROR   ] context_manager | Failed to get context summary | user_id=user_16 session_id=session_16 operation=get_context_summary\n",
      "‚úÖ 19:30:10.989 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_16_1 user_id=user_16 session_id=session_16\n",
      "Processing turn 44/50 (Conv 17.1)...‚úÖ 19:30:18.168 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:18.169 [ERROR   ] context_manager | Failed to get context summary | user_id=user_17 session_id=session_17 operation=get_context_summary\n",
      "‚úÖ 19:30:24.648 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:30:24.649 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_17_1 user_id=user_17 session_id=session_17\n",
      "Processing turn 45/50 (Conv 18.1)...‚úÖ 19:30:31.803 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:31.813 [ERROR   ] context_manager | Failed to get context summary | user_id=user_18 session_id=session_18 operation=get_context_summary\n",
      "‚úÖ 19:30:31.814 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_18_1 user_id=user_18 session_id=session_18\n",
      "Processing turn 46/50 (Conv 18.2)...‚úÖ 19:30:39.364 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:39.365 [ERROR   ] context_manager | Failed to get context summary | user_id=user_18 session_id=session_18 operation=get_context_summary\n",
      "‚úÖ 19:30:39.366 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_18_2 user_id=user_18 session_id=session_18\n",
      "Processing turn 47/50 (Conv 18.3)...‚úÖ 19:30:47.417 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:30:47.419 [ERROR   ] context_manager | Failed to get context summary | user_id=user_18 session_id=session_18 operation=get_context_summary\n",
      "‚úÖ 19:30:54.278 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:30:54.279 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_18_3 user_id=user_18 session_id=session_18\n",
      "Processing turn 48/50 (Conv 19.1)...‚úÖ 19:31:01.014 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:31:01.015 [ERROR   ] context_manager | Failed to get context summary | user_id=user_19 session_id=session_19 operation=get_context_summary\n",
      "‚úÖ 19:31:01.016 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_19_1 user_id=user_19 session_id=session_19\n",
      "Processing turn 49/50 (Conv 19.2)...‚úÖ 19:31:08.593 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:31:08.594 [ERROR   ] context_manager | Failed to get context summary | user_id=user_19 session_id=session_19 operation=get_context_summary\n",
      "‚úÖ 19:31:08.595 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_19_2 user_id=user_19 session_id=session_19\n",
      "Processing turn 50/50 (Conv 20.1)...‚úÖ 19:31:14.921 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚ùå 19:31:14.923 [ERROR   ] context_manager | Failed to get context summary | user_id=user_20 session_id=session_20 operation=get_context_summary\n",
      "‚úÖ 19:31:20.973 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "‚úÖ 19:31:20.974 [INFO    ] quality_agent   | Quality assessment completed | operation=quality_assessment query_id=quality_test_20_1 user_id=user_20 session_id=session_20\n",
      "\n",
      "‚úÖ Quality assessment completed for 50 turns!\n",
      "\n",
      "üìä Quality Assessment Summary:\n",
      "  Total turns assessed: 50\n",
      "  Average quality score: 6.92\n",
      "  Quality score range: 3.00 - 9.00\n",
      "\n",
      "üìã Quality Decisions:\n",
      "  adequate: 35 (70.0%)\n",
      "  needs_adjustment: 10 (20.0%)\n",
      "  human_intervention: 5 (10.0%)\n",
      "\n",
      "üìù Sample Quality Assessments:\n",
      "\n",
      "  Turn 1.1 [frustrated]:\n",
      "    Quality Score: 7.00/10.0\n",
      "    Decision: adequate\n",
      "    Confidence: 0.70\n",
      "    Reasoning: LLM assessment: Assessment:\n",
      "\n",
      "\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Positive Aspects:\n",
      "- Empathetic tone acknowledges the customer's frustration\n",
      "- Demonstrates active listening...\n",
      "\n",
      "  Turn 1.2 [frustrated]:\n",
      "    Quality Score: 4.00/10.0\n",
      "    Decision: human_intervention\n",
      "    Confidence: 0.50\n",
      "    Reasoning: LLM assessment: Assessment of Chatbot Response:\n",
      "\n",
      "\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Accuracy (3/10):\n",
      "- The response lacks specific details about the actual reason for the ...\n",
      "\n",
      "  Turn 1.3 [frustrated]:\n",
      "    Quality Score: 7.00/10.0\n",
      "    Decision: adequate\n",
      "    Confidence: 0.70\n",
      "    Reasoning: LLM assessment: Reasoning:\n",
      "Positive Aspects:\n",
      "- The response shows empathy and a genuine desire to help\n",
      "- Uses a friendly, approachable tone\n",
      "- Demonstr...\n",
      "  ... and 47 more assessments\n",
      "\n",
      "‚úÖ Quality assessment data ready for analysis and export!\n"
     ]
    }
   ],
   "source": [
    "# Run quality assessment on all conversation turns\n",
    "quality_results = []\n",
    "\n",
    "def create_state_for_turn(turn_data):\n",
    "    \"\"\"Create HybridSystemState for a conversation turn\"\"\"\n",
    "    return HybridSystemState({\n",
    "        \"query_id\": f\"quality_test_{turn_data['conversation_id']}_{turn_data['turn_number']}\",\n",
    "        \"user_id\": f\"user_{turn_data['conversation_id']}\",\n",
    "        \"session_id\": f\"session_{turn_data['conversation_id']}\",\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"query\": turn_data['customer_query'],\n",
    "        \"ai_response\": turn_data['chatbot_response'],\n",
    "        \"customer_type\": turn_data['customer_type'],\n",
    "        \"complexity\": turn_data['complexity'],\n",
    "        \"conversation_metadata\": turn_data['conversation_metadata']\n",
    "    })\n",
    "\n",
    "def assess_conversation_quality(conversation_turns):\n",
    "    \"\"\"Run quality assessment on conversation turns\"\"\"\n",
    "    if not quality_agent:\n",
    "        print(\"‚ùå Quality Agent not initialized. Please run the previous step.\")\n",
    "        return []\n",
    "    \n",
    "    if not conversation_turns:\n",
    "        print(\"‚ùå No conversation turns to assess. Please load conversation data first.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üîç Running quality assessment on {len(conversation_turns)} conversation turns...\")\n",
    "    \n",
    "    assessed_turns = []\n",
    "    \n",
    "    for i, turn in enumerate(conversation_turns):\n",
    "        try:\n",
    "            # Create state for this turn\n",
    "            state = create_state_for_turn(turn)\n",
    "            \n",
    "            print(f\"\\rProcessing turn {i+1}/{len(conversation_turns)} (Conv {turn['conversation_id']}.{turn['turn_number']})...\", end='', flush=True)\n",
    "            \n",
    "            # Run quality assessment\n",
    "            result_state = quality_agent(state)\n",
    "            \n",
    "            # Extract quality assessment results\n",
    "            quality_assessment = result_state.get('quality_assessment', {})\n",
    "            next_action = result_state.get('next_action', 'unknown')\n",
    "            \n",
    "            # Create enhanced turn data with quality metrics\n",
    "            enhanced_turn = {\n",
    "                **turn,  # Original turn data\n",
    "                'quality_assessment': quality_assessment,\n",
    "                'next_action': next_action,\n",
    "                'quality_score': quality_assessment.get('overall_score', 0.0),\n",
    "                'quality_decision': quality_assessment.get('decision', 'unknown'),\n",
    "                'quality_confidence': quality_assessment.get('confidence', 0.0),\n",
    "                'quality_reasoning': quality_assessment.get('reasoning', ''),\n",
    "                'adjustment_needed': quality_assessment.get('adjustment_needed', False),\n",
    "                'adjusted_response': quality_assessment.get('adjusted_response'),\n",
    "                'assessment_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            assessed_turns.append(enhanced_turn)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error assessing turn {turn['conversation_id']}.{turn['turn_number']}: {e}\")\n",
    "            # Add turn with error information\n",
    "            error_turn = {\n",
    "                **turn,\n",
    "                'quality_assessment': {'error': str(e)},\n",
    "                'quality_score': 0.0,\n",
    "                'quality_decision': 'error',\n",
    "                'quality_confidence': 0.0,\n",
    "                'quality_reasoning': f'Assessment error: {str(e)}',\n",
    "                'adjustment_needed': False,\n",
    "                'assessment_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            assessed_turns.append(error_turn)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Quality assessment completed for {len(assessed_turns)} turns!\")\n",
    "    return assessed_turns\n",
    "\n",
    "# Run assessment if we have data and agent\n",
    "if conversation_data and quality_agent:\n",
    "    print(\"üöÄ Starting quality assessment process...\")\n",
    "    quality_results = assess_conversation_quality(conversation_data)\n",
    "    \n",
    "    if quality_results:\n",
    "        # Display summary statistics\n",
    "        df_results = pd.DataFrame(quality_results)\n",
    "        \n",
    "        print(f\"\\nüìä Quality Assessment Summary:\")\n",
    "        print(f\"  Total turns assessed: {len(quality_results)}\")\n",
    "        print(f\"  Average quality score: {df_results['quality_score'].mean():.2f}\")\n",
    "        print(f\"  Quality score range: {df_results['quality_score'].min():.2f} - {df_results['quality_score'].max():.2f}\")\n",
    "        \n",
    "        # Decision distribution\n",
    "        decision_counts = df_results['quality_decision'].value_counts()\n",
    "        print(f\"\\nüìã Quality Decisions:\")\n",
    "        for decision, count in decision_counts.items():\n",
    "            percentage = count / len(quality_results) * 100\n",
    "            print(f\"  {decision}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show sample assessments\n",
    "        print(f\"\\nüìù Sample Quality Assessments:\")\n",
    "        for i, result in enumerate(quality_results[:3]):\n",
    "            print(f\"\\n  Turn {result['conversation_id']}.{result['turn_number']} [{result['customer_type']}]:\")\n",
    "            print(f\"    Quality Score: {result['quality_score']:.2f}/10.0\")\n",
    "            print(f\"    Decision: {result['quality_decision']}\")\n",
    "            print(f\"    Confidence: {result['quality_confidence']:.2f}\")\n",
    "            reasoning = result['quality_reasoning'][:150] + \"...\" if len(result['quality_reasoning']) > 150 else result['quality_reasoning']\n",
    "            print(f\"    Reasoning: {reasoning}\")\n",
    "            if result['adjustment_needed'] and result.get('adjusted_response'):\n",
    "                adjusted = result['adjusted_response'][:100] + \"...\" if len(result['adjusted_response']) > 100 else result['adjusted_response']\n",
    "                print(f\"    Adjusted Response: {adjusted}\")\n",
    "        \n",
    "        if len(quality_results) > 3:\n",
    "            print(f\"  ... and {len(quality_results) - 3} more assessments\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ Quality assessment data ready for analysis and export!\")\n",
    "    \n",
    "elif not conversation_data:\n",
    "    print(\"‚ö†Ô∏è No conversation data loaded. Please upload conversation results in Step 3.\")\n",
    "elif not quality_agent:\n",
    "    print(\"‚ö†Ô∏è Quality Agent not initialized. Please run Step 4 first.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Unable to start quality assessment. Please check previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Quality Assessment Results\n",
    "\n",
    "Detailed analysis of quality patterns, scores, and improvement recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Quality Assessment Analysis Dashboard\n",
      "============================================================\n",
      "\n",
      "üéØ OVERALL QUALITY STATISTICS\n",
      "Total turns assessed: 50\n",
      "Mean quality score: 6.92/10.0\n",
      "Median quality score: 7.00/10.0\n",
      "Standard deviation: 1.58\n",
      "Score range: 3.00 - 9.00\n",
      "\n",
      "üìã QUALITY DECISION BREAKDOWN\n",
      "  Adequate: 35 (70.0%)\n",
      "  Needs Adjustment: 10 (20.0%)\n",
      "  Human Intervention: 5 (10.0%)\n",
      "\n",
      "üë• QUALITY BY CUSTOMER TYPE\n",
      "  Frustrated:\n",
      "    Average quality: 6.33/10.0\n",
      "    Average confidence: 0.65\n",
      "    Adjustment rate: 20.0%\n",
      "    Sample size: 15\n",
      "  Urgent:\n",
      "    Average quality: 6.20/10.0\n",
      "    Average confidence: 0.65\n",
      "    Adjustment rate: 26.7%\n",
      "    Sample size: 15\n",
      "  Confused:\n",
      "    Average quality: 8.07/10.0\n",
      "    Average confidence: 0.81\n",
      "    Adjustment rate: 14.3%\n",
      "    Sample size: 14\n",
      "  Normal:\n",
      "    Average quality: 7.50/10.0\n",
      "    Average confidence: 0.75\n",
      "    Adjustment rate: 16.7%\n",
      "    Sample size: 6\n",
      "\n",
      "üî¢ QUALITY BY COMPLEXITY LEVEL\n",
      "  Medium Complexity:\n",
      "    Average quality: 7.30/10.0\n",
      "    Average confidence: 0.73\n",
      "    Human escalation rate: 5.0%\n",
      "    Sample size: 20\n",
      "  Simple Complexity:\n",
      "    Average quality: 7.08/10.0\n",
      "    Average confidence: 0.71\n",
      "    Human escalation rate: 0.0%\n",
      "    Sample size: 12\n",
      "  Complex Complexity:\n",
      "    Average quality: 6.39/10.0\n",
      "    Average confidence: 0.67\n",
      "    Human escalation rate: 22.2%\n",
      "    Sample size: 18\n",
      "\n",
      "üõ†Ô∏è RESPONSE IMPROVEMENT ANALYSIS\n",
      "Turns needing adjustment: 10 (20.0%)\n",
      "Adjusted responses generated: 10\n",
      "\n",
      "üìà QUALITY SCORE DISTRIBUTION\n",
      "  Excellent (9.0-10.0): 7 (14.0%)\n",
      "  Very Good (8.0-8.99): 13 (26.0%)\n",
      "  Good (7.0-7.99): 15 (30.0%)\n",
      "  Acceptable (6.0-6.99): 6 (12.0%)\n",
      "  Needs Improvement (5.0-5.99): 4 (8.0%)\n",
      "  Poor (0.0-4.99): 5 (10.0%)\n",
      "\n",
      "üéØ CONFIDENCE vs QUALITY CORRELATION\n",
      "High confidence + High quality: 20 (40.0%)\n",
      "Low confidence + Low quality: 9 (18.0%)\n",
      "\n",
      "üìù DETAILED EXAMPLES BY QUALITY DECISION\n",
      "\n",
      "‚úÖ ADEQUATE RESPONSES:\n",
      "  Example 1.1 (Score: 7.0):\n",
      "    Query: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!...\n",
      "    Response:   Sorry to hear that your premium increased by $200, [Customer Name]. I can understand why this woul...\n",
      "    Reasoning: LLM assessment: Assessment:\n",
      "\n",
      "\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Positive Aspects:\n",
      "- Empathetic tone a...\n",
      "  Example 1.3 (Score: 7.0):\n",
      "    Query: This isn't working. I need to speak to someone who can actually help me!...\n",
      "    Response:   Hello there! üòä I apologize for any inconvenience you're experiencing. I'm here to help you resolve...\n",
      "    Reasoning: LLM assessment: Reasoning:\n",
      "Positive Aspects:\n",
      "- The response shows empathy and a ...\n",
      "\n",
      "üîß NEEDS ADJUSTMENT:\n",
      "  Example 5.2 (Score: 6.0):\n",
      "    Query: \"ARE YOU KIDDING ME?! After 15 years of loyalty, you're going to deny my claim without even giving m...\n",
      "    Original:   Hello there, and thank you for reaching out to us about your frustrating experience with our custo...\n",
      "    Adjusted: Here's an improved response addressing the quality issues:\n",
      "\n",
      "\"I understand your frustration, and I ap...\n",
      "    Reasoning: LLM assessment: Assessment of Chatbot Response:\n",
      "\n",
      "\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Positive Aspects:...\n",
      "  Example 5.3 (Score: 6.0):\n",
      "    Query: This isn't working. I need to speak to someone who can actually help me!...\n",
      "    Original:   Hello there! I'm so sorry to hear that you're experiencing an issue with our service. üòû Can you pl...\n",
      "    Adjusted: Here's an improved response addressing the identified quality issues:\n",
      "\n",
      "\"I understand you're frustrat...\n",
      "    Reasoning: LLM assessment: Reasoning:\n",
      "\n",
      "Positive Aspects:\n",
      "- The response shows empathy and a...\n",
      "\n",
      "üö® HUMAN INTERVENTION:\n",
      "  Example 1.2 (Score: 4.0):\n",
      "    Query: \"ARE YOU KIDDING ME?! I DON'T HAVE TIME FOR THIS NONSENSE! I JUST WANT TO KNOW WHY MY PREMIUM INCREA...\n",
      "    Response:   Oh no, I'm so sorry to hear that you're dealing with an unexpected premium increase! üòî I completel...\n",
      "    Reasoning: LLM assessment: Assessment of Chatbot Response:\n",
      "\n",
      "\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Accuracy (3/10):\n",
      "...\n",
      "  Example 5.4 (Score: 3.0):\n",
      "    Query: Finally! Thank you for the explanation....\n",
      "    Response:   Of course! I'm here to help. Thank you for reaching out to us. I understand that you're looking fo...\n",
      "    Reasoning: LLM assessment: Reasoning:\n",
      "This response is highly inappropriate and misses seve...\n",
      "\n",
      "üìä QUALITY ASSESSMENT SUMMARY TABLE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>turn_number</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>quality_decision</th>\n",
       "      <th>quality_confidence</th>\n",
       "      <th>adjustment_needed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>4.0</td>\n",
       "      <td>human_intervention</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>7.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>7.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>7.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "      <td>9.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adequate</td>\n",
       "      <td>0.8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id  turn_number customer_type complexity  quality_score  \\\n",
       "0                1            1    frustrated     medium            7.0   \n",
       "1                1            2    frustrated     medium            4.0   \n",
       "2                1            3    frustrated     medium            7.0   \n",
       "3                1            4    frustrated     medium            7.0   \n",
       "4                2            1        urgent     simple            7.0   \n",
       "5                2            2        urgent     simple            7.0   \n",
       "6                2            3        urgent     simple            8.0   \n",
       "7                3            1      confused     simple            8.0   \n",
       "8                3            2      confused     simple            9.0   \n",
       "9                4            1        normal     simple            8.0   \n",
       "\n",
       "     quality_decision  quality_confidence  adjustment_needed  \n",
       "0            adequate                 0.7              False  \n",
       "1  human_intervention                 0.5              False  \n",
       "2            adequate                 0.7              False  \n",
       "3            adequate                 0.7              False  \n",
       "4            adequate                 0.7              False  \n",
       "5            adequate                 0.7              False  \n",
       "6            adequate                 0.8              False  \n",
       "7            adequate                 0.8              False  \n",
       "8            adequate                 0.9              False  \n",
       "9            adequate                 0.8              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... showing first 10 of 50 assessed turns\n",
      "\n",
      "============================================================\n",
      "‚úÖ Quality analysis complete! Use Step 7 to export detailed results.\n"
     ]
    }
   ],
   "source": [
    "# Detailed analysis of quality assessment results\n",
    "if quality_results:\n",
    "    print(\"üìä Quality Assessment Analysis Dashboard\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_quality = pd.DataFrame(quality_results)\n",
    "    \n",
    "    # Overall Quality Statistics\n",
    "    print(f\"\\nüéØ OVERALL QUALITY STATISTICS\")\n",
    "    print(f\"Total turns assessed: {len(df_quality)}\")\n",
    "    print(f\"Mean quality score: {df_quality['quality_score'].mean():.2f}/10.0\")\n",
    "    print(f\"Median quality score: {df_quality['quality_score'].median():.2f}/10.0\")\n",
    "    print(f\"Standard deviation: {df_quality['quality_score'].std():.2f}\")\n",
    "    print(f\"Score range: {df_quality['quality_score'].min():.2f} - {df_quality['quality_score'].max():.2f}\")\n",
    "    \n",
    "    # Quality Decision Breakdown\n",
    "    print(f\"\\nüìã QUALITY DECISION BREAKDOWN\")\n",
    "    decision_analysis = df_quality['quality_decision'].value_counts()\n",
    "    for decision, count in decision_analysis.items():\n",
    "        percentage = count / len(df_quality) * 100\n",
    "        print(f\"  {decision.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Quality by Customer Type\n",
    "    print(f\"\\nüë• QUALITY BY CUSTOMER TYPE\")\n",
    "    customer_type_analysis = df_quality.groupby('customer_type').agg({\n",
    "        'quality_score': ['mean', 'std', 'count'],\n",
    "        'quality_confidence': 'mean',\n",
    "        'adjustment_needed': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    for customer_type in df_quality['customer_type'].unique():\n",
    "        type_data = df_quality[df_quality['customer_type'] == customer_type]\n",
    "        avg_score = type_data['quality_score'].mean()\n",
    "        avg_confidence = type_data['quality_confidence'].mean()\n",
    "        adjustment_rate = type_data['adjustment_needed'].sum() / len(type_data) * 100\n",
    "        print(f\"  {customer_type.title()}:\")\n",
    "        print(f\"    Average quality: {avg_score:.2f}/10.0\")\n",
    "        print(f\"    Average confidence: {avg_confidence:.2f}\")\n",
    "        print(f\"    Adjustment rate: {adjustment_rate:.1f}%\")\n",
    "        print(f\"    Sample size: {len(type_data)}\")\n",
    "    \n",
    "    # Quality by Complexity\n",
    "    print(f\"\\nüî¢ QUALITY BY COMPLEXITY LEVEL\")\n",
    "    for complexity in df_quality['complexity'].unique():\n",
    "        complexity_data = df_quality[df_quality['complexity'] == complexity]\n",
    "        avg_score = complexity_data['quality_score'].mean()\n",
    "        avg_confidence = complexity_data['quality_confidence'].mean()\n",
    "        escalation_rate = len(complexity_data[complexity_data['quality_decision'] == 'human_intervention']) / len(complexity_data) * 100\n",
    "        print(f\"  {complexity.title()} Complexity:\")\n",
    "        print(f\"    Average quality: {avg_score:.2f}/10.0\")\n",
    "        print(f\"    Average confidence: {avg_confidence:.2f}\")\n",
    "        print(f\"    Human escalation rate: {escalation_rate:.1f}%\")\n",
    "        print(f\"    Sample size: {len(complexity_data)}\")\n",
    "    \n",
    "    # Response Improvement Analysis\n",
    "    print(f\"\\nüõ†Ô∏è RESPONSE IMPROVEMENT ANALYSIS\")\n",
    "    adjustment_needed = df_quality['adjustment_needed'].sum()\n",
    "    adjusted_responses = len([r for r in quality_results if r.get('adjusted_response')])\n",
    "    print(f\"Turns needing adjustment: {adjustment_needed} ({adjustment_needed/len(df_quality)*100:.1f}%)\")\n",
    "    print(f\"Adjusted responses generated: {adjusted_responses}\")\n",
    "    \n",
    "    # Show quality distribution\n",
    "    print(f\"\\nüìà QUALITY SCORE DISTRIBUTION\")\n",
    "    quality_ranges = [\n",
    "        (9.0, 10.0, \"Excellent\"),\n",
    "        (8.0, 8.99, \"Very Good\"),\n",
    "        (7.0, 7.99, \"Good\"),\n",
    "        (6.0, 6.99, \"Acceptable\"),\n",
    "        (5.0, 5.99, \"Needs Improvement\"),\n",
    "        (0.0, 4.99, \"Poor\")\n",
    "    ]\n",
    "    \n",
    "    for min_score, max_score, label in quality_ranges:\n",
    "        count = len(df_quality[(df_quality['quality_score'] >= min_score) & (df_quality['quality_score'] <= max_score)])\n",
    "        percentage = count / len(df_quality) * 100\n",
    "        print(f\"  {label} ({min_score}-{max_score}): {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Confidence vs Score Analysis\n",
    "    print(f\"\\nüéØ CONFIDENCE vs QUALITY CORRELATION\")\n",
    "    high_conf_high_qual = len(df_quality[(df_quality['quality_confidence'] >= 0.8) & (df_quality['quality_score'] >= 8.0)])\n",
    "    low_conf_low_qual = len(df_quality[(df_quality['quality_confidence'] <= 0.5) & (df_quality['quality_score'] <= 5.0)])\n",
    "    print(f\"High confidence + High quality: {high_conf_high_qual} ({high_conf_high_qual/len(df_quality)*100:.1f}%)\")\n",
    "    print(f\"Low confidence + Low quality: {low_conf_low_qual} ({low_conf_low_qual/len(df_quality)*100:.1f}%)\")\n",
    "    \n",
    "    # Show detailed examples of each quality decision type\n",
    "    print(f\"\\nüìù DETAILED EXAMPLES BY QUALITY DECISION\")\n",
    "    \n",
    "    # Adequate responses\n",
    "    adequate_examples = df_quality[df_quality['quality_decision'] == 'adequate'].head(2)\n",
    "    if len(adequate_examples) > 0:\n",
    "        print(f\"\\n‚úÖ ADEQUATE RESPONSES:\")\n",
    "        for idx, row in adequate_examples.iterrows():\n",
    "            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n",
    "            print(f\"    Query: {row['customer_query'][:100]}...\")\n",
    "            print(f\"    Response: {row['chatbot_response'][:100]}...\")\n",
    "            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n",
    "    \n",
    "    # Needs adjustment\n",
    "    adjustment_examples = df_quality[df_quality['quality_decision'] == 'needs_adjustment'].head(2)\n",
    "    if len(adjustment_examples) > 0:\n",
    "        print(f\"\\nüîß NEEDS ADJUSTMENT:\")\n",
    "        for idx, row in adjustment_examples.iterrows():\n",
    "            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n",
    "            print(f\"    Query: {row['customer_query'][:100]}...\")\n",
    "            print(f\"    Original: {row['chatbot_response'][:100]}...\")\n",
    "            if row.get('adjusted_response'):\n",
    "                print(f\"    Adjusted: {row['adjusted_response'][:100]}...\")\n",
    "            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n",
    "    \n",
    "    # Human intervention\n",
    "    escalation_examples = df_quality[df_quality['quality_decision'] == 'human_intervention'].head(2)\n",
    "    if len(escalation_examples) > 0:\n",
    "        print(f\"\\nüö® HUMAN INTERVENTION:\")\n",
    "        for idx, row in escalation_examples.iterrows():\n",
    "            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n",
    "            print(f\"    Query: {row['customer_query'][:100]}...\")\n",
    "            print(f\"    Response: {row['chatbot_response'][:100]}...\")\n",
    "            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n",
    "    \n",
    "    # Show summary table\n",
    "    print(f\"\\nüìä QUALITY ASSESSMENT SUMMARY TABLE\")\n",
    "    summary_df = df_quality[['conversation_id', 'turn_number', 'customer_type', 'complexity', \n",
    "                           'quality_score', 'quality_decision', 'quality_confidence', \n",
    "                           'adjustment_needed']].copy()\n",
    "    summary_df['quality_score'] = summary_df['quality_score'].round(1)\n",
    "    summary_df['quality_confidence'] = summary_df['quality_confidence'].round(2)\n",
    "    \n",
    "    display(summary_df.head(10))\n",
    "    \n",
    "    if len(summary_df) > 10:\n",
    "        print(f\"... showing first 10 of {len(summary_df)} assessed turns\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Quality analysis complete! Use Step 7 to export detailed results.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No quality assessment results to analyze. Please run quality assessment in Step 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 7: Export Quality Assessment Results\n",
    "\n",
    "Save the enhanced conversation data with quality metrics to files with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting Quality Assessment Results and Configuration\n",
      "============================================================\n",
      "‚úÖ Quality assessment results exported to: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_assessment_results.json\n",
      "‚úÖ Configuration settings exported to: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_agent_config.json\n",
      "‚úÖ Summary CSV exported to: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_summary.csv\n",
      "‚úÖ Quality report exported to: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_report.txt\n",
      "\n",
      "üìÅ All files exported successfully!\n",
      "\n",
      "üìã Export Summary:\n",
      "  Results JSON: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_assessment_results.json\n",
      "  Configuration: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_agent_config.json\n",
      "  Summary CSV: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_summary.csv\n",
      "  Quality Report: /workspace/notebooks/agent_testers/outputs/quality_evals/20250724_193215/quality_report.txt\n",
      "\n",
      "üí° File Usage:\n",
      "  ‚Ä¢ Results JSON: Complete data for further analysis or integration\n",
      "  ‚Ä¢ Configuration: Settings used for quality assessment\n",
      "  ‚Ä¢ Summary CSV: Import into Excel, Google Sheets, or data analysis tools\n",
      "  ‚Ä¢ Quality Report: Human-readable summary for stakeholder review\n",
      "\n",
      "üîÑ Next Steps:\n",
      "  ‚Ä¢ Use results to fine-tune quality thresholds\n",
      "  ‚Ä¢ Analyze patterns to improve chatbot response quality\n",
      "  ‚Ä¢ Compare quality scores across different configurations\n",
      "  ‚Ä¢ Share quality report with stakeholders\n"
     ]
    }
   ],
   "source": [
    "# Export quality assessment results and settings to files with timestamps\n",
    "def export_quality_results():\n",
    "    \"\"\"Export quality assessment results and configuration to JSON files\"\"\"\n",
    "    if not quality_results:\n",
    "        print(\"‚ùå No quality assessment results to export.\")\n",
    "        return\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = Path(f'/workspace/notebooks/agent_testers/outputs/quality_evals/{timestamp}')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Prepare export data\n",
    "    export_data = {\n",
    "        'metadata': {\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'quality_agent_version': quality_agent.agent_config.version if quality_agent and quality_agent.agent_config else '1.0.0',\n",
    "            'total_turns_assessed': len(quality_results),\n",
    "            'average_quality_score': sum(r['quality_score'] for r in quality_results) / len(quality_results),\n",
    "            'assessment_model': quality_agent.llm_provider.model_name if quality_agent and quality_agent.llm_provider else 'unknown',\n",
    "            'quality_thresholds': quality_agent.agent_config.settings.get('quality_thresholds', {}) if quality_agent and quality_agent.agent_config else {}\n",
    "        },\n",
    "        'quality_assessments': quality_results,\n",
    "        'summary_statistics': {\n",
    "            'decision_distribution': dict(pd.DataFrame(quality_results)['quality_decision'].value_counts()),\n",
    "            'score_statistics': {\n",
    "                'mean': sum(r['quality_score'] for r in quality_results) / len(quality_results),\n",
    "                'median': sorted([r['quality_score'] for r in quality_results])[len(quality_results)//2],\n",
    "                'min': min(r['quality_score'] for r in quality_results),\n",
    "                'max': max(r['quality_score'] for r in quality_results)\n",
    "            },\n",
    "            'customer_type_analysis': {},\n",
    "            'complexity_analysis': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add customer type analysis\n",
    "    df_quality = pd.DataFrame(quality_results)\n",
    "    for customer_type in df_quality['customer_type'].unique():\n",
    "        type_data = df_quality[df_quality['customer_type'] == customer_type]\n",
    "        export_data['summary_statistics']['customer_type_analysis'][customer_type] = {\n",
    "            'count': len(type_data),\n",
    "            'average_quality_score': type_data['quality_score'].mean(),\n",
    "            'adjustment_rate': type_data['adjustment_needed'].sum() / len(type_data)\n",
    "        }\n",
    "    \n",
    "    # Add complexity analysis\n",
    "    for complexity in df_quality['complexity'].unique():\n",
    "        complexity_data = df_quality[df_quality['complexity'] == complexity]\n",
    "        export_data['summary_statistics']['complexity_analysis'][complexity] = {\n",
    "            'count': len(complexity_data),\n",
    "            'average_quality_score': complexity_data['quality_score'].mean(),\n",
    "            'escalation_rate': len(complexity_data[complexity_data['quality_decision'] == 'human_intervention']) / len(complexity_data)\n",
    "        }\n",
    "    \n",
    "    # Export main results\n",
    "    results_filename = f'quality_assessment_results.json'\n",
    "    results_path = output_dir / results_filename\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Quality assessment results exported to: {results_path}\")\n",
    "    \n",
    "    # Export configuration used\n",
    "    config_export = {\n",
    "        'export_timestamp': datetime.now().isoformat(),\n",
    "        'agent_config': {},\n",
    "        'prompts_config': {},\n",
    "        'models_config': {}\n",
    "    }\n",
    "    \n",
    "    # Read current config contents\n",
    "    try:\n",
    "        with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "            yaml = YAML()\n",
    "            config_export['agent_config'] = yaml.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "            yaml = YAML()\n",
    "            config_export['prompts_config'] = yaml.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        with open(temp_file_paths['models_config'], 'r') as f:\n",
    "            yaml = YAML()\n",
    "            config_export['models_config'] = yaml.load(f)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    config_filename = f'quality_agent_config.json'\n",
    "    config_path = output_dir / config_filename\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_export, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Configuration settings exported to: {config_path}\")\n",
    "    \n",
    "    # Export summary CSV for easy analysis\n",
    "    summary_df = pd.DataFrame(quality_results)\n",
    "    csv_filename = f'quality_summary.csv'\n",
    "    csv_path = output_dir / csv_filename\n",
    "    \n",
    "    # Select key columns for CSV\n",
    "    csv_columns = ['conversation_id', 'turn_number', 'customer_type', 'complexity',\n",
    "                   'quality_score', 'quality_decision', 'quality_confidence', \n",
    "                   'adjustment_needed', 'customer_query', 'chatbot_response', \n",
    "                   'quality_reasoning']\n",
    "    \n",
    "    summary_csv = summary_df[csv_columns].copy()\n",
    "    summary_csv.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Summary CSV exported to: {csv_path}\")\n",
    "    \n",
    "    # Generate quality report\n",
    "    report_filename = f'quality_report.txt'\n",
    "    report_path = output_dir / report_filename\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"QUALITY ASSESSMENT REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Turns Assessed: {len(quality_results)}\\n\")\n",
    "        f.write(f\"Average Quality Score: {export_data['metadata']['average_quality_score']:.2f}/10.0\\n\")\n",
    "        f.write(f\"Assessment Model: {export_data['metadata']['assessment_model']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"DECISION DISTRIBUTION:\\n\")\n",
    "        for decision, count in export_data['summary_statistics']['decision_distribution'].items():\n",
    "            percentage = count / len(quality_results) * 100\n",
    "            f.write(f\"  {decision.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"CUSTOMER TYPE ANALYSIS:\\n\")\n",
    "        for customer_type, stats in export_data['summary_statistics']['customer_type_analysis'].items():\n",
    "            f.write(f\"  {customer_type.title()}:\\n\")\n",
    "            f.write(f\"    Sample Size: {stats['count']}\\n\")\n",
    "            f.write(f\"    Average Quality: {stats['average_quality_score']:.2f}/10.0\\n\")\n",
    "            f.write(f\"    Adjustment Rate: {stats['adjustment_rate']*100:.1f}%\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"COMPLEXITY ANALYSIS:\\n\")\n",
    "        for complexity, stats in export_data['summary_statistics']['complexity_analysis'].items():\n",
    "            f.write(f\"  {complexity.title()} Complexity:\\n\")\n",
    "            f.write(f\"    Sample Size: {stats['count']}\\n\")\n",
    "            f.write(f\"    Average Quality: {stats['average_quality_score']:.2f}/10.0\\n\")\n",
    "            f.write(f\"    Escalation Rate: {stats['escalation_rate']*100:.1f}%\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Quality report exported to: {report_path}\")\n",
    "    \n",
    "    return {\n",
    "        'results_file': str(results_path),\n",
    "        'config_file': str(config_path),\n",
    "        'csv_file': str(csv_path),\n",
    "        'report_file': str(report_path)\n",
    "    }\n",
    "\n",
    "# Export results if available\n",
    "if quality_results:\n",
    "    print(\"üíæ Exporting Quality Assessment Results and Configuration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    export_files = export_quality_results()\n",
    "    \n",
    "    if export_files:\n",
    "        print(f\"\\nüìÅ All files exported successfully!\")\n",
    "        print(f\"\\nüìã Export Summary:\")\n",
    "        print(f\"  Results JSON: {export_files['results_file']}\")\n",
    "        print(f\"  Configuration: {export_files['config_file']}\")\n",
    "        print(f\"  Summary CSV: {export_files['csv_file']}\")\n",
    "        print(f\"  Quality Report: {export_files['report_file']}\")\n",
    "        \n",
    "        print(f\"\\nüí° File Usage:\")\n",
    "        print(f\"  ‚Ä¢ Results JSON: Complete data for further analysis or integration\")\n",
    "        print(f\"  ‚Ä¢ Configuration: Settings used for quality assessment\")\n",
    "        print(f\"  ‚Ä¢ Summary CSV: Import into Excel, Google Sheets, or data analysis tools\")\n",
    "        print(f\"  ‚Ä¢ Quality Report: Human-readable summary for stakeholder review\")\n",
    "        \n",
    "        print(f\"\\nüîÑ Next Steps:\")\n",
    "        print(f\"  ‚Ä¢ Use results to fine-tune quality thresholds\")\n",
    "        print(f\"  ‚Ä¢ Analyze patterns to improve chatbot response quality\")\n",
    "        print(f\"  ‚Ä¢ Compare quality scores across different configurations\")\n",
    "        print(f\"  ‚Ä¢ Share quality report with stakeholders\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Export failed. Please check for errors above.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No quality assessment results to export. Please run quality assessment in Step 5 first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
