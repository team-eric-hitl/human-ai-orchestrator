{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Export quality assessment results and settings to files with timestamps\ndef export_quality_results():\n    \"\"\"Export quality assessment results and configuration to JSON files\"\"\"\n    if not quality_results:\n        print(\"‚ùå No quality assessment results to export.\")\n        return\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    output_dir = Path('quality_assessment_exports')\n    output_dir.mkdir(exist_ok=True)\n    \n    # Prepare export data\n    export_data = {\n        'metadata': {\n            'export_timestamp': datetime.now().isoformat(),\n            'quality_agent_version': quality_agent.agent_config.agent_info['version'] if quality_agent.agent_config else '1.0.0',\n            'total_turns_assessed': len(quality_results),\n            'average_quality_score': sum(r['quality_score'] for r in quality_results) / len(quality_results),\n            'assessment_model': quality_agent.llm_provider.model_name if quality_agent.llm_provider else 'unknown',\n            'quality_thresholds': quality_agent.agent_config.settings.get('quality_thresholds', {}) if quality_agent.agent_config else {}\n        },\n        'quality_assessments': quality_results,\n        'summary_statistics': {\n            'decision_distribution': dict(pd.DataFrame(quality_results)['quality_decision'].value_counts()),\n            'score_statistics': {\n                'mean': sum(r['quality_score'] for r in quality_results) / len(quality_results),\n                'median': sorted([r['quality_score'] for r in quality_results])[len(quality_results)//2],\n                'min': min(r['quality_score'] for r in quality_results),\n                'max': max(r['quality_score'] for r in quality_results)\n            },\n            'customer_type_analysis': {},\n            'complexity_analysis': {}\n        }\n    }\n    \n    # Add customer type analysis\n    df_quality = pd.DataFrame(quality_results)\n    for customer_type in df_quality['customer_type'].unique():\n        type_data = df_quality[df_quality['customer_type'] == customer_type]\n        export_data['summary_statistics']['customer_type_analysis'][customer_type] = {\n            'count': len(type_data),\n            'average_quality_score': type_data['quality_score'].mean(),\n            'adjustment_rate': type_data['adjustment_needed'].sum() / len(type_data)\n        }\n    \n    # Add complexity analysis\n    for complexity in df_quality['complexity'].unique():\n        complexity_data = df_quality[df_quality['complexity'] == complexity]\n        export_data['summary_statistics']['complexity_analysis'][complexity] = {\n            'count': len(complexity_data),\n            'average_quality_score': complexity_data['quality_score'].mean(),\n            'escalation_rate': len(complexity_data[complexity_data['quality_decision'] == 'human_intervention']) / len(complexity_data)\n        }\n    \n    # Export main results\n    results_filename = f'quality_assessment_results_{timestamp}.json'\n    results_path = output_dir / results_filename\n    \n    with open(results_path, 'w') as f:\n        json.dump(export_data, f, indent=2, default=str)\n    \n    print(f\"‚úÖ Quality assessment results exported to: {results_path}\")\n    \n    # Export configuration used\n    config_export = {\n        'export_timestamp': datetime.now().isoformat(),\n        'agent_config': {},\n        'prompts_config': {},\n        'models_config': {}\n    }\n    \n    # Read current config contents\n    try:\n        with open(temp_file_paths['agent_config'], 'r') as f:\n            yaml = YAML()\n            config_export['agent_config'] = yaml.load(f)\n    except:\n        pass\n    \n    try:\n        with open(temp_file_paths['prompts_config'], 'r') as f:\n            yaml = YAML()\n            config_export['prompts_config'] = yaml.load(f)\n    except:\n        pass\n    \n    try:\n        with open(temp_file_paths['models_config'], 'r') as f:\n            yaml = YAML()\n            config_export['models_config'] = yaml.load(f)\n    except:\n        pass\n    \n    config_filename = f'quality_agent_config_{timestamp}.json'\n    config_path = output_dir / config_filename\n    \n    with open(config_path, 'w') as f:\n        json.dump(config_export, f, indent=2, default=str)\n    \n    print(f\"‚úÖ Configuration settings exported to: {config_path}\")\n    \n    # Export summary CSV for easy analysis\n    summary_df = pd.DataFrame(quality_results)\n    csv_filename = f'quality_summary_{timestamp}.csv'\n    csv_path = output_dir / csv_filename\n    \n    # Select key columns for CSV\n    csv_columns = ['conversation_id', 'turn_number', 'customer_type', 'complexity',\n                   'quality_score', 'quality_decision', 'quality_confidence', \n                   'adjustment_needed', 'customer_query', 'chatbot_response', \n                   'quality_reasoning']\n    \n    summary_csv = summary_df[csv_columns].copy()\n    summary_csv.to_csv(csv_path, index=False)\n    \n    print(f\"‚úÖ Summary CSV exported to: {csv_path}\")\n    \n    # Generate quality report\n    report_filename = f'quality_report_{timestamp}.txt'\n    report_path = output_dir / report_filename\n    \n    with open(report_path, 'w') as f:\n        f.write(\"QUALITY ASSESSMENT REPORT\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Total Turns Assessed: {len(quality_results)}\\n\")\n        f.write(f\"Average Quality Score: {export_data['metadata']['average_quality_score']:.2f}/10.0\\n\")\n        f.write(f\"Assessment Model: {export_data['metadata']['assessment_model']}\\n\\n\")\n        \n        f.write(\"DECISION DISTRIBUTION:\\n\")\n        for decision, count in export_data['summary_statistics']['decision_distribution'].items():\n            percentage = count / len(quality_results) * 100\n            f.write(f\"  {decision.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"CUSTOMER TYPE ANALYSIS:\\n\")\n        for customer_type, stats in export_data['summary_statistics']['customer_type_analysis'].items():\n            f.write(f\"  {customer_type.title()}:\\n\")\n            f.write(f\"    Sample Size: {stats['count']}\\n\")\n            f.write(f\"    Average Quality: {stats['average_quality_score']:.2f}/10.0\\n\")\n            f.write(f\"    Adjustment Rate: {stats['adjustment_rate']*100:.1f}%\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"COMPLEXITY ANALYSIS:\\n\")\n        for complexity, stats in export_data['summary_statistics']['complexity_analysis'].items():\n            f.write(f\"  {complexity.title()} Complexity:\\n\")\n            f.write(f\"    Sample Size: {stats['count']}\\n\")\n            f.write(f\"    Average Quality: {stats['average_quality_score']:.2f}/10.0\\n\")\n            f.write(f\"    Escalation Rate: {stats['escalation_rate']*100:.1f}%\\n\")\n    \n    print(f\"‚úÖ Quality report exported to: {report_path}\")\n    \n    return {\n        'results_file': str(results_path),\n        'config_file': str(config_path),\n        'csv_file': str(csv_path),\n        'report_file': str(report_path)\n    }\n\n# Export results if available\nif quality_results:\n    print(\"üíæ Exporting Quality Assessment Results and Configuration\")\n    print(\"=\" * 60)\n    \n    export_files = export_quality_results()\n    \n    if export_files:\n        print(f\"\\nüìÅ All files exported successfully!\")\n        print(f\"\\nüìã Export Summary:\")\n        print(f\"  Results JSON: {export_files['results_file']}\")\n        print(f\"  Configuration: {export_files['config_file']}\")\n        print(f\"  Summary CSV: {export_files['csv_file']}\")\n        print(f\"  Quality Report: {export_files['report_file']}\")\n        \n        print(f\"\\nüí° File Usage:\")\n        print(f\"  ‚Ä¢ Results JSON: Complete data for further analysis or integration\")\n        print(f\"  ‚Ä¢ Configuration: Settings used for quality assessment\")\n        print(f\"  ‚Ä¢ Summary CSV: Import into Excel, Google Sheets, or data analysis tools\")\n        print(f\"  ‚Ä¢ Quality Report: Human-readable summary for stakeholder review\")\n        \n        print(f\"\\nüîÑ Next Steps:\")\n        print(f\"  ‚Ä¢ Use results to fine-tune quality thresholds\")\n        print(f\"  ‚Ä¢ Analyze patterns to improve chatbot response quality\")\n        print(f\"  ‚Ä¢ Compare quality scores across different configurations\")\n        print(f\"  ‚Ä¢ Share quality report with stakeholders\")\n        \n    else:\n        print(\"‚ùå Export failed. Please check for errors above.\")\n\nelse:\n    print(\"‚ö†Ô∏è No quality assessment results to export. Please run quality assessment in Step 5 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Export Quality Assessment Results\n\nSave the enhanced conversation data with quality metrics to files with timestamps.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detailed analysis of quality assessment results\nif quality_results:\n    print(\"üìä Quality Assessment Analysis Dashboard\")\n    print(\"=\" * 60)\n    \n    # Create DataFrame for analysis\n    df_quality = pd.DataFrame(quality_results)\n    \n    # Overall Quality Statistics\n    print(f\"\\nüéØ OVERALL QUALITY STATISTICS\")\n    print(f\"Total turns assessed: {len(df_quality)}\")\n    print(f\"Mean quality score: {df_quality['quality_score'].mean():.2f}/10.0\")\n    print(f\"Median quality score: {df_quality['quality_score'].median():.2f}/10.0\")\n    print(f\"Standard deviation: {df_quality['quality_score'].std():.2f}\")\n    print(f\"Score range: {df_quality['quality_score'].min():.2f} - {df_quality['quality_score'].max():.2f}\")\n    \n    # Quality Decision Breakdown\n    print(f\"\\nüìã QUALITY DECISION BREAKDOWN\")\n    decision_analysis = df_quality['quality_decision'].value_counts()\n    for decision, count in decision_analysis.items():\n        percentage = count / len(df_quality) * 100\n        print(f\"  {decision.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n    \n    # Quality by Customer Type\n    print(f\"\\nüë• QUALITY BY CUSTOMER TYPE\")\n    customer_type_analysis = df_quality.groupby('customer_type').agg({\n        'quality_score': ['mean', 'std', 'count'],\n        'quality_confidence': 'mean',\n        'adjustment_needed': 'sum'\n    }).round(2)\n    \n    for customer_type in df_quality['customer_type'].unique():\n        type_data = df_quality[df_quality['customer_type'] == customer_type]\n        avg_score = type_data['quality_score'].mean()\n        avg_confidence = type_data['quality_confidence'].mean()\n        adjustment_rate = type_data['adjustment_needed'].sum() / len(type_data) * 100\n        print(f\"  {customer_type.title()}:\")\n        print(f\"    Average quality: {avg_score:.2f}/10.0\")\n        print(f\"    Average confidence: {avg_confidence:.2f}\")\n        print(f\"    Adjustment rate: {adjustment_rate:.1f}%\")\n        print(f\"    Sample size: {len(type_data)}\")\n    \n    # Quality by Complexity\n    print(f\"\\nüî¢ QUALITY BY COMPLEXITY LEVEL\")\n    for complexity in df_quality['complexity'].unique():\n        complexity_data = df_quality[df_quality['complexity'] == complexity]\n        avg_score = complexity_data['quality_score'].mean()\n        avg_confidence = complexity_data['quality_confidence'].mean()\n        escalation_rate = len(complexity_data[complexity_data['quality_decision'] == 'human_intervention']) / len(complexity_data) * 100\n        print(f\"  {complexity.title()} Complexity:\")\n        print(f\"    Average quality: {avg_score:.2f}/10.0\")\n        print(f\"    Average confidence: {avg_confidence:.2f}\")\n        print(f\"    Human escalation rate: {escalation_rate:.1f}%\")\n        print(f\"    Sample size: {len(complexity_data)}\")\n    \n    # Response Improvement Analysis\n    print(f\"\\nüõ†Ô∏è RESPONSE IMPROVEMENT ANALYSIS\")\n    adjustment_needed = df_quality['adjustment_needed'].sum()\n    adjusted_responses = len([r for r in quality_results if r.get('adjusted_response')])\n    print(f\"Turns needing adjustment: {adjustment_needed} ({adjustment_needed/len(df_quality)*100:.1f}%)\")\n    print(f\"Adjusted responses generated: {adjusted_responses}\")\n    \n    # Show quality distribution\n    print(f\"\\nüìà QUALITY SCORE DISTRIBUTION\")\n    quality_ranges = [\n        (9.0, 10.0, \"Excellent\"),\n        (8.0, 8.99, \"Very Good\"),\n        (7.0, 7.99, \"Good\"),\n        (6.0, 6.99, \"Acceptable\"),\n        (5.0, 5.99, \"Needs Improvement\"),\n        (0.0, 4.99, \"Poor\")\n    ]\n    \n    for min_score, max_score, label in quality_ranges:\n        count = len(df_quality[(df_quality['quality_score'] >= min_score) & (df_quality['quality_score'] <= max_score)])\n        percentage = count / len(df_quality) * 100\n        print(f\"  {label} ({min_score}-{max_score}): {count} ({percentage:.1f}%)\")\n    \n    # Confidence vs Score Analysis\n    print(f\"\\nüéØ CONFIDENCE vs QUALITY CORRELATION\")\n    high_conf_high_qual = len(df_quality[(df_quality['quality_confidence'] >= 0.8) & (df_quality['quality_score'] >= 8.0)])\n    low_conf_low_qual = len(df_quality[(df_quality['quality_confidence'] <= 0.5) & (df_quality['quality_score'] <= 5.0)])\n    print(f\"High confidence + High quality: {high_conf_high_qual} ({high_conf_high_qual/len(df_quality)*100:.1f}%)\")\n    print(f\"Low confidence + Low quality: {low_conf_low_qual} ({low_conf_low_qual/len(df_quality)*100:.1f}%)\")\n    \n    # Show detailed examples of each quality decision type\n    print(f\"\\nüìù DETAILED EXAMPLES BY QUALITY DECISION\")\n    \n    # Adequate responses\n    adequate_examples = df_quality[df_quality['quality_decision'] == 'adequate'].head(2)\n    if len(adequate_examples) > 0:\n        print(f\"\\n‚úÖ ADEQUATE RESPONSES:\")\n        for idx, row in adequate_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            print(f\"    Response: {row['chatbot_response'][:100]}...\")\n            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n    \n    # Needs adjustment\n    adjustment_examples = df_quality[df_quality['quality_decision'] == 'needs_adjustment'].head(2)\n    if len(adjustment_examples) > 0:\n        print(f\"\\nüîß NEEDS ADJUSTMENT:\")\n        for idx, row in adjustment_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            print(f\"    Original: {row['chatbot_response'][:100]}...\")\n            if row.get('adjusted_response'):\n                print(f\"    Adjusted: {row['adjusted_response'][:100]}...\")\n            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n    \n    # Human intervention\n    escalation_examples = df_quality[df_quality['quality_decision'] == 'human_intervention'].head(2)\n    if len(escalation_examples) > 0:\n        print(f\"\\nüö® HUMAN INTERVENTION:\")\n        for idx, row in escalation_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['quality_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            print(f\"    Response: {row['chatbot_response'][:100]}...\")\n            print(f\"    Reasoning: {row['quality_reasoning'][:80]}...\")\n    \n    # Show summary table\n    print(f\"\\nüìä QUALITY ASSESSMENT SUMMARY TABLE\")\n    summary_df = df_quality[['conversation_id', 'turn_number', 'customer_type', 'complexity', \n                           'quality_score', 'quality_decision', 'quality_confidence', \n                           'adjustment_needed']].copy()\n    summary_df['quality_score'] = summary_df['quality_score'].round(1)\n    summary_df['quality_confidence'] = summary_df['quality_confidence'].round(2)\n    \n    display(summary_df.head(10))\n    \n    if len(summary_df) > 10:\n        print(f\"... showing first 10 of {len(summary_df)} assessed turns\")\n    \n    print(f\"\\n\" + \"=\"*60)\n    print(f\"‚úÖ Quality analysis complete! Use Step 7 to export detailed results.\")\n\nelse:\n    print(\"‚ö†Ô∏è No quality assessment results to analyze. Please run quality assessment in Step 5 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Analyze Quality Assessment Results\n\nDetailed analysis of quality patterns, scores, and improvement recommendations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run quality assessment on all conversation turns\nquality_results = []\n\ndef create_state_for_turn(turn_data):\n    \"\"\"Create HybridSystemState for a conversation turn\"\"\"\n    return HybridSystemState({\n        \"query_id\": f\"quality_test_{turn_data['conversation_id']}_{turn_data['turn_number']}\",\n        \"user_id\": f\"user_{turn_data['conversation_id']}\",\n        \"session_id\": f\"session_{turn_data['conversation_id']}\",\n        \"timestamp\": datetime.now(),\n        \"query\": turn_data['customer_query'],\n        \"ai_response\": turn_data['chatbot_response'],\n        \"customer_type\": turn_data['customer_type'],\n        \"complexity\": turn_data['complexity'],\n        \"conversation_metadata\": turn_data['conversation_metadata']\n    })\n\ndef assess_conversation_quality(conversation_turns):\n    \"\"\"Run quality assessment on conversation turns\"\"\"\n    if not quality_agent:\n        print(\"‚ùå Quality Agent not initialized. Please run the previous step.\")\n        return []\n    \n    if not conversation_turns:\n        print(\"‚ùå No conversation turns to assess. Please load conversation data first.\")\n        return []\n    \n    print(f\"üîç Running quality assessment on {len(conversation_turns)} conversation turns...\")\n    \n    assessed_turns = []\n    \n    for i, turn in enumerate(conversation_turns):\n        try:\n            # Create state for this turn\n            state = create_state_for_turn(turn)\n            \n            print(f\"\\\\rProcessing turn {i+1}/{len(conversation_turns)} (Conv {turn['conversation_id']}.{turn['turn_number']})...\", end='', flush=True)\n            \n            # Run quality assessment\n            result_state = quality_agent(state)\n            \n            # Extract quality assessment results\n            quality_assessment = result_state.get('quality_assessment', {})\\n            next_action = result_state.get('next_action', 'unknown')\n            \n            # Create enhanced turn data with quality metrics\n            enhanced_turn = {\n                **turn,  # Original turn data\n                'quality_assessment': quality_assessment,\n                'next_action': next_action,\n                'quality_score': quality_assessment.get('overall_score', 0.0),\n                'quality_decision': quality_assessment.get('decision', 'unknown'),\n                'quality_confidence': quality_assessment.get('confidence', 0.0),\n                'quality_reasoning': quality_assessment.get('reasoning', ''),\n                'adjustment_needed': quality_assessment.get('adjustment_needed', False),\n                'adjusted_response': quality_assessment.get('adjusted_response'),\n                'assessment_timestamp': datetime.now().isoformat()\n            }\n            \n            assessed_turns.append(enhanced_turn)\n            \n        except Exception as e:\n            print(f\"\\\\n‚ùå Error assessing turn {turn['conversation_id']}.{turn['turn_number']}: {e}\")\n            # Add turn with error information\n            error_turn = {\n                **turn,\n                'quality_assessment': {'error': str(e)},\n                'quality_score': 0.0,\n                'quality_decision': 'error',\n                'quality_confidence': 0.0,\n                'quality_reasoning': f'Assessment error: {str(e)}',\n                'adjustment_needed': False,\n                'assessment_timestamp': datetime.now().isoformat()\n            }\n            assessed_turns.append(error_turn)\n    \n    print(f\"\\\\n‚úÖ Quality assessment completed for {len(assessed_turns)} turns!\")\n    return assessed_turns\n\n# Run assessment if we have data and agent\nif conversation_data and quality_agent:\n    print(\"üöÄ Starting quality assessment process...\")\n    quality_results = assess_conversation_quality(conversation_data)\n    \n    if quality_results:\n        # Display summary statistics\n        df_results = pd.DataFrame(quality_results)\n        \n        print(f\"\\\\nüìä Quality Assessment Summary:\")\n        print(f\"  Total turns assessed: {len(quality_results)}\")\n        print(f\"  Average quality score: {df_results['quality_score'].mean():.2f}\")\n        print(f\"  Quality score range: {df_results['quality_score'].min():.2f} - {df_results['quality_score'].max():.2f}\")\n        \n        # Decision distribution\n        decision_counts = df_results['quality_decision'].value_counts()\n        print(f\"\\\\nüìã Quality Decisions:\")\n        for decision, count in decision_counts.items():\n            percentage = count / len(quality_results) * 100\n            print(f\"  {decision}: {count} ({percentage:.1f}%)\")\n        \n        # Show sample assessments\n        print(f\"\\\\nüìù Sample Quality Assessments:\")\n        for i, result in enumerate(quality_results[:3]):\n            print(f\"\\\\n  Turn {result['conversation_id']}.{result['turn_number']} [{result['customer_type']}]:\")\n            print(f\"    Quality Score: {result['quality_score']:.2f}/10.0\")\n            print(f\"    Decision: {result['quality_decision']}\")\n            print(f\"    Confidence: {result['quality_confidence']:.2f}\")\n            reasoning = result['quality_reasoning'][:150] + \"...\" if len(result['quality_reasoning']) > 150 else result['quality_reasoning']\n            print(f\"    Reasoning: {reasoning}\")\n            if result['adjustment_needed'] and result.get('adjusted_response'):\n                adjusted = result['adjusted_response'][:100] + \"...\" if len(result['adjusted_response']) > 100 else result['adjusted_response']\n                print(f\"    Adjusted Response: {adjusted}\")\n        \n        if len(quality_results) > 3:\n            print(f\"  ... and {len(quality_results) - 3} more assessments\")\n            \n        print(f\"\\\\n‚úÖ Quality assessment data ready for analysis and export!\")\n    \nelif not conversation_data:\n    print(\"‚ö†Ô∏è No conversation data loaded. Please upload conversation results in Step 3.\")\nelif not quality_agent:\n    print(\"‚ö†Ô∏è Quality Agent not initialized. Please run Step 4 first.\")\nelse:\n    print(\"‚ö†Ô∏è Unable to start quality assessment. Please check previous steps.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Run Quality Assessment\n\nProcess each conversation turn through the Quality Agent to generate quality scores and assessments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize Quality Agent with temporary configuration\ndef initialize_quality_agent():\n    \"\"\"Initialize Quality Agent using temporary configuration files\"\"\"\n    try:\n        # Create ConfigManager pointing to temp config directory\n        config_manager = ConfigManager(config_dir=temp_config_dir.parent, agent_config_dir=temp_config_dir)\n        \n        # Initialize context provider (using in-memory for testing)\n        context_provider = ContextManager(db_path=\":memory:\")\n        \n        # Create Quality Agent\n        quality_agent = QualityAgentNode(\n            config_manager=config_manager,\n            context_provider=context_provider\n        )\n        \n        print(\"‚úÖ Quality Agent initialized successfully!\")\n        print(f\"  Agent config: {quality_agent.agent_config.agent_info['name'] if quality_agent.agent_config else 'Unknown'}\")\n        print(f\"  LLM provider: {quality_agent.llm_provider.model_name if quality_agent.llm_provider else 'None'}\")\n        \n        return quality_agent, context_provider\n        \n    except Exception as e:\n        print(f\"‚ùå Error initializing Quality Agent: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nquality_agent = None\ncontext_provider = None\n\nprint(\"üîß Initializing Quality Agent with configured settings...\")\n\n# Initialize the quality agent\nquality_agent, context_provider = initialize_quality_agent()\n\nif quality_agent:\n    # Display agent configuration\n    if quality_agent.agent_config:\n        settings = quality_agent.agent_config.settings\n        print(f\"\\nüìã Quality Agent Configuration:\")\n        print(f\"  Quality thresholds:\")\n        quality_thresholds = settings.get('quality_thresholds', {})\n        for threshold_name, threshold_value in quality_thresholds.items():\n            print(f\"    {threshold_name}: {threshold_value}\")\n        \n        assessment_settings = settings.get('assessment', {})\n        print(f\"  Assessment settings:\")\n        print(f\"    Use LLM evaluation: {assessment_settings.get('use_llm_evaluation', True)}\")\n        print(f\"    Confidence threshold: {assessment_settings.get('confidence_threshold', 0.7)}\")\n        print(f\"    Context weight: {assessment_settings.get('context_weight', 0.3)}\")\n    \n    print(f\"\\nüöÄ Quality Agent is ready for conversation assessment!\")\nelse:\n    print(f\"\\n‚ùå Quality Agent initialization failed. Please check configuration and try again.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Initialize Quality Agent\n\nCreate the Quality Agent instance with the configured settings.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load conversation results from uploaded file\nconversation_data = []\n\ndef load_conversations_from_file(file_content, filename):\n    \"\"\"Load conversations from uploaded JSON file\"\"\"\n    try:\n        # Handle different content types\n        if isinstance(file_content, memoryview):\n            content_bytes = file_content.tobytes()\n        elif hasattr(file_content, 'decode'):\n            content_bytes = file_content\n        else:\n            content_bytes = str(file_content).encode('utf-8')\n        \n        # Decode to string and parse JSON\n        data = json.loads(content_bytes.decode('utf-8'))\n        \n        # Handle different JSON formats\n        if isinstance(data, dict):\n            # Check for different export formats\n            if 'conversations' in data:\n                conversations = data['conversations']\n            elif 'results' in data:\n                conversations = data['results']\n            else:\n                # Single conversation object\n                conversations = [data]\n        elif isinstance(data, list):\n            conversations = data\n        else:\n            print(f\"‚ùå Unexpected data format: {type(data)}\")\n            return []\n            \n        return conversations\n            \n    except json.JSONDecodeError as e:\n        print(f\"‚ùå JSON parsing error: {e}\")\n        return []\n    except Exception as e:\n        print(f\"‚ùå Error loading file: {e}\")\n        return []\n\ndef extract_conversation_turns(conversations):\n    \"\"\"Extract individual turns from conversations for quality assessment\"\"\"\n    turns = []\n    \n    for conv_idx, conversation in enumerate(conversations):\n        conv_id = conversation.get('id', conv_idx + 1)\n        customer_type = conversation.get('customer_type', 'unknown')\n        complexity = conversation.get('complexity', 'medium')\n        \n        # Get conversation history\n        history = conversation.get('conversation_history', [])\n        \n        # If no conversation_history, try to extract from other fields\n        if not history:\n            # Check for single turn conversation\n            query = conversation.get('query') or conversation.get('customer_query') or conversation.get('original_question')\n            response = conversation.get('ai_response') or conversation.get('chatbot_response') or conversation.get('response')\n            \n            if query and response:\n                history = [{\\n                    'turn_number': 1,\\n                    'customer_query': query,\\n                    'chatbot_response': response\\n                }]\n        \n        # Extract turns from conversation history\n        for turn_idx, turn in enumerate(history):\n            turn_data = {\n                'conversation_id': conv_id,\n                'turn_number': turn.get('turn_number', turn_idx + 1),\n                'customer_type': customer_type,\n                'complexity': complexity,\n                'customer_query': turn.get('customer_query', ''),\n                'chatbot_response': turn.get('chatbot_response', ''),\n                'original_customer_satisfaction': turn.get('customer_satisfaction'),\n                'conversation_metadata': {\n                    'total_turns': len(history),\n                    'final_outcome': conversation.get('final_outcome'),\n                    'original_question': conversation.get('original_question', ''),\n                }\n            }\n            turns.append(turn_data)\n    \n    return turns\n\n# Process uploaded file\nif file_upload.value:\n    uploaded_file = None\n    filename = None\n    file_content = None\n    \n    # Handle different file upload widget formats\n    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n        uploaded_file = file_upload.value[0]\n        filename = uploaded_file['name']\n        file_content = uploaded_file['content']\n    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n        uploaded_file = list(file_upload.value.values())[0]\n        filename = uploaded_file['metadata']['name']\n        file_content = uploaded_file['content']\n    else:\n        print(f\"‚ùå Unable to read uploaded file format\")\n    \n    if uploaded_file and filename and file_content is not None:\n        print(f\"üìÅ Loading conversations from: {filename}\")\n        \n        raw_conversations = load_conversations_from_file(file_content, filename)\n        \n        if raw_conversations:\n            conversation_data = extract_conversation_turns(raw_conversations)\n            print(f\"‚úÖ Loaded {len(conversation_data)} conversation turns from {len(raw_conversations)} conversations\")\n            \n            # Display conversation statistics\n            df_preview = pd.DataFrame(conversation_data)\n            print(f\"\\nüìä Conversation Turn Distribution:\")\n            print(f\"  Total turns: {len(conversation_data)}\")\n            print(f\"  Unique conversations: {df_preview['conversation_id'].nunique()}\")\n            print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n            print(f\"  Complexity levels: {dict(df_preview['complexity'].value_counts())}\")\n            \n            # Show sample turns\n            print(f\"\\nüìã Sample Conversation Turns:\")\n            for i, turn in enumerate(conversation_data[:3]):\n                query_preview = turn['customer_query'][:80] + \"...\" if len(turn['customer_query']) > 80 else turn['customer_query']\n                response_preview = turn['chatbot_response'][:80] + \"...\" if len(turn['chatbot_response']) > 80 else turn['chatbot_response']\n                print(f\"  Turn {turn['conversation_id']}.{turn['turn_number']} [{turn['customer_type']}]:\")\n                print(f\"    Query: {query_preview}\")\n                print(f\"    Response: {response_preview}\")\n            if len(conversation_data) > 3:\n                print(f\"  ... and {len(conversation_data) - 3} more turns\")\n                \n        else:\n            print(\"‚ùå No conversations loaded from file\")\n    else:\n        print(\"‚ùå Error accessing uploaded file\")\nelse:\n    print(\"‚ö†Ô∏è Please upload a JSON file with conversation results from chatbot_tester.ipynb.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# File upload widget for loading conversation results\nfile_upload = widgets.FileUpload(\n    accept='.json',\n    multiple=False,\n    description='Upload conversation results:'\n)\n\n# Instructions for file format\nprint(\"üìù Load Chatbot Conversation Results\")\nprint(\"\\nüí° How to get conversation data:\")\nprint(\"1. Run chatbot_tester.ipynb and export conversation results\")\nprint(\"2. Upload the exported JSON file below\")\nprint(\"3. The file should contain conversation turns with queries and responses\")\nprint(\"\\nüìÑ Expected JSON format:\")\nprint(\"- Array of conversations with conversation_history containing:\")\nprint(\"- Each turn: {'customer_query': '...', 'chatbot_response': '...', 'turn_number': N}\")\nprint(\"- Metadata: customer_type, complexity, conversation details\")\nprint(\"\\nüìÅ Upload your conversation results file:\")\ndisplay(file_upload)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Load Chatbot Conversation Results\n\nLoad conversation results from chatbot_tester.ipynb exports for quality assessment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display and edit configuration files in separate windows\n\ndef load_config_file_contents():\n    \"\"\"Load current config file contents from temp files\"\"\"\n    with open(temp_file_paths['agent_config'], 'r') as f:\n        agent_config_content = f.read()\n    with open(temp_file_paths['prompts_config'], 'r') as f:\n        prompts_config_content = f.read()\n    with open(temp_file_paths['models_config'], 'r') as f:\n        models_config_content = f.read()\n    \n    return agent_config_content, prompts_config_content, models_config_content\n\n# Load current config file contents\nagent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n\nprint(\"‚öôÔ∏è Quality Agent Configuration File Editor\")\nprint(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\nprint(\"Changes are saved to temporary files and will be used in quality assessment.\\n\")\n\n# Create text areas for each config file\nprint(\"üìÑ 1. Quality Agent Configuration (config.yaml)\")\nprint(\"Contains: quality thresholds, assessment settings, escalation rules\")\n\nagent_config_editor = widgets.Textarea(\n    value=agent_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='300px'),\n    style={'description_width': '0px'}\n)\n\ndef save_agent_config(button):\n    \"\"\"Save agent config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        yaml.load(agent_config_editor.value)\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['agent_config'], 'w') as f:\n            f.write(agent_config_editor.value)\n        \n        print(\"‚úÖ Quality Agent config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n\nagent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\nagent_save_btn.on_click(save_agent_config)\n\ndisplay(agent_config_editor)\ndisplay(agent_save_btn)\n\nprint(\"\\nüìÑ 2. Quality Assessment Prompts Configuration (prompts.yaml)\")\nprint(\"Contains: system prompts, quality assessment criteria, adjustment templates\")\n\nprompts_config_editor = widgets.Textarea(\n    value=prompts_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='300px'),\n    style={'description_width': '0px'}\n)\n\ndef save_prompts_config(button):\n    \"\"\"Save prompts config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        yaml.load(prompts_config_editor.value)\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['prompts_config'], 'w') as f:\n            f.write(prompts_config_editor.value)\n        \n        print(\"‚úÖ Prompts config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n\nprompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\nprompts_save_btn.on_click(save_prompts_config)\n\ndisplay(prompts_config_editor)\ndisplay(prompts_save_btn)\n\nprint(\"\\nüìÑ 3. Quality Assessment Models Configuration (models.yaml)\")\nprint(\"Contains: preferred models for quality assessment, response adjustment\")\n\n# Show available model aliases from shared models config\ndef display_available_models():\n    \"\"\"Display available model aliases and their actual models\"\"\"\n    try:\n        shared_models = configs['shared_models']\n        \n        # Extract model aliases and models sections\n        model_aliases = shared_models.get('model_aliases', {})\n        models = shared_models.get('models', {})\n        \n        if not model_aliases:\n            print(\"‚ùå No model aliases found in shared configuration\")\n            return\n            \n        print(\"\\nüîç Available Model Aliases for Quality Assessment:\")\n        print(\"Use these aliases in your models configuration below:\\n\")\n        \n        # Group by provider for better organization\n        providers = {}\n        for alias, actual_model_name in model_aliases.items():\n            # Get model details from models section\n            model_details = models.get(actual_model_name, {})\n            provider = model_details.get('type', 'unknown')\n            description = model_details.get('description', '')\n            \n            if provider not in providers:\n                providers[provider] = []\n            providers[provider].append({\n                'alias': alias,\n                'model_name': actual_model_name,\n                'description': description\n            })\n        \n        # Display by provider with quality assessment recommendations\n        for provider, provider_models in providers.items():\n            print(f\"üì° {provider.upper()} Provider:\")\n            for model in provider_models:\n                desc = f\" - {model['description']}\" if model['description'] else \"\"\n                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n            print()\n        \n        # Show current configuration\n        try:\n            yaml = YAML()\n            yaml.preserve_quotes = True\n            current_models_config = yaml.load(models_config_content)\n            current_preferred = current_models_config.get('primary_model', current_models_config.get('preferred', 'unknown'))\n            \n            # Show model preferences for different tasks\n            model_preferences = current_models_config.get('model_preferences', {})\n        except:\n            current_preferred = 'unknown'\n            model_preferences = {}\n        \n        print(f\"üìã Current Models Configuration:\")\n        print(f\"  Primary model: {current_preferred}\")\n        if model_preferences:\n            print(f\"  Task-specific preferences:\")\n            for task, prefs in model_preferences.items():\n                print(f\"    {task}: {prefs.get('primary', 'unknown')}\")\n        print()\n        \n        print(\"üí° Quality Assessment Model Recommendations:\")\n        print(\"  ‚Ä¢ Lower temperature models (0.1-0.3) for consistent quality scoring\")\n        print(\"  ‚Ä¢ Reasoning models for complex quality analysis\")\n        print(\"  ‚Ä¢ Standard models sufficient for basic quality checks\")\n        print(\"  ‚Ä¢ Budget models acceptable for rule-based assessments\")\n        print()\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading available models: {e}\")\n        print(\"Continuing with models configuration editor...\\n\")\n\n# Display available models before showing the editor\ndisplay_available_models()\n\nmodels_config_editor = widgets.Textarea(\n    value=models_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='200px'),\n    style={'description_width': '0px'}\n)\n\ndef save_models_config(button):\n    \"\"\"Save models config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        parsed_config = yaml.load(models_config_editor.value)\n        \n        # Additional validation for model aliases\n        if isinstance(parsed_config, dict):\n            preferred = parsed_config.get('primary_model') or parsed_config.get('preferred')\n            \n            # Get available aliases\n            model_aliases = configs['shared_models'].get('model_aliases', {})\n            \n            # Check if preferred model exists\n            if preferred and preferred not in model_aliases:\n                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n            \n            # Check model preferences\n            model_preferences = parsed_config.get('model_preferences', {})\n            for task, prefs in model_preferences.items():\n                task_primary = prefs.get('primary')\n                if task_primary and task_primary not in model_aliases:\n                    print(f\"‚ö†Ô∏è Warning: {task} primary model '{task_primary}' not found in available model aliases\")\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['models_config'], 'w') as f:\n            f.write(models_config_editor.value)\n        \n        print(\"‚úÖ Models config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in models config: {e}\")\n\nmodels_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\nmodels_save_btn.on_click(save_models_config)\n\ndisplay(models_config_editor)\ndisplay(models_save_btn)\n\n# Save All button for convenience\ndef save_all_configs(button):\n    \"\"\"Save all config changes at once\"\"\"\n    save_agent_config(None)\n    save_prompts_config(None)\n    save_models_config(None)\n\nprint(\"\\nüíæ Save All Changes\")\nsave_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\nsave_all_btn.on_click(save_all_configs)\ndisplay(save_all_btn)\n\nprint(f\"\\nüíæ Temp config files location:\")\nfor config_type, path in temp_file_paths.items():\n    print(f\"  {config_type}: {path}\")\n\nprint(\"\\nüí° Quality Assessment Tips:\")\nprint(\"  ‚Ä¢ Lower adequate_score = more responses marked as adequate\")\nprint(\"  ‚Ä¢ Higher adjustment_score = more attempts at response improvement\")\nprint(\"  ‚Ä¢ Lower temperature = more consistent quality scoring\")\nprint(\"  ‚Ä¢ Edit quality assessment prompts to customize evaluation criteria\")\nprint(\"  ‚Ä¢ Model preferences allow different models for different quality tasks\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Editable Configuration Settings\n\nEdit these settings to customize how the Quality Agent assesses chatbot responses.\nThese variables map directly to the configuration files and can be exported later.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load configuration from files and create temporary editable copies\nconfig_base_path = Path('/workspace/config')\nagent_config_path = config_base_path / 'agents' / 'quality_agent'\ntemp_config_dir = Path('/tmp/quality_agent_configs')\n\ndef load_and_create_temp_configs():\n    \"\"\"Load all configuration files and create temporary editable copies with comments preserved\"\"\"\n    configs = {}\n    \n    # Create YAML instance for comment preservation\n    yaml = YAML()\n    yaml.preserve_quotes = True\n    yaml.default_flow_style = False\n    \n    # Create temp directory\n    temp_config_dir.mkdir(exist_ok=True)\n    \n    # Load original files for parsing (to access data)\n    with open(agent_config_path / 'config.yaml', 'r') as f:\n        configs['agent'] = yaml.load(f)\n    \n    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n        configs['prompts'] = yaml.load(f)\n    \n    with open(agent_config_path / 'models.yaml', 'r') as f:\n        configs['models'] = yaml.load(f)\n    \n    # Load shared models for reference\n    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n        configs['shared_models'] = yaml.load(f)\n    \n    # Create temp file paths\n    temp_agent_path = temp_config_dir / 'config.yaml'\n    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n    temp_models_path = temp_config_dir / 'models.yaml'\n    \n    # Copy original files to temp directory to preserve comments and formatting\n    import shutil\n    shutil.copy2(agent_config_path / 'config.yaml', temp_agent_path)\n    shutil.copy2(agent_config_path / 'prompts.yaml', temp_prompts_path)  \n    shutil.copy2(agent_config_path / 'models.yaml', temp_models_path)\n    \n    return configs, {\n        'agent_config': temp_agent_path,\n        'prompts_config': temp_prompts_path,\n        'models_config': temp_models_path\n    }\n\n# Load configurations and create temp files\nconfigs, temp_file_paths = load_and_create_temp_configs()\n\nprint(\"üìÅ Configuration files loaded and temporary copies created with comments preserved!\")\nprint(f\"Agent name: {configs['agent']['agent']['name']}\")\nprint(f\"Agent version: {configs['agent']['agent']['version']}\")\n\n# Get preferred model from models config\npreferred_model = \"Unknown\"\nif 'primary_model' in configs['models']:\n    preferred_model = configs['models']['primary_model']\nelif 'preferred' in configs['models']:\n    preferred_model = configs['models']['preferred']\n\nprint(f\"Preferred model: {preferred_model}\")\n\n# Display key quality thresholds\nquality_thresholds = configs['agent']['settings']['quality_thresholds']\nprint(f\"\\nüéØ Quality Assessment Thresholds:\")\nprint(f\"  Adequate score: {quality_thresholds['adequate_score']}\")\nprint(f\"  Adjustment needed: {quality_thresholds['adjustment_score']}\")\n\nprint(f\"\\nüíæ Temporary config files created at:\")\nfor config_type, path in temp_file_paths.items():\n    print(f\"  {config_type}: {path}\")\nprint(f\"\\nüí° These temp files retain original comments and can be edited directly in Step 2.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1: Load Configuration Settings\n\nThe following cell loads the current configuration for the Quality Agent.\nYou can edit these values to customize the agent's quality assessment behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport os\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nimport uuid\n\nimport ipywidgets as widgets\nimport pandas as pd\nfrom ruamel.yaml import YAML\nfrom IPython.display import display, HTML, clear_output\n\n# Set the working directory to the root of the project\nos.chdir('/workspace')\n\n# Add workspace to path for imports\nsys.path.insert(0, '/workspace')\n\n# Import our system components\nfrom src.nodes.quality_agent import QualityAgentNode\nfrom src.core.config import ConfigManager\nfrom src.core.context_manager import ContextManager\nfrom src.interfaces.core.state_schema import HybridSystemState\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"Ready to start testing the Quality Agent.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Quality Agent with chatbot interactions.\n",
    "It reads output from chatbot_tester.ipynb and evaluates the quality of each chatbot response.\n",
    "\n",
    "## Features:\n",
    "- Load and edit Quality Agent configuration settings\n",
    "- Import chatbot conversation results from chatbot_tester.ipynb\n",
    "- Process each conversation turn through the Quality Agent\n",
    "- Generate quality assessments, scores, and improvement suggestions\n",
    "- Analyze quality patterns across different customer types and scenarios\n",
    "- Export enhanced results with quality metrics\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load chatbot conversation results from chatbot_tester exports\n",
    "4. Review conversations before quality assessment\n",
    "5. Run quality analysis and review detailed results\n",
    "\n",
    "## Input Requirements:\n",
    "Load conversation results exported from chatbot_tester.ipynb containing:\n",
    "- Customer queries and chatbot responses\n",
    "- Conversation metadata and turn-by-turn interactions\n",
    "- Customer types, complexity levels, and satisfaction scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}