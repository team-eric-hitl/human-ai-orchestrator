{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustration Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Frustration Agent with chatbot interactions.\n",
    "It reads output from chatbot_tester.ipynb and analyzes customer frustration levels in conversations.\n",
    "\n",
    "## Features:\n",
    "- Load and edit Frustration Agent configuration settings\n",
    "- Import chatbot conversation results from chatbot_tester.ipynb\n",
    "- Process each conversation turn through the Frustration Agent\n",
    "- Generate frustration scores, sentiment analysis, and escalation recommendations\n",
    "- Track frustration patterns and escalation trends across conversations\n",
    "- Analyze employee wellbeing impact and routing recommendations\n",
    "- Export enhanced results with frustration and sentiment metrics\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load chatbot conversation results from chatbot_tester exports\n",
    "4. Review conversations before frustration analysis\n",
    "5. Run frustration detection and review detailed results\n",
    "\n",
    "## Input Requirements:\n",
    "Load conversation results exported from chatbot_tester.ipynb containing:\n",
    "- Customer queries with potential frustration indicators\n",
    "- Conversation metadata and turn-by-turn interactions\n",
    "- Customer types, complexity levels, and interaction history"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport json\nimport os\nimport sys\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport uuid\n\nimport ipywidgets as widgets\nimport pandas as pd\nfrom ruamel.yaml import YAML\nfrom IPython.display import display, HTML, clear_output\n\n# Set the working directory to the root of the project\nos.chdir('/workspace')\n\n# Add workspace to path for imports\nsys.path.insert(0, '/workspace')\n\n# Import our system components\nfrom src.nodes.frustration_agent import FrustrationAgentNode\nfrom src.core.config import ConfigManager\nfrom src.core.context_manager import ContextManager\nfrom src.interfaces.core.state_schema import HybridSystemState\nfrom src.interfaces.core.context import ContextEntry\n\nprint(\"‚úÖ All libraries imported successfully!\")\nprint(\"Ready to start testing the Frustration Agent.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1: Load Configuration Settings\n\nThe following cell loads the current configuration for the Frustration Agent.\nYou can edit these values to customize the agent's frustration detection behavior.",
   "metadata": {},
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "source": "# Load configuration from files and create temporary editable copies\nconfig_base_path = Path('/workspace/config')\nagent_config_path = config_base_path / 'agents' / 'frustration_agent'\ntemp_config_dir = Path('/tmp/frustration_agent_configs')\n\ndef load_and_create_temp_configs():\n    \"\"\"Load all configuration files and create temporary editable copies with comments preserved\"\"\"\n    configs = {}\n    \n    # Create YAML instance for comment preservation\n    yaml = YAML()\n    yaml.preserve_quotes = True\n    yaml.default_flow_style = False\n    \n    # Create temp directory\n    temp_config_dir.mkdir(exist_ok=True)\n    \n    # Load original files for parsing (to access data)\n    with open(agent_config_path / 'config.yaml', 'r') as f:\n        configs['agent'] = yaml.load(f)\n    \n    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n        configs['prompts'] = yaml.load(f)\n    \n    with open(agent_config_path / 'models.yaml', 'r') as f:\n        configs['models'] = yaml.load(f)\n    \n    # Load shared models for reference\n    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n        configs['shared_models'] = yaml.load(f)\n    \n    # Create temp file paths\n    temp_agent_path = temp_config_dir / 'config.yaml'\n    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n    temp_models_path = temp_config_dir / 'models.yaml'\n    \n    # Copy original files to temp directory to preserve comments and formatting\n    import shutil\n    shutil.copy2(agent_config_path / 'config.yaml', temp_agent_path)\n    shutil.copy2(agent_config_path / 'prompts.yaml', temp_prompts_path)  \n    shutil.copy2(agent_config_path / 'models.yaml', temp_models_path)\n    \n    return configs, {\n        'agent_config': temp_agent_path,\n        'prompts_config': temp_prompts_path,\n        'models_config': temp_models_path\n    }\n\n# Load configurations and create temp files\nconfigs, temp_file_paths = load_and_create_temp_configs()\n\nprint(\"üìÅ Configuration files loaded and temporary copies created with comments preserved!\")\nprint(f\"Agent name: {configs['agent']['agent']['name']}\")\nprint(f\"Agent version: {configs['agent']['agent']['version']}\")\n\n# Get preferred model from models config\npreferred_model = \"Unknown\"\nif 'primary_model' in configs['models']:\n    preferred_model = configs['models']['primary_model']\nelif 'preferred' in configs['models']:\n    preferred_model = configs['models']['preferred']\n\nprint(f\"Preferred model: {preferred_model}\")\n\n# Display key frustration thresholds\nfrustration_thresholds = configs['agent']['settings']['frustration_thresholds']\nprint(f\"\\nüò§ Frustration Detection Thresholds:\")\nprint(f\"  Critical: {frustration_thresholds['critical']}\")\nprint(f\"  High: {frustration_thresholds['high']}\")\nprint(f\"  Moderate: {frustration_thresholds['moderate']}\")\n\n# Display intervention threshold\nintervention_threshold = configs['agent']['settings']['intervention_threshold']\nprint(f\"  Intervention trigger: {intervention_threshold}\")\n\nprint(f\"\\nüíæ Temporary config files created at:\")\nfor config_type, path in temp_file_paths.items():\n    print(f\"  {config_type}: {path}\")\nprint(f\"\\nüí° These temp files retain original comments and can be edited directly in Step 2.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Editable Configuration Settings\n\nEdit these settings to customize how the Frustration Agent detects and analyzes customer frustration.\nThese variables map directly to the configuration files and can be exported later.",
   "metadata": {},
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "source": "# Display and edit configuration files in separate windows\n\ndef load_config_file_contents():\n    \"\"\"Load current config file contents from temp files\"\"\"\n    with open(temp_file_paths['agent_config'], 'r') as f:\n        agent_config_content = f.read()\n    with open(temp_file_paths['prompts_config'], 'r') as f:\n        prompts_config_content = f.read()\n    with open(temp_file_paths['models_config'], 'r') as f:\n        models_config_content = f.read()\n    \n    return agent_config_content, prompts_config_content, models_config_content\n\n# Load current config file contents\nagent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n\nprint(\"‚öôÔ∏è Frustration Agent Configuration File Editor\")\nprint(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\nprint(\"Changes are saved to temporary files and will be used in frustration detection.\\n\")\n\n# Create text areas for each config file\nprint(\"üìÑ 1. Frustration Agent Configuration (config.yaml)\")\nprint(\"Contains: frustration thresholds, intervention settings, pattern detection rules\")\n\nagent_config_editor = widgets.Textarea(\n    value=agent_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='350px'),\n    style={'description_width': '0px'}\n)\n\ndef save_agent_config(button):\n    \"\"\"Save agent config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        yaml.load(agent_config_editor.value)\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['agent_config'], 'w') as f:\n            f.write(agent_config_editor.value)\n        \n        print(\"‚úÖ Frustration Agent config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n\nagent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\nagent_save_btn.on_click(save_agent_config)\n\ndisplay(agent_config_editor)\ndisplay(agent_save_btn)\n\nprint(\"\\nüìÑ 2. Frustration Detection Prompts Configuration (prompts.yaml)\")\nprint(\"Contains: system prompts, frustration analysis criteria, pattern detection templates\")\n\nprompts_config_editor = widgets.Textarea(\n    value=prompts_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='300px'),\n    style={'description_width': '0px'}\n)\n\ndef save_prompts_config(button):\n    \"\"\"Save prompts config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        yaml.load(prompts_config_editor.value)\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['prompts_config'], 'w') as f:\n            f.write(prompts_config_editor.value)\n        \n        print(\"‚úÖ Prompts config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n\nprompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\nprompts_save_btn.on_click(save_prompts_config)\n\ndisplay(prompts_config_editor)\ndisplay(prompts_save_btn)\n\nprint(\"\\nüìÑ 3. Frustration Analysis Models Configuration (models.yaml)\")\nprint(\"Contains: preferred models for frustration detection, pattern analysis\")\n\n# Show available model aliases from shared models config\ndef display_available_models():\n    \"\"\"Display available model aliases and their actual models\"\"\"\n    try:\n        shared_models = configs['shared_models']\n        \n        # Extract model aliases and models sections\n        model_aliases = shared_models.get('model_aliases', {})\n        models = shared_models.get('models', {})\n        \n        if not model_aliases:\n            print(\"‚ùå No model aliases found in shared configuration\")\n            return\n            \n        print(\"\\nüîç Available Model Aliases for Frustration Detection:\")\n        print(\"Use these aliases in your models configuration below:\\n\")\n        \n        # Group by provider for better organization\n        providers = {}\n        for alias, actual_model_name in model_aliases.items():\n            # Get model details from models section\n            model_details = models.get(actual_model_name, {})\n            provider = model_details.get('type', 'unknown')\n            description = model_details.get('description', '')\n            \n            if provider not in providers:\n                providers[provider] = []\n            providers[provider].append({\n                'alias': alias,\n                'model_name': actual_model_name,\n                'description': description\n            })\n        \n        # Display by provider with frustration detection recommendations\n        for provider, provider_models in providers.items():\n            print(f\"üì° {provider.upper()} Provider:\")\n            for model in provider_models:\n                desc = f\" - {model['description']}\" if model['description'] else \"\"\n                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n            print()\n        \n        # Show current configuration\n        try:\n            yaml = YAML()\n            yaml.preserve_quotes = True\n            current_models_config = yaml.load(models_config_content)\n            current_preferred = current_models_config.get('primary_model', current_models_config.get('preferred', 'unknown'))\n            \n            # Show model preferences for different tasks\n            model_preferences = current_models_config.get('model_preferences', {})\n        except:\n            current_preferred = 'unknown'\n            model_preferences = {}\n        \n        print(f\"üìã Current Models Configuration:\")\n        print(f\"  Primary model: {current_preferred}\")\n        if model_preferences:\n            print(f\"  Task-specific preferences:\")\n            for task, prefs in model_preferences.items():\n                print(f\"    {task}: {prefs.get('primary', 'unknown')}\")\n        print()\n        \n        print(\"üí° Frustration Detection Model Recommendations:\")\n        print(\"  ‚Ä¢ Very low temperature models (0.1-0.2) for consistent sentiment analysis\")\n        print(\"  ‚Ä¢ Standard models sufficient for basic frustration detection\")\n        print(\"  ‚Ä¢ Budget models acceptable for rule-based pattern matching\")\n        print(\"  ‚Ä¢ Reasoning models beneficial for complex emotional analysis\")\n        print()\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading available models: {e}\")\n        print(\"Continuing with models configuration editor...\\n\")\n\n# Display available models before showing the editor\ndisplay_available_models()\n\nmodels_config_editor = widgets.Textarea(\n    value=models_config_content,\n    description=\"\",\n    layout=widgets.Layout(width='100%', height='200px'),\n    style={'description_width': '0px'}\n)\n\ndef save_models_config(button):\n    \"\"\"Save models config changes with comments preserved\"\"\"\n    try:\n        yaml = YAML()\n        yaml.preserve_quotes = True\n        yaml.default_flow_style = False\n        \n        # Validate YAML syntax\n        parsed_config = yaml.load(models_config_editor.value)\n        \n        # Additional validation for model aliases\n        if isinstance(parsed_config, dict):\n            preferred = parsed_config.get('primary_model') or parsed_config.get('preferred')\n            \n            # Get available aliases\n            model_aliases = configs['shared_models'].get('model_aliases', {})\n            \n            # Check if preferred model exists\n            if preferred and preferred not in model_aliases:\n                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n            \n            # Check model preferences\n            model_preferences = parsed_config.get('model_preferences', {})\n            for task, prefs in model_preferences.items():\n                task_primary = prefs.get('primary')\n                if task_primary and task_primary not in model_aliases:\n                    print(f\"‚ö†Ô∏è Warning: {task} primary model '{task_primary}' not found in available model aliases\")\n        \n        # Save to temp file (preserves comments in the editor content)\n        with open(temp_file_paths['models_config'], 'w') as f:\n            f.write(models_config_editor.value)\n        \n        print(\"‚úÖ Models config saved successfully with comments preserved!\")\n        \n    except Exception as e:\n        print(f\"‚ùå YAML syntax error in models config: {e}\")\n\nmodels_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\nmodels_save_btn.on_click(save_models_config)\n\ndisplay(models_config_editor)\ndisplay(models_save_btn)\n\n# Save All button for convenience\ndef save_all_configs(button):\n    \"\"\"Save all config changes at once\"\"\"\n    save_agent_config(None)\n    save_prompts_config(None)\n    save_models_config(None)\n\nprint(\"\\nüíæ Save All Changes\")\nsave_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\nsave_all_btn.on_click(save_all_configs)\ndisplay(save_all_btn)\n\nprint(f\"\\nüíæ Temp config files location:\")\nfor config_type, path in temp_file_paths.items():\n    print(f\"  {config_type}: {path}\")\n\nprint(\"\\nüí° Frustration Detection Tips:\")\nprint(\"  ‚Ä¢ Lower critical/high thresholds = more sensitive frustration detection\")\nprint(\"  ‚Ä¢ intervention_threshold controls when to escalate to humans\")\nprint(\"  ‚Ä¢ Lower temperature = more consistent frustration scoring\")\nprint(\"  ‚Ä¢ Edit frustration_indicators to customize keyword detection\")\nprint(\"  ‚Ä¢ Employee protection settings help manage agent workload\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-5"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Load Chatbot Conversation Results\n\nLoad conversation results from chatbot_tester.ipynb exports for frustration analysis.",
   "metadata": {},
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "source": "# File upload widget for loading conversation results\nfile_upload = widgets.FileUpload(\n    accept='.json',\n    multiple=False,\n    description='Upload conversation results:'\n)\n\n# Instructions for file format\nprint(\"üìù Load Chatbot Conversation Results for Frustration Analysis\")\nprint(\"\\nüí° How to get conversation data:\")\nprint(\"1. Run chatbot_tester.ipynb and export conversation results\")\nprint(\"2. Upload the exported JSON file below\")\nprint(\"3. The file should contain conversation turns with customer queries\")\nprint(\"\\nüìÑ Expected JSON format:\")\nprint(\"- Array of conversations with conversation_history containing:\")\nprint(\"- Each turn: {'customer_query': '...', 'chatbot_response': '...', 'turn_number': N}\")\nprint(\"- Customer queries will be analyzed for frustration indicators\")\nprint(\"- Conversation patterns will be tracked for escalation trends\")\nprint(\"\\nüìÅ Upload your conversation results file:\")\ndisplay(file_upload)",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "source": "# Load conversation results from uploaded file and prepare context data\nconversation_data = []\n\ndef load_conversations_from_file(file_content, filename):\n    \"\"\"Load conversations from uploaded JSON file with enhanced format detection\"\"\"\n    try:\n        # Handle different content types\n        if isinstance(file_content, memoryview):\n            content_bytes = file_content.tobytes()\n        elif hasattr(file_content, 'decode'):\n            content_bytes = file_content\n        else:\n            content_bytes = str(file_content).encode('utf-8')\n        \n        # Decode to string and parse JSON\n        data = json.loads(content_bytes.decode('utf-8'))\n        \n        # Handle different JSON formats with enhanced detection\n        conversations = []\n        \n        if isinstance(data, dict):\n            # Check for different export formats from chatbot_tester\n            if 'conversation_results' in data:\n                # New chatbot_tester export format\n                conversations = data['conversation_results']\n            elif 'conversations' in data:\n                # Standard conversations format\n                conversations = data['conversations']\n            elif 'results' in data:\n                # Results array format\n                conversations = data['results']\n            elif 'metadata' in data and 'conversations' in data:\n                # Metadata + conversations format\n                conversations = data['conversations']\n            elif any(key in data for key in ['conversation_history', 'customer_type', 'original_question']):\n                # Single conversation object\n                conversations = [data]\n            else:\n                # Try to extract any array-like data\n                array_fields = [v for v in data.values() if isinstance(v, list)]\n                if array_fields:\n                    conversations = array_fields[0]\n                else:\n                    print(f\"‚ö†Ô∏è Unknown dictionary format. Keys found: {list(data.keys())}\")\n                    return []\n        elif isinstance(data, list):\n            # Direct list of conversations\n            conversations = data\n        else:\n            print(f\"‚ùå Unexpected data format: {type(data)}\")\n            return []\n        \n        # Validate that we have actual conversation data\n        if not conversations:\n            print(\"‚ùå No conversations found in the uploaded file\")\n            return []\n        \n        # Validate conversation structure\n        valid_conversations = []\n        for i, conv in enumerate(conversations):\n            if not isinstance(conv, dict):\n                print(f\"‚ö†Ô∏è Skipping invalid conversation {i+1}: not a dictionary\")\n                continue\n            \n            # Check for required fields or conversation history\n            has_history = 'conversation_history' in conv and conv['conversation_history']\n            has_single_turn = any(field in conv for field in ['query', 'customer_query', 'original_question'])\n            \n            if not (has_history or has_single_turn):\n                print(f\"‚ö†Ô∏è Skipping conversation {i+1}: no conversation history or query found\")\n                continue\n                \n            valid_conversations.append(conv)\n        \n        if not valid_conversations:\n            print(\"‚ùå No valid conversations found after validation\")\n            return []\n        \n        print(f\"‚úÖ Successfully loaded {len(valid_conversations)} valid conversations from {len(conversations)} total entries\")\n        return valid_conversations\n            \n    except json.JSONDecodeError as e:\n        print(f\"‚ùå JSON parsing error: {e}\")\n        print(\"Please ensure the file is valid JSON format\")\n        return []\n    except Exception as e:\n        print(f\"‚ùå Error loading file '{filename}': {e}\")\n        print(\"Please check the file format and try again\")\n        return []\n\ndef extract_conversation_turns_with_context(conversations):\n    \"\"\"Extract individual turns from conversations and build context for frustration analysis\"\"\"\n    turns = []\n    \n    for conv_idx, conversation in enumerate(conversations):\n        conv_id = conversation.get('id', conversation.get('conversation_id', conv_idx + 1))\n        customer_type = conversation.get('customer_type', 'unknown')\n        complexity = conversation.get('complexity', 'medium')\n        \n        # Get conversation history with multiple format support\n        history = conversation.get('conversation_history', [])\n        \n        # If no conversation_history, try to extract from other fields\n        if not history:\n            # Check for single turn conversation\n            query = (conversation.get('query') or \n                    conversation.get('customer_query') or \n                    conversation.get('original_question'))\n            response = (conversation.get('ai_response') or \n                       conversation.get('chatbot_response') or \n                       conversation.get('response'))\n            \n            if query and response:\n                history = [{\n                    'turn_number': 1,\n                    'customer_query': query,\n                    'chatbot_response': response\n                }]\n        \n        # Skip conversations with no extractable history\n        if not history:\n            print(f\"‚ö†Ô∏è Skipping conversation {conv_id}: no turns found\")\n            continue\n        \n        # Extract turns from conversation history and build context\n        for turn_idx, turn in enumerate(history):\n            # Skip invalid turns\n            if not isinstance(turn, dict):\n                continue\n                \n            customer_query = turn.get('customer_query', turn.get('query', ''))\n            chatbot_response = turn.get('chatbot_response', turn.get('response', ''))\n            \n            if not customer_query:\n                continue\n            \n            # Create simulated timestamps for frustration pattern analysis\n            base_timestamp = datetime.now() - timedelta(hours=1) + timedelta(minutes=turn_idx * 5)\n            \n            turn_data = {\n                'conversation_id': conv_id,\n                'user_id': f'user_{conv_id}',\n                'session_id': f'session_{conv_id}',\n                'turn_number': turn.get('turn_number', turn_idx + 1),\n                'timestamp': base_timestamp,\n                'customer_type': customer_type,\n                'complexity': complexity,\n                'customer_query': customer_query,\n                'chatbot_response': chatbot_response,\n                'original_customer_satisfaction': turn.get('customer_satisfaction'),\n                'conversation_metadata': {\n                    'total_turns': len(history),\n                    'final_outcome': conversation.get('final_outcome'),\n                    'original_question': conversation.get('original_question', ''),\n                    'turn_index': turn_idx,  # Position in conversation\n                    'is_first_turn': turn_idx == 0,\n                    'is_last_turn': turn_idx == len(history) - 1,\n                },\n                # Previous turns in conversation for pattern analysis\n                'previous_turns': history[:turn_idx] if turn_idx > 0 else []\n            }\n            turns.append(turn_data)\n    \n    return turns\n\ndef simulate_context_history(turn_data, context_provider):\n    \"\"\"Simulate conversation history in context provider for pattern analysis\"\"\"\n    try:\n        # Add previous turns to context for pattern analysis\n        for prev_turn_idx, prev_turn in enumerate(turn_data['previous_turns']):\n            # Create context entries for previous queries\n            prev_timestamp = turn_data['timestamp'] - timedelta(minutes=(len(turn_data['previous_turns']) - prev_turn_idx) * 5)\n            \n            query_entry = ContextEntry(\n                entry_id=f\"{turn_data['conversation_id']}_{prev_turn_idx + 1}_query\",\n                user_id=turn_data['user_id'],\n                session_id=turn_data['session_id'],\n                timestamp=prev_timestamp,\n                entry_type=\"query\",\n                content=prev_turn.get('customer_query', ''),\n                metadata={\n                    'turn_number': prev_turn_idx + 1,\n                    'conversation_id': turn_data['conversation_id']\n                }\n            )\n            context_provider.save_context_entry(query_entry)\n            \n            # Create context entries for previous responses\n            response_entry = ContextEntry(\n                entry_id=f\"{turn_data['conversation_id']}_{prev_turn_idx + 1}_response\",\n                user_id=turn_data['user_id'],\n                session_id=turn_data['session_id'],\n                timestamp=prev_timestamp + timedelta(seconds=30),\n                entry_type=\"response\",\n                content=prev_turn.get('chatbot_response', ''),\n                metadata={\n                    'turn_number': prev_turn_idx + 1,\n                    'conversation_id': turn_data['conversation_id']\n                }\n            )\n            context_provider.save_context_entry(response_entry)\n            \n    except Exception as e:\n        print(f\"Warning: Could not simulate context history: {e}\")\n\n# Process uploaded file with enhanced error handling\nif file_upload.value:\n    uploaded_file = None\n    filename = None\n    file_content = None\n    \n    # Handle different file upload widget formats\n    try:\n        if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n            uploaded_file = file_upload.value[0]\n            filename = uploaded_file['name']\n            file_content = uploaded_file['content']\n        elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n            uploaded_file = list(file_upload.value.values())[0]\n            filename = uploaded_file['metadata']['name']\n            file_content = uploaded_file['content']\n        else:\n            print(f\"‚ùå Unable to read uploaded file format: {type(file_upload.value)}\")\n    except Exception as e:\n        print(f\"‚ùå Error accessing uploaded file: {e}\")\n    \n    if uploaded_file and filename and file_content is not None:\n        print(f\"üìÅ Loading conversations from: {filename}\")\n        \n        raw_conversations = load_conversations_from_file(file_content, filename)\n        \n        if raw_conversations:\n            conversation_data = extract_conversation_turns_with_context(raw_conversations)\n            \n            if conversation_data:\n                print(f\"‚úÖ Loaded {len(conversation_data)} conversation turns from {len(raw_conversations)} conversations\")\n                \n                # Display conversation statistics\n                df_preview = pd.DataFrame(conversation_data)\n                print(f\"\\nüìä Conversation Turn Distribution:\")\n                print(f\"  Total turns: {len(conversation_data)}\")\n                print(f\"  Unique conversations: {df_preview['conversation_id'].nunique()}\")\n                print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n                print(f\"  Complexity levels: {dict(df_preview['complexity'].value_counts())}\")\n                \n                # Analyze frustration indicators in queries\n                print(f\"\\nüîç Preliminary Frustration Indicator Analysis:\")\n                high_frustration_keywords = ['angry', 'frustrated', 'terrible', 'awful', 'ridiculous', 'unacceptable', 'furious', 'livid', 'outraged']\n                moderate_frustration_keywords = ['annoyed', 'disappointed', 'upset', 'confused', 'stuck', 'concerned', 'worried', 'bothered']\n                escalation_keywords = ['manager', 'supervisor', 'complaint', 'cancel', 'refund', 'speak to someone', 'escalate', 'higher up']\n                \n                high_frustration_turns = 0\n                moderate_frustration_turns = 0\n                escalation_turns = 0\n                \n                for turn in conversation_data:\n                    query_lower = turn['customer_query'].lower()\n                    if any(keyword in query_lower for keyword in high_frustration_keywords):\n                        high_frustration_turns += 1\n                    if any(keyword in query_lower for keyword in moderate_frustration_keywords):\n                        moderate_frustration_turns += 1\n                    if any(keyword in query_lower for keyword in escalation_keywords):\n                        escalation_turns += 1\n                \n                print(f\"  Potential high frustration turns: {high_frustration_turns} ({high_frustration_turns/len(conversation_data)*100:.1f}%)\")\n                print(f\"  Potential moderate frustration turns: {moderate_frustration_turns} ({moderate_frustration_turns/len(conversation_data)*100:.1f}%)\")\n                print(f\"  Potential escalation requests: {escalation_turns} ({escalation_turns/len(conversation_data)*100:.1f}%)\")\n                \n                # Show sample turns\n                print(f\"\\nüìã Sample Conversation Turns for Frustration Analysis:\")\n                for i, turn in enumerate(conversation_data[:3]):\n                    query_preview = turn['customer_query'][:80] + \"...\" if len(turn['customer_query']) > 80 else turn['customer_query']\n                    print(f\"  Turn {turn['conversation_id']}.{turn['turn_number']} [{turn['customer_type']}]:\")\n                    print(f\"    Query: {query_preview}\")\n                    print(f\"    Context: {len(turn['previous_turns'])} previous turns in conversation\")\n                if len(conversation_data) > 3:\n                    print(f\"  ... and {len(conversation_data) - 3} more turns\")\n            else:\n                print(\"‚ùå No valid conversation turns extracted from the loaded data\")\n        else:\n            print(\"‚ùå Failed to load conversations from file\")\n    else:\n        print(\"‚ùå Error accessing uploaded file data\")\nelse:\n    print(\"‚ö†Ô∏è Please upload a JSON file with conversation results from chatbot_tester.ipynb.\")\n    print(\"\\nüí° Supported file formats:\")\n    print(\"  ‚Ä¢ chatbot_tester.ipynb export files (conversation_results format)\")\n    print(\"  ‚Ä¢ Standard conversation JSON files\")\n    print(\"  ‚Ä¢ Any JSON file with conversation history or query/response pairs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Initialize Frustration Agent\n\nCreate the Frustration Agent instance with the configured settings.",
   "metadata": {},
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "source": "# Initialize Frustration Agent with temporary configuration\ndef initialize_frustration_agent():\n    \"\"\"Initialize Frustration Agent using temporary configuration files\"\"\"\n    try:\n        # Create a complete temporary config structure that ConfigManager expects\n        temp_base_config_dir = temp_config_dir.parent / 'temp_frustration_config'\n        temp_base_config_dir.mkdir(exist_ok=True)\n        \n        # Create the expected directory structure\n        temp_agents_dir = temp_base_config_dir / 'agents' / 'frustration_agent'\n        temp_shared_dir = temp_base_config_dir / 'shared'\n        temp_agents_dir.mkdir(parents=True, exist_ok=True)\n        temp_shared_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Copy agent-specific configs to the expected location\n        import shutil\n        shutil.copy2(temp_file_paths['agent_config'], temp_agents_dir / 'config.yaml')\n        shutil.copy2(temp_file_paths['prompts_config'], temp_agents_dir / 'prompts.yaml')\n        shutil.copy2(temp_file_paths['models_config'], temp_agents_dir / 'models.yaml')\n        \n        # Copy shared configs that agents might need\n        original_shared_dir = config_base_path / 'shared'\n        for shared_file in ['models.yaml', 'system.yaml', 'providers.yaml']:\n            if (original_shared_dir / shared_file).exists():\n                shutil.copy2(original_shared_dir / shared_file, temp_shared_dir / shared_file)\n        \n        # Copy main config.yaml if needed\n        if (config_base_path / 'config.yaml').exists():\n            shutil.copy2(config_base_path / 'config.yaml', temp_base_config_dir / 'config.yaml')\n        \n        # Create ConfigManager with the complete structure\n        config_manager = ConfigManager(config_dir=str(temp_base_config_dir))\n        \n        # Initialize context provider (using in-memory for testing)\n        context_provider = ContextManager(db_path=\":memory:\")\n        \n        # Create Frustration Agent\n        frustration_agent = FrustrationAgentNode(\n            config_manager=config_manager,\n            context_provider=context_provider\n        )\n        \n        print(\"‚úÖ Frustration Agent initialized successfully!\")\n        print(f\"  Agent config: {frustration_agent.agent_config.agent_info['name'] if frustration_agent.agent_config else 'Unknown'}\")\n        print(f\"  LLM provider: {frustration_agent.llm_provider.model_name if frustration_agent.llm_provider else 'None'}\")\n        \n        return frustration_agent, context_provider\n        \n    except Exception as e:\n        print(f\"‚ùå Error initializing Frustration Agent: {e}\")\n        import traceback\n        print(\"Full traceback:\")\n        traceback.print_exc()\n        return None, None\n\nfrustration_agent = None\ncontext_provider = None\n\nprint(\"üîß Initializing Frustration Agent with configured settings...\")\n\n# Initialize the frustration agent\nfrustration_agent, context_provider = initialize_frustration_agent()\n\nif frustration_agent:\n    # Display agent configuration\n    if frustration_agent.agent_config:\n        settings = frustration_agent.agent_config.settings\n        print(f\"\\nüìã Frustration Agent Configuration:\")\n        print(f\"  Frustration thresholds:\")\n        frustration_thresholds = settings.get('frustration_thresholds', {})\n        for threshold_name, threshold_value in frustration_thresholds.items():\n            print(f\"    {threshold_name}: {threshold_value}\")\n        \n        print(f\"  Intervention threshold: {settings.get('intervention_threshold', 'high')}\")\n        \n        analysis_settings = settings.get('analysis', {})\n        print(f\"  Analysis settings:\")\n        print(f\"    Use LLM analysis: {analysis_settings.get('use_llm_analysis', True)}\")\n        print(f\"    Track frustration progression: {analysis_settings.get('track_frustration_progression', True)}\")\n        print(f\"    Consider interaction history: {analysis_settings.get('consider_interaction_history', True)}\")\n    \n    # Show frustration indicators\n    if hasattr(frustration_agent, 'frustration_indicators'):\n        indicators = frustration_agent.frustration_indicators\n        print(f\"\\nüîç Frustration Detection Indicators:\")\n        print(f\"  High frustration keywords: {len(indicators.get('high_frustration', []))}\")\n        print(f\"  Moderate frustration keywords: {len(indicators.get('moderate_frustration', []))}\")\n        print(f\"  Escalation phrases: {len(indicators.get('escalation_phrases', []))}\")\n        print(f\"  Urgency indicators: {len(indicators.get('urgency_indicators', []))}\")\n    \n    print(f\"\\nüöÄ Frustration Agent is ready for conversation analysis!\")\nelse:\n    print(f\"\\n‚ùå Frustration Agent initialization failed. Please check configuration and try again.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Run Frustration Analysis\n\nProcess each conversation turn through the Frustration Agent to generate frustration scores and intervention recommendations.",
   "metadata": {},
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "source": "# Run frustration analysis on all conversation turns\nfrustration_results = []\n\ndef create_state_for_turn(turn_data):\n    \"\"\"Create HybridSystemState for a conversation turn\"\"\"\n    return HybridSystemState({\n        \"query_id\": f\"frustration_test_{turn_data['conversation_id']}_{turn_data['turn_number']}\",\n        \"user_id\": turn_data['user_id'],\n        \"session_id\": turn_data['session_id'],\n        \"timestamp\": turn_data['timestamp'],\n        \"query\": turn_data['customer_query'],\n        \"customer_type\": turn_data['customer_type'],\n        \"complexity\": turn_data['complexity'],\n        \"conversation_metadata\": turn_data['conversation_metadata']\n    })\n\ndef analyze_conversation_frustration(conversation_turns):\n    \"\"\"Run frustration analysis on conversation turns\"\"\"\n    if not frustration_agent:\n        print(\"‚ùå Frustration Agent not initialized. Please run the previous step.\")\n        return []\n    \n    if not conversation_turns:\n        print(\"‚ùå No conversation turns to analyze. Please load conversation data first.\")\n        return []\n    \n    print(f\"üò§ Running frustration analysis on {len(conversation_turns)} conversation turns...\")\n    \n    analyzed_turns = []\n    \n    for i, turn in enumerate(conversation_turns):\n        try:\n            # Simulate context history for pattern analysis\n            simulate_context_history(turn, context_provider)\n            \n            # Create state for this turn\n            state = create_state_for_turn(turn)\n            \n            print(f\"\\\\rProcessing turn {i+1}/{len(conversation_turns)} (Conv {turn['conversation_id']}.{turn['turn_number']})...\", end='', flush=True)\n            \n            # Run frustration analysis\n            result_state = frustration_agent(state)\n            \n            # Extract frustration analysis results\n            frustration_analysis = result_state.get('frustration_analysis', {})\n            frustration_intervention_needed = result_state.get('frustration_intervention_needed', False)\n            next_action = result_state.get('next_action', 'continue')\n            \n            # Create enhanced turn data with frustration metrics\n            enhanced_turn = {\n                **turn,  # Original turn data\n                'frustration_analysis': frustration_analysis,\n                'frustration_intervention_needed': frustration_intervention_needed,\n                'next_action': next_action,\n                'frustration_score': frustration_analysis.get('overall_score', 0.0),\n                'frustration_level': frustration_analysis.get('overall_level', 'low'),\n                'frustration_confidence': frustration_analysis.get('confidence', 0.0),\n                'contributing_factors': frustration_analysis.get('contributing_factors', []),\n                'current_query_score': frustration_analysis.get('current_analysis', {}).get('current_query_score', 0.0),\n                'pattern_score': frustration_analysis.get('history_analysis', {}).get('pattern_score', 0.0),\n                'escalation_trend': frustration_analysis.get('history_analysis', {}).get('escalation_trend', 'stable'),\n                'analysis_timestamp': datetime.now().isoformat()\n            }\n            \n            analyzed_turns.append(enhanced_turn)\n            \n        except Exception as e:\n            print(f\"\\\\n‚ùå Error analyzing turn {turn['conversation_id']}.{turn['turn_number']}: {e}\")\n            # Add turn with error information\n            error_turn = {\n                **turn,\n                'frustration_analysis': {'error': str(e)},\n                'frustration_score': 0.0,\n                'frustration_level': 'error',\n                'frustration_confidence': 0.0,\n                'contributing_factors': [f'Analysis error: {str(e)}'],\n                'frustration_intervention_needed': False,\n                'analysis_timestamp': datetime.now().isoformat()\n            }\n            analyzed_turns.append(error_turn)\n    \n    print(f\"\\\\n‚úÖ Frustration analysis completed for {len(analyzed_turns)} turns!\")\n    return analyzed_turns\n\n# Run analysis if we have data and agent\nif conversation_data and frustration_agent:\n    print(\"üöÄ Starting frustration analysis process...\")\n    frustration_results = analyze_conversation_frustration(conversation_data)\n    \n    if frustration_results:\n        # Display summary statistics\n        df_results = pd.DataFrame(frustration_results)\n        \n        print(f\"\\\\nüìä Frustration Analysis Summary:\")\n        print(f\"  Total turns analyzed: {len(frustration_results)}\")\n        print(f\"  Average frustration score: {df_results['frustration_score'].mean():.2f}\")\n        print(f\"  Frustration score range: {df_results['frustration_score'].min():.2f} - {df_results['frustration_score'].max():.2f}\")\n        \n        # Frustration level distribution\n        level_counts = df_results['frustration_level'].value_counts()\n        print(f\"\\\\nüò§ Frustration Levels:\")\n        for level, count in level_counts.items():\n            percentage = count / len(frustration_results) * 100\n            print(f\"  {level.title()}: {count} ({percentage:.1f}%)\")\n        \n        # Intervention recommendations\n        intervention_needed = df_results['frustration_intervention_needed'].sum()\n        print(f\"\\\\nüö® Intervention Recommendations:\")\n        print(f\"  Human intervention needed: {intervention_needed} ({intervention_needed/len(frustration_results)*100:.1f}%)\")\n        \n        # Escalation trends\n        escalation_trends = df_results['escalation_trend'].value_counts()\n        print(f\"\\\\nüìà Escalation Trends:\")\n        for trend, count in escalation_trends.items():\n            percentage = count / len(frustration_results) * 100\n            print(f\"  {trend.title()}: {count} ({percentage:.1f}%)\")\n        \n        # Show sample analyses\n        print(f\"\\\\nüìù Sample Frustration Analyses:\")\n        for i, result in enumerate(frustration_results[:3]):\n            print(f\"\\\\n  Turn {result['conversation_id']}.{result['turn_number']} [{result['customer_type']}]:\")\n            print(f\"    Frustration Score: {result['frustration_score']:.2f}/10.0\")\n            print(f\"    Frustration Level: {result['frustration_level']}\")\n            print(f\"    Confidence: {result['frustration_confidence']:.2f}\")\n            print(f\"    Intervention Needed: {result['frustration_intervention_needed']}\")\n            factors = result['contributing_factors'][:3]\n            if factors:\n                print(f\"    Contributing Factors: {', '.join(factors)}\")\n            print(f\"    Query Score: {result['current_query_score']:.2f}, Pattern Score: {result['pattern_score']:.2f}\")\n            print(f\"    Escalation Trend: {result['escalation_trend']}\")\n        \n        if len(frustration_results) > 3:\n            print(f\"  ... and {len(frustration_results) - 3} more analyses\")\n            \n        print(f\"\\\\n‚úÖ Frustration analysis data ready for detailed review and export!\")\n    \nelif not conversation_data:\n    print(\"‚ö†Ô∏è No conversation data loaded. Please upload conversation results in Step 3.\")\nelif not frustration_agent:\n    print(\"‚ö†Ô∏è Frustration Agent not initialized. Please run Step 4 first.\")\nelse:\n    print(\"‚ö†Ô∏è Unable to start frustration analysis. Please check previous steps.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Analyze Frustration Patterns and Results\n\nDetailed analysis of frustration patterns, escalation trends, and employee wellbeing recommendations.",
   "metadata": {},
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "source": "# Detailed analysis of frustration detection results\nif frustration_results:\n    print(\"üò§ Frustration Analysis Dashboard\")\n    print(\"=\" * 60)\n    \n    # Create DataFrame for analysis\n    df_frustration = pd.DataFrame(frustration_results)\n    \n    # Overall Frustration Statistics\n    print(f\"\\nüéØ OVERALL FRUSTRATION STATISTICS\")\n    print(f\"Total turns analyzed: {len(df_frustration)}\")\n    print(f\"Mean frustration score: {df_frustration['frustration_score'].mean():.2f}/10.0\")\n    print(f\"Median frustration score: {df_frustration['frustration_score'].median():.2f}/10.0\")\n    print(f\"Standard deviation: {df_frustration['frustration_score'].std():.2f}\")\n    print(f\"Score range: {df_frustration['frustration_score'].min():.2f} - {df_frustration['frustration_score'].max():.2f}\")\n    \n    # Frustration Level Breakdown\n    print(f\"\\nüò§ FRUSTRATION LEVEL BREAKDOWN\")\n    level_analysis = df_frustration['frustration_level'].value_counts()\n    for level, count in level_analysis.items():\n        percentage = count / len(df_frustration) * 100\n        print(f\"  {level.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n    \n    # Intervention Analysis\n    print(f\"\\nüö® INTERVENTION ANALYSIS\")\n    intervention_needed = df_frustration['frustration_intervention_needed'].sum()\n    intervention_rate = intervention_needed / len(df_frustration) * 100\n    print(f\"  Human intervention recommended: {intervention_needed} ({intervention_rate:.1f}%)\")\n    print(f\"  Customers continuing with AI: {len(df_frustration) - intervention_needed} ({100-intervention_rate:.1f}%)\")\n    \n    # Frustration by Customer Type\n    print(f\"\\nüë• FRUSTRATION BY CUSTOMER TYPE\")\n    for customer_type in df_frustration['customer_type'].unique():\n        type_data = df_frustration[df_frustration['customer_type'] == customer_type]\n        avg_score = type_data['frustration_score'].mean()\n        avg_confidence = type_data['frustration_confidence'].mean()\n        intervention_rate = type_data['frustration_intervention_needed'].sum() / len(type_data) * 100\n        high_frustration_rate = len(type_data[type_data['frustration_level'].isin(['high', 'critical'])]) / len(type_data) * 100\n        print(f\"  {customer_type.title()}:\")\n        print(f\"    Average frustration: {avg_score:.2f}/10.0\")\n        print(f\"    Average confidence: {avg_confidence:.2f}\")\n        print(f\"    Intervention rate: {intervention_rate:.1f}%\")\n        print(f\"    High/Critical frustration rate: {high_frustration_rate:.1f}%\")\n        print(f\"    Sample size: {len(type_data)}\")\n    \n    # Frustration by Complexity\n    print(f\"\\nüî¢ FRUSTRATION BY COMPLEXITY LEVEL\")\n    for complexity in df_frustration['complexity'].unique():\n        complexity_data = df_frustration[df_frustration['complexity'] == complexity]\n        avg_score = complexity_data['frustration_score'].mean()\n        avg_confidence = complexity_data['frustration_confidence'].mean()\n        intervention_rate = complexity_data['frustration_intervention_needed'].sum() / len(complexity_data) * 100\n        print(f\"  {complexity.title()} Complexity:\")\n        print(f\"    Average frustration: {avg_score:.2f}/10.0\")\n        print(f\"    Average confidence: {avg_confidence:.2f}\")\n        print(f\"    Intervention rate: {intervention_rate:.1f}%\")\n        print(f\"    Sample size: {len(complexity_data)}\")\n    \n    # Escalation Trend Analysis\n    print(f\"\\nüìà ESCALATION TREND ANALYSIS\")\n    escalation_analysis = df_frustration['escalation_trend'].value_counts()\n    for trend, count in escalation_analysis.items():\n        percentage = count / len(df_frustration) * 100\n        print(f\"  {trend.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n    \n    # Show frustration distribution with enhanced ranges\n    print(f\"\\nüìä FRUSTRATION SCORE DISTRIBUTION\")\n    frustration_ranges = [\n        (8.0, 10.0, \"Critical\", \"üö®\"),\n        (6.0, 7.99, \"High\", \"üò†\"),\n        (4.0, 5.99, \"Moderate\", \"üò§\"),\n        (2.0, 3.99, \"Low\", \"üòê\"),\n        (0.0, 1.99, \"Minimal\", \"üòä\")\n    ]\n    \n    for min_score, max_score, label, emoji in frustration_ranges:\n        count = len(df_frustration[(df_frustration['frustration_score'] >= min_score) & (df_frustration['frustration_score'] <= max_score)])\n        percentage = count / len(df_frustration) * 100\n        print(f\"  {emoji} {label} ({min_score}-{max_score}): {count} ({percentage:.1f}%)\")\n    \n    # Contributing Factors Analysis\n    print(f\"\\nüîç CONTRIBUTING FACTORS ANALYSIS\")\n    all_factors = []\n    for result in frustration_results:\n        all_factors.extend(result.get('contributing_factors', []))\n    \n    if all_factors:\n        factor_counts = pd.Series(all_factors).value_counts()\n        print(f\"  Top contributing factors:\")\n        for factor, count in factor_counts.head(8).items():\n            percentage = count / len(frustration_results) * 100\n            print(f\"    ‚Ä¢ {factor.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\")\n    else:\n        print(f\"  No contributing factors data available\")\n    \n    # Pattern vs Current Query Analysis\n    print(f\"\\nüîÑ PATTERN vs CURRENT QUERY ANALYSIS\")\n    high_pattern = len(df_frustration[df_frustration['pattern_score'] >= 6.0])\n    high_current = len(df_frustration[df_frustration['current_query_score'] >= 6.0])\n    both_high = len(df_frustration[(df_frustration['pattern_score'] >= 6.0) & (df_frustration['current_query_score'] >= 6.0)])\n    \n    print(f\"  High pattern frustration (‚â•6.0): {high_pattern} ({high_pattern/len(df_frustration)*100:.1f}%)\")\n    print(f\"  High current query frustration (‚â•6.0): {high_current} ({high_current/len(df_frustration)*100:.1f}%)\")\n    print(f\"  Both pattern and current high: {both_high} ({both_high/len(df_frustration)*100:.1f}%)\")\n    \n    # Confidence vs Score Analysis\n    print(f\"\\nüéØ CONFIDENCE vs FRUSTRATION CORRELATION\")\n    high_conf_high_frust = len(df_frustration[(df_frustration['frustration_confidence'] >= 0.8) & (df_frustration['frustration_score'] >= 6.0)])\n    low_conf_low_frust = len(df_frustration[(df_frustration['frustration_confidence'] <= 0.5) & (df_frustration['frustration_score'] <= 3.0)])\n    print(f\"High confidence + High frustration: {high_conf_high_frust} ({high_conf_high_frust/len(df_frustration)*100:.1f}%)\")\n    print(f\"Low confidence + Low frustration: {low_conf_low_frust} ({low_conf_low_frust/len(df_frustration)*100:.1f}%)\")\n    \n    # Show detailed examples by frustration level\n    print(f\"\\nüìù DETAILED EXAMPLES BY FRUSTRATION LEVEL\")\n    \n    # Critical frustration examples\n    critical_examples = df_frustration[df_frustration['frustration_level'] == 'critical'].head(2)\n    if len(critical_examples) > 0:\n        print(f\"\\nüö® CRITICAL FRUSTRATION:\")\n        for idx, row in critical_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['frustration_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            if row['contributing_factors']:\n                factors_str = ', '.join(row['contributing_factors'][:3])\n                print(f\"    Factors: {factors_str}\")\n            print(f\"    Trend: {row['escalation_trend']}, Intervention: {row['frustration_intervention_needed']}\")\n            print(f\"    Confidence: {row['frustration_confidence']:.2f}\")\n    \n    # High frustration examples\n    high_examples = df_frustration[df_frustration['frustration_level'] == 'high'].head(2)\n    if len(high_examples) > 0:\n        print(f\"\\nüò† HIGH FRUSTRATION:\")\n        for idx, row in high_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['frustration_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            if row['contributing_factors']:\n                factors_str = ', '.join(row['contributing_factors'][:3])\n                print(f\"    Factors: {factors_str}\")\n            print(f\"    Trend: {row['escalation_trend']}, Intervention: {row['frustration_intervention_needed']}\")\n            print(f\"    Confidence: {row['frustration_confidence']:.2f}\")\n    \n    # Moderate frustration examples\n    moderate_examples = df_frustration[df_frustration['frustration_level'] == 'moderate'].head(2)\n    if len(moderate_examples) > 0:\n        print(f\"\\nüò§ MODERATE FRUSTRATION:\")\n        for idx, row in moderate_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['frustration_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            if row['contributing_factors']:\n                factors_str = ', '.join(row['contributing_factors'][:3])\n                print(f\"    Factors: {factors_str}\")\n            print(f\"    Trend: {row['escalation_trend']}, Intervention: {row['frustration_intervention_needed']}\")\n            print(f\"    Confidence: {row['frustration_confidence']:.2f}\")\n    \n    # Low frustration examples for comparison\n    low_examples = df_frustration[df_frustration['frustration_level'].isin(['low', 'minimal'])].head(2)\n    if len(low_examples) > 0:\n        print(f\"\\nüòä LOW/MINIMAL FRUSTRATION:\")\n        for idx, row in low_examples.iterrows():\n            print(f\"  Example {row['conversation_id']}.{row['turn_number']} (Score: {row['frustration_score']:.1f}):\")\n            print(f\"    Query: {row['customer_query'][:100]}...\")\n            print(f\"    Level: {row['frustration_level']}, Confidence: {row['frustration_confidence']:.2f}\")\n    \n    # Employee Wellbeing Impact Analysis\n    print(f\"\\nüë• EMPLOYEE WELLBEING IMPACT ANALYSIS\")\n    critical_high_count = len(df_frustration[df_frustration['frustration_level'].isin(['critical', 'high'])])\n    escalating_count = len(df_frustration[df_frustration['escalation_trend'] == 'escalating'])\n    \n    print(f\"  High-stress customer interactions: {critical_high_count} ({critical_high_count/len(df_frustration)*100:.1f}%)\")\n    print(f\"  Escalating frustration patterns: {escalating_count} ({escalating_count/len(df_frustration)*100:.1f}%)\")\n    print(f\"  Recommended for experienced agents: {intervention_needed}\")\n    \n    if critical_high_count > 0:\n        print(f\"\\nüí° Employee Protection Recommendations:\")\n        print(f\"  ‚Ä¢ Rotate high-frustration customers among multiple agents\")\n        print(f\"  ‚Ä¢ Provide emotional support for agents handling {critical_high_count} difficult cases\")\n        print(f\"  ‚Ä¢ Consider priority routing for {escalating_count} escalating situations\")\n        print(f\"  ‚Ä¢ Monitor agent stress levels during peak frustration periods\")\n        print(f\"  ‚Ä¢ Implement cooldown periods after handling critical frustration cases\")\n        print(f\"  ‚Ä¢ Provide specialized training for {intervention_needed} intervention-required cases\")\n    \n    # Frustration Progression Analysis\n    print(f\"\\nüìä FRUSTRATION PROGRESSION ANALYSIS\")\n    multi_turn_convs = df_frustration[df_frustration.groupby('conversation_id')['conversation_id'].transform('count') > 1]\n    if len(multi_turn_convs) > 0:\n        print(f\"  Multi-turn conversations: {multi_turn_convs['conversation_id'].nunique()}\")\n        \n        # Analyze progression patterns\n        escalating_convs = multi_turn_convs[multi_turn_convs['escalation_trend'] == 'escalating']['conversation_id'].nunique()\n        stable_convs = multi_turn_convs[multi_turn_convs['escalation_trend'] == 'stable']['conversation_id'].nunique()\n        improving_convs = multi_turn_convs[multi_turn_convs['escalation_trend'] == 'improving']['conversation_id'].nunique()\n        \n        total_multi = multi_turn_convs['conversation_id'].nunique()\n        print(f\"  Escalating patterns: {escalating_convs} ({escalating_convs/total_multi*100:.1f}%)\")\n        print(f\"  Stable patterns: {stable_convs} ({stable_convs/total_multi*100:.1f}%)\")\n        print(f\"  Improving patterns: {improving_convs} ({improving_convs/total_multi*100:.1f}%)\")\n    else:\n        print(f\"  All conversations are single-turn\")\n    \n    # Show summary table with enhanced columns\n    print(f\"\\nüìä FRUSTRATION ANALYSIS SUMMARY TABLE\")\n    summary_df = df_frustration[['conversation_id', 'turn_number', 'customer_type', 'complexity', \n                               'frustration_score', 'frustration_level', 'frustration_confidence', \n                               'frustration_intervention_needed', 'escalation_trend', \n                               'current_query_score', 'pattern_score']].copy()\n    summary_df['frustration_score'] = summary_df['frustration_score'].round(1)\n    summary_df['frustration_confidence'] = summary_df['frustration_confidence'].round(2)\n    summary_df['current_query_score'] = summary_df['current_query_score'].round(1)\n    summary_df['pattern_score'] = summary_df['pattern_score'].round(1)\n    \n    display(summary_df.head(10))\n    \n    if len(summary_df) > 10:\n        print(f\"... showing first 10 of {len(summary_df)} analyzed turns\")\n    \n    # Advanced Analytics Summary\n    print(f\"\\nüìà ADVANCED ANALYTICS SUMMARY\")\n    avg_conf = df_frustration['frustration_confidence'].mean()\n    high_confidence_rate = len(df_frustration[df_frustration['frustration_confidence'] >= 0.8]) / len(df_frustration) * 100\n    consistent_analysis = len(df_frustration[abs(df_frustration['current_query_score'] - df_frustration['pattern_score']) <= 2.0]) / len(df_frustration) * 100\n    \n    print(f\"  Average confidence: {avg_conf:.2f}\")\n    print(f\"  High confidence detections (‚â•0.8): {high_confidence_rate:.1f}%\")\n    print(f\"  Consistent pattern-query analysis (‚â§2.0 difference): {consistent_analysis:.1f}%\")\n    \n    print(f\"\\n\" + \"=\"*60)\n    print(f\"‚úÖ Comprehensive frustration analysis complete! Use Step 7 to export detailed results.\")\n\nelse:\n    print(\"‚ö†Ô∏è No frustration analysis results to review. Please run frustration analysis in Step 5 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Export Frustration Analysis Results\n\nSave the enhanced conversation data with frustration metrics and employee wellbeing recommendations to files with timestamps.",
   "metadata": {},
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "source": "# Export frustration analysis results and settings to files with timestamps\ndef export_frustration_results():\n    \"\"\"Export frustration analysis results and configuration to JSON files\"\"\"\n    if not frustration_results:\n        print(\"‚ùå No frustration analysis results to export.\")\n        return\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    output_dir = Path('frustration_analysis_exports')\n    output_dir.mkdir(exist_ok=True)\n    \n    # Prepare export data\n    export_data = {\n        'metadata': {\n            'export_timestamp': datetime.now().isoformat(),\n            'frustration_agent_version': frustration_agent.agent_config.agent_info['version'] if frustration_agent.agent_config else '1.0.0',\n            'total_turns_analyzed': len(frustration_results),\n            'average_frustration_score': sum(r['frustration_score'] for r in frustration_results) / len(frustration_results),\n            'analysis_model': frustration_agent.llm_provider.model_name if frustration_agent.llm_provider else 'unknown',\n            'frustration_thresholds': frustration_agent.agent_config.settings.get('frustration_thresholds', {}) if frustration_agent.agent_config else {},\n            'intervention_threshold': frustration_agent.agent_config.settings.get('intervention_threshold', 'high') if frustration_agent.agent_config else 'high'\n        },\n        'frustration_analyses': frustration_results,\n        'summary_statistics': {\n            'level_distribution': dict(pd.DataFrame(frustration_results)['frustration_level'].value_counts()),\n            'escalation_trends': dict(pd.DataFrame(frustration_results)['escalation_trend'].value_counts()),\n            'intervention_statistics': {\n                'total_interventions_recommended': sum(1 for r in frustration_results if r['frustration_intervention_needed']),\n                'intervention_rate': sum(1 for r in frustration_results if r['frustration_intervention_needed']) / len(frustration_results),\n            },\n            'score_statistics': {\n                'mean': sum(r['frustration_score'] for r in frustration_results) / len(frustration_results),\n                'median': sorted([r['frustration_score'] for r in frustration_results])[len(frustration_results)//2],\n                'min': min(r['frustration_score'] for r in frustration_results),\n                'max': max(r['frustration_score'] for r in frustration_results)\n            },\n            'customer_type_analysis': {},\n            'complexity_analysis': {},\n            'employee_wellbeing_impact': {}\n        }\n    }\n    \n    # Add customer type analysis\n    df_frustration = pd.DataFrame(frustration_results)\n    for customer_type in df_frustration['customer_type'].unique():\n        type_data = df_frustration[df_frustration['customer_type'] == customer_type]\n        export_data['summary_statistics']['customer_type_analysis'][customer_type] = {\n            'count': len(type_data),\n            'average_frustration_score': type_data['frustration_score'].mean(),\n            'intervention_rate': type_data['frustration_intervention_needed'].sum() / len(type_data),\n            'high_frustration_rate': len(type_data[type_data['frustration_level'].isin(['high', 'critical'])]) / len(type_data)\n        }\n    \n    # Add complexity analysis\n    for complexity in df_frustration['complexity'].unique():\n        complexity_data = df_frustration[df_frustration['complexity'] == complexity]\n        export_data['summary_statistics']['complexity_analysis'][complexity] = {\n            'count': len(complexity_data),\n            'average_frustration_score': complexity_data['frustration_score'].mean(),\n            'intervention_rate': complexity_data['frustration_intervention_needed'].sum() / len(complexity_data)\n        }\n    \n    # Add employee wellbeing analysis\n    critical_high_count = len(df_frustration[df_frustration['frustration_level'].isin(['critical', 'high'])])\n    escalating_count = len(df_frustration[df_frustration['escalation_trend'] == 'escalating'])\n    \n    export_data['summary_statistics']['employee_wellbeing_impact'] = {\n        'high_stress_interactions': critical_high_count,\n        'high_stress_rate': critical_high_count / len(df_frustration),\n        'escalating_patterns': escalating_count,\n        'escalating_rate': escalating_count / len(df_frustration),\n        'recommendations': [\n            'Rotate high-frustration customers among multiple agents',\n            'Provide emotional support for agents handling difficult cases',\n            'Monitor agent stress levels during peak frustration periods',\n            'Consider priority routing for escalating situations'\n        ]\n    }\n    \n    # Export main results\n    results_filename = f'frustration_analysis_results_{timestamp}.json'\n    results_path = output_dir / results_filename\n    \n    with open(results_path, 'w') as f:\n        json.dump(export_data, f, indent=2, default=str)\n    \n    print(f\"‚úÖ Frustration analysis results exported to: {results_path}\")\n    \n    # Export configuration used\n    config_export = {\n        'export_timestamp': datetime.now().isoformat(),\n        'agent_config': {},\n        'prompts_config': {},\n        'models_config': {}\n    }\n    \n    # Read current config contents\n    try:\n        with open(temp_file_paths['agent_config'], 'r') as f:\n            yaml = YAML()\n            config_export['agent_config'] = yaml.load(f)\n    except:\n        pass\n    \n    try:\n        with open(temp_file_paths['prompts_config'], 'r') as f:\n            yaml = YAML()\n            config_export['prompts_config'] = yaml.load(f)\n    except:\n        pass\n    \n    try:\n        with open(temp_file_paths['models_config'], 'r') as f:\n            yaml = YAML()\n            config_export['models_config'] = yaml.load(f)\n    except:\n        pass\n    \n    config_filename = f'frustration_agent_config_{timestamp}.json'\n    config_path = output_dir / config_filename\n    \n    with open(config_path, 'w') as f:\n        json.dump(config_export, f, indent=2, default=str)\n    \n    print(f\"‚úÖ Configuration settings exported to: {config_path}\")\n    \n    # Export summary CSV for easy analysis\n    summary_df = pd.DataFrame(frustration_results)\n    csv_filename = f'frustration_summary_{timestamp}.csv'\n    csv_path = output_dir / csv_filename\n    \n    # Select key columns for CSV\n    csv_columns = ['conversation_id', 'turn_number', 'customer_type', 'complexity',\n                   'frustration_score', 'frustration_level', 'frustration_confidence', \n                   'frustration_intervention_needed', 'escalation_trend', 'contributing_factors',\n                   'current_query_score', 'pattern_score', 'customer_query']\n    \n    summary_csv = summary_df[csv_columns].copy()\n    # Convert list columns to strings for CSV\n    summary_csv['contributing_factors'] = summary_csv['contributing_factors'].apply(lambda x: '; '.join(x) if x else '')\n    summary_csv.to_csv(csv_path, index=False)\n    \n    print(f\"‚úÖ Summary CSV exported to: {csv_path}\")\n    \n    # Generate frustration analysis report\n    report_filename = f'frustration_report_{timestamp}.txt'\n    report_path = output_dir / report_filename\n    \n    with open(report_path, 'w') as f:\n        f.write(\"FRUSTRATION ANALYSIS REPORT\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        f.write(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        f.write(f\"Total Turns Analyzed: {len(frustration_results)}\\n\")\n        f.write(f\"Average Frustration Score: {export_data['metadata']['average_frustration_score']:.2f}/10.0\\n\")\n        f.write(f\"Analysis Model: {export_data['metadata']['analysis_model']}\\n\")\n        f.write(f\"Intervention Threshold: {export_data['metadata']['intervention_threshold']}\\n\\n\")\n        \n        f.write(\"FRUSTRATION LEVEL DISTRIBUTION:\\n\")\n        for level, count in export_data['summary_statistics']['level_distribution'].items():\n            percentage = count / len(frustration_results) * 100\n            f.write(f\"  {level.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"INTERVENTION ANALYSIS:\\n\")\n        intervention_stats = export_data['summary_statistics']['intervention_statistics']\n        f.write(f\"  Interventions Recommended: {intervention_stats['total_interventions_recommended']}\\n\")\n        f.write(f\"  Intervention Rate: {intervention_stats['intervention_rate']*100:.1f}%\\n\\n\")\n        \n        f.write(\"ESCALATION TRENDS:\\n\")\n        for trend, count in export_data['summary_statistics']['escalation_trends'].items():\n            percentage = count / len(frustration_results) * 100\n            f.write(f\"  {trend.replace('_', ' ').title()}: {count} ({percentage:.1f}%)\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"CUSTOMER TYPE ANALYSIS:\\n\")\n        for customer_type, stats in export_data['summary_statistics']['customer_type_analysis'].items():\n            f.write(f\"  {customer_type.title()}:\\n\")\n            f.write(f\"    Sample Size: {stats['count']}\\n\")\n            f.write(f\"    Average Frustration: {stats['average_frustration_score']:.2f}/10.0\\n\")\n            f.write(f\"    Intervention Rate: {stats['intervention_rate']*100:.1f}%\\n\")\n            f.write(f\"    High Frustration Rate: {stats['high_frustration_rate']*100:.1f}%\\n\")\n        f.write(\"\\n\")\n        \n        f.write(\"EMPLOYEE WELLBEING IMPACT:\\n\")\n        wellbeing = export_data['summary_statistics']['employee_wellbeing_impact']\n        f.write(f\"  High-Stress Interactions: {wellbeing['high_stress_interactions']} ({wellbeing['high_stress_rate']*100:.1f}%)\\n\")\n        f.write(f\"  Escalating Patterns: {wellbeing['escalating_patterns']} ({wellbeing['escalating_rate']*100:.1f}%)\\n\")\n        f.write(\"\\n  Recommendations:\\n\")\n        for rec in wellbeing['recommendations']:\n            f.write(f\"    ‚Ä¢ {rec}\\n\")\n    \n    print(f\"‚úÖ Frustration analysis report exported to: {report_path}\")\n    \n    return {\n        'results_file': str(results_path),\n        'config_file': str(config_path),\n        'csv_file': str(csv_path),\n        'report_file': str(report_path)\n    }\n\n# Export results if available\nif frustration_results:\n    print(\"üíæ Exporting Frustration Analysis Results and Configuration\")\n    print(\"=\" * 60)\n    \n    export_files = export_frustration_results()\n    \n    if export_files:\n        print(f\"\\nüìÅ All files exported successfully!\")\n        print(f\"\\nüìã Export Summary:\")\n        print(f\"  Results JSON: {export_files['results_file']}\")\n        print(f\"  Configuration: {export_files['config_file']}\")\n        print(f\"  Summary CSV: {export_files['csv_file']}\")\n        print(f\"  Frustration Report: {export_files['report_file']}\")\n        \n        print(f\"\\nüí° File Usage:\")\n        print(f\"  ‚Ä¢ Results JSON: Complete data for further analysis or integration\")\n        print(f\"  ‚Ä¢ Configuration: Settings used for frustration detection\")\n        print(f\"  ‚Ä¢ Summary CSV: Import into Excel, Google Sheets, or data analysis tools\")\n        print(f\"  ‚Ä¢ Frustration Report: Human-readable summary for management review\")\n        \n        print(f\"\\nüîÑ Next Steps:\")\n        print(f\"  ‚Ä¢ Use results to fine-tune frustration detection thresholds\")\n        print(f\"  ‚Ä¢ Analyze patterns to improve customer experience\")\n        print(f\"  ‚Ä¢ Implement employee wellbeing protection measures\")\n        print(f\"  ‚Ä¢ Compare frustration trends across different configurations\")\n        print(f\"  ‚Ä¢ Share analysis with customer service management\")\n        \n    else:\n        print(\"‚ùå Export failed. Please check for errors above.\")\n\nelse:\n    print(\"‚ö†Ô∏è No frustration analysis results to export. Please run frustration analysis in Step 5 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "cell-16"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}