{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Live Chat Testing Notebook\n",
    "\n",
    "This notebook provides a live chat interface for real-time testing of the Human-in-the-Loop AI system.\n",
    "A human user can chat with the chatbot and see real-time quality and frustration assessments.\n",
    "\n",
    "## Features:\n",
    "- Interactive chat window for real human-chatbot conversations\n",
    "- Real-time quality scoring after each chatbot response\n",
    "- Real-time frustration detection after each user message\n",
    "- Live display of agent assessments and scores\n",
    "- Export complete conversation with all scoring data\n",
    "- Perfect for demos, validation, and real-world testing\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Configure agents as needed\n",
    "3. Use the chat interface to have a real conversation\n",
    "4. Watch scores update in real-time below each interaction\n",
    "5. Export results when finished\n",
    "\n",
    "## Use Cases:\n",
    "- **Stakeholder Demos**: Show live AI quality and frustration detection\n",
    "- **System Validation**: Test with real human interactions vs simulated\n",
    "- **Configuration Tuning**: Immediately see impact of settings changes\n",
    "- **Training Data**: Generate high-quality human-validated conversation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import asyncio\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.chatbot_agent import ChatbotAgentNode\n",
    "from src.nodes.quality_agent import QualityAgentNode\n",
    "from src.nodes.frustration_agent import FrustrationAgentNode\n",
    "from src.core.config.agent_config_manager import AgentConfigManager\n",
    "from src.core.context_manager import ContextManager\n",
    "from src.integrations.llm_providers import LLMProviderFactory\n",
    "from src.interfaces.core.state_schema import HybridSystemState\n",
    "from src.interfaces.core.context import ContextEntry\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start live chat testing with real-time agent scoring.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Step 1: Initialize AI Agents\n",
    "\n",
    "Load and initialize the three AI agents for live testing:\n",
    "- **Chatbot Agent**: Generates responses to user queries\n",
    "- **Quality Agent**: Scores chatbot response quality in real-time\n",
    "- **Frustration Agent**: Detects user frustration levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all three agents\n",
    "chatbot_agent = None\n",
    "quality_agent = None\n",
    "frustration_agent = None\n",
    "context_provider = None\n",
    "\n",
    "def initialize_agents():\n",
    "    \"\"\"Initialize all agents for live chat testing\"\"\"\n",
    "    global chatbot_agent, quality_agent, frustration_agent, context_provider\n",
    "    \n",
    "    try:\n",
    "        print(\"üîß Initializing agents for live chat testing...\")\n",
    "        \n",
    "        # Initialize shared context provider (in-memory for demo)\n",
    "        context_provider = ContextManager(db_path=\":memory:\")\n",
    "        print(\"‚úÖ Context provider initialized\")\n",
    "        \n",
    "        # Initialize Chatbot Agent\n",
    "        config_manager = AgentConfigManager('/workspace/config')\n",
    "        chatbot_agent = ChatbotAgentNode(\n",
    "            config_manager=config_manager,\n",
    "            context_provider=context_provider\n",
    "        )\n",
    "        print(f\"‚úÖ Chatbot Agent initialized: {chatbot_agent.llm_provider.model_name if chatbot_agent.llm_provider else 'Unknown model'}\")\n",
    "        \n",
    "        # Initialize Quality Agent\n",
    "        quality_config_manager = AgentConfigManager(\n",
    "            config_dir='/workspace/config',\n",
    "            agent_name='quality_agent'\n",
    "        )\n",
    "        quality_agent = QualityAgentNode(\n",
    "            config_manager=quality_config_manager,\n",
    "            context_provider=context_provider\n",
    "        )\n",
    "        print(f\"‚úÖ Quality Agent initialized: {quality_agent.llm_provider.model_name if quality_agent.llm_provider else 'Unknown model'}\")\n",
    "        \n",
    "        # Initialize Frustration Agent\n",
    "        frustration_config_manager = AgentConfigManager(\n",
    "            config_dir='/workspace/config',\n",
    "            agent_name='frustration_agent'\n",
    "        )\n",
    "        frustration_agent = FrustrationAgentNode(\n",
    "            config_manager=frustration_config_manager,\n",
    "            context_provider=context_provider\n",
    "        )\n",
    "        print(f\"‚úÖ Frustration Agent initialized: {frustration_agent.llm_provider.model_name if frustration_agent.llm_provider else 'Unknown model'}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing agents: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Initialize agents\n",
    "if initialize_agents():\n",
    "    print(\"\\nüöÄ All agents ready for live chat testing!\")\n",
    "    print(\"\\nüìã Agent Configuration Summary:\")\n",
    "    if chatbot_agent and chatbot_agent.agent_config:\n",
    "        print(f\"  Chatbot: {chatbot_agent.agent_config.agent_info['name']} v{chatbot_agent.agent_config.agent_info['version']}\")\n",
    "    if quality_agent and quality_agent.agent_config:\n",
    "        quality_thresholds = quality_agent.agent_config.settings.get('quality_thresholds', {})\n",
    "        print(f\"  Quality Agent: Adequate ‚â•{quality_thresholds.get('adequate_score', 7.0)}, Adjustment ‚â§{quality_thresholds.get('adjustment_score', 5.0)}\")\n",
    "    if frustration_agent and frustration_agent.agent_config:\n",
    "        frustration_thresholds = frustration_agent.agent_config.settings.get('frustration_thresholds', {})\n",
    "        print(f\"  Frustration Agent: High ‚â•{frustration_thresholds.get('high', 6.0)}, Critical ‚â•{frustration_thresholds.get('critical', 8.0)}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Agent initialization failed. Please check configuration and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Step 2: Configure Chat Session\n",
    "\n",
    "Set up the chat session parameters and customer profile for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat session configuration\n",
    "chat_config = {\n",
    "    'session_id': f\"live_chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    'user_id': f\"user_{uuid.uuid4().hex[:8]}\",\n",
    "    'customer_type': 'normal',  # Will be updated based on user behavior\n",
    "    'complexity': 'medium'      # Will be updated based on queries\n",
    "}\n",
    "\n",
    "# Conversation tracking\n",
    "conversation_history = []\n",
    "turn_number = 0\n",
    "live_assessments = []\n",
    "\n",
    "print(f\"üÜî Chat Session Configuration:\")\n",
    "print(f\"  Session ID: {chat_config['session_id']}\")\n",
    "print(f\"  User ID: {chat_config['user_id']}\")\n",
    "print(f\"  Initial Customer Type: {chat_config['customer_type']}\")\n",
    "print(f\"  Initial Complexity: {chat_config['complexity']}\")\n",
    "\n",
    "# Customer profile options\n",
    "print(f\"\\nüë• Customer Profile Options:\")\n",
    "customer_type_selector = widgets.Dropdown(\n",
    "    options=['normal', 'frustrated', 'polite', 'technical', 'non_technical'],\n",
    "    value='normal',\n",
    "    description='Customer Type:'\n",
    ")\n",
    "\n",
    "complexity_selector = widgets.Dropdown(\n",
    "    options=['simple', 'medium', 'complex'],\n",
    "    value='medium',\n",
    "    description='Query Complexity:'\n",
    ")\n",
    "\n",
    "def update_chat_config(change):\n",
    "    \"\"\"Update chat configuration when selectors change\"\"\"\n",
    "    chat_config['customer_type'] = customer_type_selector.value\n",
    "    chat_config['complexity'] = complexity_selector.value\n",
    "    print(f\"Updated: Customer Type = {chat_config['customer_type']}, Complexity = {chat_config['complexity']}\")\n",
    "\n",
    "customer_type_selector.observe(update_chat_config, names='value')\n",
    "complexity_selector.observe(update_chat_config, names='value')\n",
    "\n",
    "display(widgets.HBox([customer_type_selector, complexity_selector]))\n",
    "\n",
    "print(f\"\\nüí° Tips:\")\n",
    "print(f\"  ‚Ä¢ Customer Type affects chatbot response style\")\n",
    "print(f\"  ‚Ä¢ Query Complexity influences routing decisions\")\n",
    "print(f\"  ‚Ä¢ These can be changed during the conversation\")\n",
    "print(f\"  ‚Ä¢ Agents will also detect these automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Step 3: Live Chat Interface\n",
    "\n",
    "Interactive chat interface with real-time AI agent scoring.\n",
    "Type your messages and see live quality and frustration assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat interface\n",
    "chat_output = widgets.Output(layout={'border': '1px solid #ccc', 'height': '400px', 'overflow': 'auto'})\n",
    "message_input = widgets.Text(\n",
    "    placeholder=\"Type your message here and press Enter...\",\n",
    "    layout={'width': '70%'}\n",
    ")\n",
    "send_button = widgets.Button(\n",
    "    description=\"Send\",\n",
    "    button_style='primary',\n",
    "    layout={'width': '10%'}\n",
    ")\n",
    "clear_button = widgets.Button(\n",
    "    description=\"Clear Chat\",\n",
    "    button_style='warning',\n",
    "    layout={'width': '15%'}\n",
    ")\n",
    "\n",
    "# Score display area\n",
    "score_output = widgets.Output(layout={'border': '1px solid #ddd', 'height': '200px', 'overflow': 'auto'})\n",
    "\n",
    "def create_state_for_chat(query: str, ai_response: str = None) -> HybridSystemState:\n",
    "    \"\"\"Create HybridSystemState for chat interaction\"\"\"\n",
    "    global turn_number\n",
    "    turn_number += 1\n",
    "    \n",
    "    return HybridSystemState({\n",
    "        \"query_id\": f\"live_chat_{chat_config['session_id']}_{turn_number}\",\n",
    "        \"user_id\": chat_config['user_id'],\n",
    "        \"session_id\": chat_config['session_id'],\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"query\": query,\n",
    "        \"ai_response\": ai_response,\n",
    "        \"customer_type\": chat_config['customer_type'],\n",
    "        \"complexity\": chat_config['complexity'],\n",
    "        \"turn_number\": turn_number,\n",
    "        \"conversation_metadata\": {\n",
    "            \"live_chat\": True,\n",
    "            \"total_turns\": turn_number,\n",
    "            \"session_start\": chat_config['session_id']\n",
    "        }\n",
    "    })\n",
    "\n",
    "def get_chatbot_response(user_message: str) -> tuple:\n",
    "    \"\"\"Get chatbot response and return (response, confidence, metadata)\"\"\"\n",
    "    try:\n",
    "        state = create_state_for_chat(user_message)\n",
    "        result_state = chatbot_agent(state)\n",
    "        \n",
    "        response = result_state.get('ai_response', 'Sorry, I encountered an error.')\n",
    "        confidence = result_state.get('confidence', 0.5)\n",
    "        metadata = result_state.get('response_metadata', {})\n",
    "        \n",
    "        return response, confidence, metadata\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\", 0.0, {'error': str(e)}\n",
    "\n",
    "def assess_quality(user_message: str, chatbot_response: str) -> dict:\n",
    "    \"\"\"Assess response quality in real-time\"\"\"\n",
    "    try:\n",
    "        state = create_state_for_chat(user_message, chatbot_response)\n",
    "        result_state = quality_agent(state)\n",
    "        \n",
    "        quality_assessment = result_state.get('quality_assessment', {})\n",
    "        \n",
    "        return {\n",
    "            'score': quality_assessment.get('overall_score', 0.0),\n",
    "            'decision': quality_assessment.get('decision', 'unknown'),\n",
    "            'confidence': quality_assessment.get('confidence', 0.0),\n",
    "            'reasoning': quality_assessment.get('reasoning', 'No reasoning available'),\n",
    "            'adjustment_needed': quality_assessment.get('adjustment_needed', False),\n",
    "            'adjusted_response': quality_assessment.get('adjusted_response')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'score': 0.0,\n",
    "            'decision': 'error',\n",
    "            'confidence': 0.0,\n",
    "            'reasoning': f'Quality assessment error: {str(e)}',\n",
    "            'adjustment_needed': False\n",
    "        }\n",
    "\n",
    "def assess_frustration(user_message: str) -> dict:\n",
    "    \"\"\"Assess user frustration in real-time\"\"\"\n",
    "    try:\n",
    "        # Add message to context history for pattern analysis\n",
    "        context_entry = ContextEntry(\n",
    "            entry_id=f\"{chat_config['session_id']}_{turn_number}_query\",\n",
    "            user_id=chat_config['user_id'],\n",
    "            session_id=chat_config['session_id'],\n",
    "            timestamp=datetime.now(),\n",
    "            entry_type=\"query\",\n",
    "            content=user_message,\n",
    "            metadata={'turn_number': turn_number}\n",
    "        )\n",
    "        context_provider.save_context_entry(context_entry)\n",
    "        \n",
    "        state = create_state_for_chat(user_message)\n",
    "        result_state = frustration_agent(state)\n",
    "        \n",
    "        frustration_analysis = result_state.get('frustration_analysis', {})\n",
    "        \n",
    "        return {\n",
    "            'score': frustration_analysis.get('overall_score', 0.0),\n",
    "            'level': frustration_analysis.get('overall_level', 'low'),\n",
    "            'confidence': frustration_analysis.get('confidence', 0.0),\n",
    "            'intervention_needed': result_state.get('frustration_intervention_needed', False),\n",
    "            'contributing_factors': frustration_analysis.get('contributing_factors', []),\n",
    "            'escalation_trend': frustration_analysis.get('history_analysis', {}).get('escalation_trend', 'stable')\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'score': 0.0,\n",
    "            'level': 'error',\n",
    "            'confidence': 0.0,\n",
    "            'intervention_needed': False,\n",
    "            'contributing_factors': [f'Frustration analysis error: {str(e)}'],\n",
    "            'escalation_trend': 'unknown'\n",
    "        }\n",
    "\n",
    "def format_score_display(user_message: str, chatbot_response: str, quality_result: dict, frustration_result: dict) -> str:\n",
    "    \"\"\"Format the real-time scoring display\"\"\"\n",
    "    html = f\"\"\"\n",
    "    <div style='background: #f8f9fa; border-left: 4px solid #007bff; padding: 10px; margin: 5px 0;'>\n",
    "        <strong>Turn {turn_number} - Real-time Assessment</strong><br>\n",
    "        \n",
    "        <div style='margin-top: 8px;'>\n",
    "            <span style='background: #e7f3ff; padding: 2px 6px; border-radius: 3px; margin-right: 10px;'>\n",
    "                ü§ñ <strong>Quality:</strong> {quality_result['score']:.1f}/10.0 \n",
    "                ({quality_result['decision']} - {quality_result['confidence']:.2f} confidence)\n",
    "            </span>\n",
    "            \n",
    "            <span style='background: #fff2e7; padding: 2px 6px; border-radius: 3px;'>\n",
    "                üò§ <strong>Frustration:</strong> {frustration_result['score']:.1f}/10.0 \n",
    "                ({frustration_result['level']} - {frustration_result['confidence']:.2f} confidence)\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        <div style='margin-top: 5px; font-size: 0.9em; color: #666;'>\n",
    "            <strong>Quality:</strong> {quality_result['reasoning'][:100]}{'...' if len(quality_result['reasoning']) > 100 else ''}<br>\n",
    "    \"\"\"\n",
    "    \n",
    "    if frustration_result['contributing_factors']:\n",
    "        factors = ', '.join(frustration_result['contributing_factors'][:3])\n",
    "        html += f\"<strong>Frustration factors:</strong> {factors}<br>\"\n",
    "    \n",
    "    if quality_result['adjustment_needed'] and quality_result.get('adjusted_response'):\n",
    "        html += f\"<strong>Suggested improvement:</strong> {quality_result['adjusted_response'][:100]}{'...' if len(quality_result['adjusted_response']) > 100 else ''}<br>\"\n",
    "    \n",
    "    if frustration_result['intervention_needed']:\n",
    "        html += \"<span style='color: red; font-weight: bold;'>‚ö†Ô∏è Human intervention recommended</span><br>\"\n",
    "    \n",
    "    html += \"</div></div>\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "def process_message():\n",
    "    \"\"\"Process user message and generate response with real-time scoring\"\"\"\n",
    "    user_message = message_input.value.strip()\n",
    "    if not user_message:\n",
    "        return\n",
    "    \n",
    "    with chat_output:\n",
    "        # Display user message\n",
    "        print(f\"üë§ You: {user_message}\")\n",
    "        \n",
    "        # Assess frustration BEFORE chatbot responds (real-world scenario)\n",
    "        frustration_result = assess_frustration(user_message)\n",
    "        \n",
    "        # Get chatbot response\n",
    "        chatbot_response, confidence, metadata = get_chatbot_response(user_message)\n",
    "        \n",
    "        # Display chatbot response\n",
    "        print(f\"ü§ñ Chatbot: {chatbot_response}\")\n",
    "        \n",
    "        # Assess response quality AFTER chatbot responds\n",
    "        quality_result = assess_quality(user_message, chatbot_response)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Display real-time scores\n",
    "    with score_output:\n",
    "        clear_output(wait=False)\n",
    "        display(HTML(format_score_display(user_message, chatbot_response, quality_result, frustration_result)))\n",
    "        \n",
    "        # Show previous turns' scores for context\n",
    "        if len(live_assessments) > 0:\n",
    "            print(f\"\\nüìä Conversation Summary ({len(live_assessments)} previous turns):\")\n",
    "            avg_quality = sum(a['quality']['score'] for a in live_assessments[-5:]) / min(len(live_assessments), 5)\n",
    "            avg_frustration = sum(a['frustration']['score'] for a in live_assessments[-5:]) / min(len(live_assessments), 5)\n",
    "            interventions = sum(1 for a in live_assessments if a['frustration']['intervention_needed'])\n",
    "            \n",
    "            print(f\"  Average Quality (last 5): {avg_quality:.1f}/10.0\")\n",
    "            print(f\"  Average Frustration (last 5): {avg_frustration:.1f}/10.0\")\n",
    "            print(f\"  Total Interventions Recommended: {interventions}\")\n",
    "    \n",
    "    # Store turn data\n",
    "    turn_data = {\n",
    "        'turn_number': turn_number,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'user_message': user_message,\n",
    "        'chatbot_response': chatbot_response,\n",
    "        'chatbot_confidence': confidence,\n",
    "        'chatbot_metadata': metadata,\n",
    "        'quality': quality_result,\n",
    "        'frustration': frustration_result,\n",
    "        'customer_type': chat_config['customer_type'],\n",
    "        'complexity': chat_config['complexity']\n",
    "    }\n",
    "    \n",
    "    conversation_history.append(turn_data)\n",
    "    live_assessments.append(turn_data)\n",
    "    \n",
    "    # Clear input\n",
    "    message_input.value = \"\"\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clear the chat display but keep conversation data\"\"\"\n",
    "    with chat_output:\n",
    "        clear_output(wait=False)\n",
    "        print(\"Chat display cleared. (Conversation data preserved for export)\")\n",
    "    \n",
    "    with score_output:\n",
    "        clear_output(wait=False)\n",
    "        print(\"Ready for new messages...\")\n",
    "\n",
    "# Event handlers\n",
    "def on_send_click(b):\n",
    "    process_message()\n",
    "\n",
    "def on_enter_key(text):\n",
    "    process_message()\n",
    "\n",
    "def on_clear_click(b):\n",
    "    clear_chat()\n",
    "\n",
    "send_button.on_click(on_send_click)\n",
    "message_input.on_submit(on_enter_key)\n",
    "clear_button.on_click(on_clear_click)\n",
    "\n",
    "# Display interface\n",
    "print(\"üí¨ Live Chat Interface with Real-time AI Assessment\")\n",
    "print(\"Type a message below and press Enter or click Send.\")\n",
    "print(\"Watch the scores update in real-time after each interaction!\\n\")\n",
    "\n",
    "input_box = widgets.HBox([message_input, send_button, clear_button])\n",
    "display(input_box)\n",
    "\n",
    "print(\"\\nüì± Chat Window:\")\n",
    "display(chat_output)\n",
    "\n",
    "print(\"\\nüìä Real-time Agent Assessments:\")\n",
    "display(score_output)\n",
    "\n",
    "# Initialize with welcome message\n",
    "with chat_output:\n",
    "    print(f\"üöÄ Live Chat Session Started: {chat_config['session_id']}\")\n",
    "    print(f\"üí° Try asking questions, expressing frustration, or testing different scenarios.\")\n",
    "    print(f\"üìä Quality and frustration scores will appear below after each interaction.\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "with score_output:\n",
    "    print(\"Ready for your first message! Scores will appear here after each turn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 4: Conversation Analysis Dashboard\n",
    "\n",
    "Real-time analysis of the ongoing conversation with trends and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation analysis dashboard\n",
    "analysis_output = widgets.Output()\n",
    "\n",
    "def update_analysis():\n",
    "    \"\"\"Update conversation analysis in real-time\"\"\"\n",
    "    if not conversation_history:\n",
    "        with analysis_output:\n",
    "            clear_output(wait=False)\n",
    "            print(\"No conversation data yet. Start chatting to see analysis!\")\n",
    "        return\n",
    "    \n",
    "    with analysis_output:\n",
    "        clear_output(wait=False)\n",
    "        \n",
    "        df = pd.DataFrame(conversation_history)\n",
    "        \n",
    "        print(f\"üìä Live Conversation Analysis Dashboard\")\n",
    "        print(f\"Session: {chat_config['session_id']}\")\n",
    "        print(f\"Total Turns: {len(conversation_history)}\")\n",
    "        print(f\"Duration: {datetime.now() - datetime.fromisoformat(conversation_history[0]['timestamp']):.0f}\" if len(conversation_history) > 0 else \"\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "        # Quality trends\n",
    "        quality_scores = [turn['quality']['score'] for turn in conversation_history]\n",
    "        avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "        quality_trend = \"‚ÜóÔ∏è\" if len(quality_scores) > 1 and quality_scores[-1] > quality_scores[-2] else \"‚ÜòÔ∏è\" if len(quality_scores) > 1 and quality_scores[-1] < quality_scores[-2] else \"‚û°Ô∏è\"\n",
    "        \n",
    "        print(f\"üéØ QUALITY METRICS:\")\n",
    "        print(f\"  Average Quality: {avg_quality:.1f}/10.0 {quality_trend}\")\n",
    "        print(f\"  Latest Quality: {quality_scores[-1]:.1f}/10.0\")\n",
    "        \n",
    "        quality_decisions = [turn['quality']['decision'] for turn in conversation_history]\n",
    "        decision_counts = pd.Series(quality_decisions).value_counts()\n",
    "        for decision, count in decision_counts.items():\n",
    "            print(f\"  {decision.replace('_', ' ').title()}: {count}/{len(conversation_history)} ({count/len(conversation_history)*100:.0f}%)\")\n",
    "        \n",
    "        # Frustration trends\n",
    "        frustration_scores = [turn['frustration']['score'] for turn in conversation_history]\n",
    "        avg_frustration = sum(frustration_scores) / len(frustration_scores)\n",
    "        frustration_trend = \"‚ö†Ô∏è\" if len(frustration_scores) > 1 and frustration_scores[-1] > frustration_scores[-2] else \"‚úÖ\" if len(frustration_scores) > 1 and frustration_scores[-1] < frustration_scores[-2] else \"‚û°Ô∏è\"\n",
    "        \n",
    "        print(f\"\\nüò§ FRUSTRATION METRICS:\")\n",
    "        print(f\"  Average Frustration: {avg_frustration:.1f}/10.0 {frustration_trend}\")\n",
    "        print(f\"  Latest Frustration: {frustration_scores[-1]:.1f}/10.0\")\n",
    "        \n",
    "        frustration_levels = [turn['frustration']['level'] for turn in conversation_history]\n",
    "        level_counts = pd.Series(frustration_levels).value_counts()\n",
    "        for level, count in level_counts.items():\n",
    "            print(f\"  {level.replace('_', ' ').title()}: {count}/{len(conversation_history)} ({count/len(conversation_history)*100:.0f}%)\")\n",
    "        \n",
    "        # Intervention recommendations\n",
    "        interventions = sum(1 for turn in conversation_history if turn['frustration']['intervention_needed'])\n",
    "        if interventions > 0:\n",
    "            print(f\"\\nüö® INTERVENTION ALERTS:\")\n",
    "            print(f\"  Human intervention recommended: {interventions} times\")\n",
    "            print(f\"  Intervention rate: {interventions/len(conversation_history)*100:.0f}%\")\n",
    "        \n",
    "        # Recent turn details\n",
    "        print(f\"\\nüìù RECENT TURNS:\")\n",
    "        for turn in conversation_history[-3:]:\n",
    "            print(f\"  Turn {turn['turn_number']}: Quality {turn['quality']['score']:.1f}, Frustration {turn['frustration']['score']:.1f} ({turn['frustration']['level']})\")\n",
    "            if turn['quality']['adjustment_needed']:\n",
    "                print(f\"    ‚ö†Ô∏è Quality improvement suggested\")\n",
    "            if turn['frustration']['intervention_needed']:\n",
    "                print(f\"    üö® Human intervention recommended\")\n",
    "\n",
    "# Auto-refresh button\n",
    "refresh_button = widgets.Button(description=\"Refresh Analysis\", button_style='info')\n",
    "refresh_button.on_click(lambda b: update_analysis())\n",
    "\n",
    "print(\"üìä Live Conversation Analysis\")\n",
    "print(\"Click 'Refresh Analysis' to see updated metrics during your conversation.\")\n",
    "display(refresh_button)\n",
    "display(analysis_output)\n",
    "\n",
    "# Initial update\n",
    "update_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 5: Export Conversation Results\n",
    "\n",
    "Export the complete conversation with all AI assessments for analysis or integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export conversation results\n",
    "def export_live_chat_results():\n",
    "    \"\"\"Export complete conversation with real-time assessments\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"‚ùå No conversation data to export. Start chatting first!\")\n",
    "        return None\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = Path('live_chat_exports')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare export data\n",
    "    export_data = {\n",
    "        'metadata': {\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'export_type': 'live_chat_session',\n",
    "            'session_id': chat_config['session_id'],\n",
    "            'user_id': chat_config['user_id'],\n",
    "            'total_turns': len(conversation_history),\n",
    "            'session_duration_minutes': ((datetime.now() - datetime.fromisoformat(conversation_history[0]['timestamp'])).total_seconds() / 60) if conversation_history else 0,\n",
    "            'customer_profile': {\n",
    "                'final_customer_type': chat_config['customer_type'],\n",
    "                'final_complexity': chat_config['complexity']\n",
    "            },\n",
    "            'agent_versions': {\n",
    "                'chatbot_agent': chatbot_agent.agent_config.agent_info['version'] if chatbot_agent.agent_config else 'unknown',\n",
    "                'quality_agent': quality_agent.agent_config.agent_info['version'] if quality_agent.agent_config else 'unknown',\n",
    "                'frustration_agent': frustration_agent.agent_config.agent_info['version'] if frustration_agent.agent_config else 'unknown'\n",
    "            }\n",
    "        },\n",
    "        'conversation_history': conversation_history,\n",
    "        'summary_statistics': {\n",
    "            'quality_metrics': {\n",
    "                'average_score': sum(turn['quality']['score'] for turn in conversation_history) / len(conversation_history),\n",
    "                'score_range': [min(turn['quality']['score'] for turn in conversation_history), max(turn['quality']['score'] for turn in conversation_history)],\n",
    "                'decision_distribution': dict(pd.Series([turn['quality']['decision'] for turn in conversation_history]).value_counts()),\n",
    "                'adjustments_needed': sum(1 for turn in conversation_history if turn['quality']['adjustment_needed'])\n",
    "            },\n",
    "            'frustration_metrics': {\n",
    "                'average_score': sum(turn['frustration']['score'] for turn in conversation_history) / len(conversation_history),\n",
    "                'score_range': [min(turn['frustration']['score'] for turn in conversation_history), max(turn['frustration']['score'] for turn in conversation_history)],\n",
    "                'level_distribution': dict(pd.Series([turn['frustration']['level'] for turn in conversation_history]).value_counts()),\n",
    "                'interventions_recommended': sum(1 for turn in conversation_history if turn['frustration']['intervention_needed']),\n",
    "                'escalation_trends': dict(pd.Series([turn['frustration']['escalation_trend'] for turn in conversation_history]).value_counts())\n",
    "            },\n",
    "            'conversation_flow': {\n",
    "                'quality_trend': 'improving' if len(conversation_history) > 1 and conversation_history[-1]['quality']['score'] > conversation_history[0]['quality']['score'] else 'declining' if len(conversation_history) > 1 and conversation_history[-1]['quality']['score'] < conversation_history[0]['quality']['score'] else 'stable',\n",
    "                'frustration_trend': 'escalating' if len(conversation_history) > 1 and conversation_history[-1]['frustration']['score'] > conversation_history[0]['frustration']['score'] else 'de-escalating' if len(conversation_history) > 1 and conversation_history[-1]['frustration']['score'] < conversation_history[0]['frustration']['score'] else 'stable'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Export main results\n",
    "    results_filename = f'live_chat_results_{timestamp}.json'\n",
    "    results_path = output_dir / results_filename\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    # Export CSV summary\n",
    "    csv_data = []\n",
    "    for turn in conversation_history:\n",
    "        csv_data.append({\n",
    "            'turn_number': turn['turn_number'],\n",
    "            'timestamp': turn['timestamp'],\n",
    "            'user_message': turn['user_message'],\n",
    "            'chatbot_response': turn['chatbot_response'],\n",
    "            'quality_score': turn['quality']['score'],\n",
    "            'quality_decision': turn['quality']['decision'],\n",
    "            'quality_confidence': turn['quality']['confidence'],\n",
    "            'frustration_score': turn['frustration']['score'],\n",
    "            'frustration_level': turn['frustration']['level'],\n",
    "            'frustration_confidence': turn['frustration']['confidence'],\n",
    "            'intervention_needed': turn['frustration']['intervention_needed'],\n",
    "            'customer_type': turn['customer_type'],\n",
    "            'complexity': turn['complexity']\n",
    "        })\n",
    "    \n",
    "    csv_filename = f'live_chat_summary_{timestamp}.csv'\n",
    "    csv_path = output_dir / csv_filename\n",
    "    pd.DataFrame(csv_data).to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Generate session report\n",
    "    report_filename = f'live_chat_report_{timestamp}.txt'\n",
    "    report_path = output_dir / report_filename\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"LIVE CHAT SESSION REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Session ID: {chat_config['session_id']}\\n\")\n",
    "        f.write(f\"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Turns: {len(conversation_history)}\\n\")\n",
    "        f.write(f\"Session Duration: {export_data['metadata']['session_duration_minutes']:.1f} minutes\\n\\n\")\n",
    "        \n",
    "        f.write(\"QUALITY ASSESSMENT SUMMARY:\\n\")\n",
    "        quality_stats = export_data['summary_statistics']['quality_metrics']\n",
    "        f.write(f\"  Average Quality Score: {quality_stats['average_score']:.2f}/10.0\\n\")\n",
    "        f.write(f\"  Quality Range: {quality_stats['score_range'][0]:.1f} - {quality_stats['score_range'][1]:.1f}\\n\")\n",
    "        f.write(f\"  Adjustments Needed: {quality_stats['adjustments_needed']}/{len(conversation_history)} ({quality_stats['adjustments_needed']/len(conversation_history)*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"FRUSTRATION ANALYSIS SUMMARY:\\n\")\n",
    "        frustration_stats = export_data['summary_statistics']['frustration_metrics']\n",
    "        f.write(f\"  Average Frustration Score: {frustration_stats['average_score']:.2f}/10.0\\n\")\n",
    "        f.write(f\"  Frustration Range: {frustration_stats['score_range'][0]:.1f} - {frustration_stats['score_range'][1]:.1f}\\n\")\n",
    "        f.write(f\"  Interventions Recommended: {frustration_stats['interventions_recommended']}/{len(conversation_history)} ({frustration_stats['interventions_recommended']/len(conversation_history)*100:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"CONVERSATION FLOW:\\n\")\n",
    "        flow = export_data['summary_statistics']['conversation_flow']\n",
    "        f.write(f\"  Quality Trend: {flow['quality_trend']}\\n\")\n",
    "        f.write(f\"  Frustration Trend: {flow['frustration_trend']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"TURN-BY-TURN SUMMARY:\\n\")\n",
    "        for turn in conversation_history:\n",
    "            f.write(f\"  Turn {turn['turn_number']}: Q={turn['quality']['score']:.1f} F={turn['frustration']['score']:.1f} ({turn['frustration']['level']})\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Live chat session exported successfully!\")\n",
    "    print(f\"\\nüìÅ Export Files:\")\n",
    "    print(f\"  Complete Data: {results_path}\")\n",
    "    print(f\"  CSV Summary: {csv_path}\")\n",
    "    print(f\"  Session Report: {report_path}\")\n",
    "    \n",
    "    print(f\"\\nüìä Session Summary:\")\n",
    "    print(f\"  Total turns: {len(conversation_history)}\")\n",
    "    print(f\"  Average quality: {export_data['summary_statistics']['quality_metrics']['average_score']:.1f}/10.0\")\n",
    "    print(f\"  Average frustration: {export_data['summary_statistics']['frustration_metrics']['average_score']:.1f}/10.0\")\n",
    "    print(f\"  Interventions needed: {export_data['summary_statistics']['frustration_metrics']['interventions_recommended']}\")\n",
    "    \n",
    "    return {\n",
    "        'results_file': str(results_path),\n",
    "        'csv_file': str(csv_path),\n",
    "        'report_file': str(report_path)\n",
    "    }\n",
    "\n",
    "# Export button\n",
    "export_button = widgets.Button(\n",
    "    description=\"Export Conversation\",\n",
    "    button_style='success',\n",
    "    icon='download'\n",
    ")\n",
    "\n",
    "export_output = widgets.Output()\n",
    "\n",
    "def on_export_click(b):\n",
    "    with export_output:\n",
    "        clear_output(wait=False)\n",
    "        export_live_chat_results()\n",
    "\n",
    "export_button.on_click(on_export_click)\n",
    "\n",
    "print(\"üíæ Export Live Chat Results\")\n",
    "print(\"Click to export your conversation with all AI assessments and scoring data.\")\n",
    "display(export_button)\n",
    "display(export_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 6: Session Summary and Insights\n",
    "\n",
    "Final summary of the live chat session with insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session summary and insights\n",
    "def generate_session_insights():\n",
    "    \"\"\"Generate insights and recommendations from the live chat session\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"‚ùå No conversation data available. Start chatting to generate insights!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üéØ LIVE CHAT SESSION INSIGHTS\")\n",
    "    print(f\"Session: {chat_config['session_id']}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    quality_scores = [turn['quality']['score'] for turn in conversation_history]\n",
    "    frustration_scores = [turn['frustration']['score'] for turn in conversation_history]\n",
    "    \n",
    "    avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "    avg_frustration = sum(frustration_scores) / len(frustration_scores)\n",
    "    \n",
    "    interventions = sum(1 for turn in conversation_history if turn['frustration']['intervention_needed'])\n",
    "    adjustments = sum(1 for turn in conversation_history if turn['quality']['adjustment_needed'])\n",
    "    \n",
    "    print(f\"\\nüìä SESSION METRICS:\")\n",
    "    print(f\"  Total Conversation Turns: {len(conversation_history)}\")\n",
    "    print(f\"  Average Response Quality: {avg_quality:.1f}/10.0\")\n",
    "    print(f\"  Average User Frustration: {avg_frustration:.1f}/10.0\")\n",
    "    print(f\"  Quality Adjustments Suggested: {adjustments} ({adjustments/len(conversation_history)*100:.0f}%)\")\n",
    "    print(f\"  Human Interventions Recommended: {interventions} ({interventions/len(conversation_history)*100:.0f}%)\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    print(f\"\\nüèÜ PERFORMANCE ASSESSMENT:\")\n",
    "    \n",
    "    if avg_quality >= 8.0:\n",
    "        print(f\"  Quality: ‚úÖ EXCELLENT - Chatbot performing very well\")\n",
    "    elif avg_quality >= 7.0:\n",
    "        print(f\"  Quality: ‚úÖ GOOD - Chatbot meeting quality standards\")\n",
    "    elif avg_quality >= 6.0:\n",
    "        print(f\"  Quality: ‚ö†Ô∏è ADEQUATE - Some improvement opportunities\")\n",
    "    else:\n",
    "        print(f\"  Quality: ‚ùå NEEDS IMPROVEMENT - Quality below acceptable threshold\")\n",
    "    \n",
    "    if avg_frustration <= 2.0:\n",
    "        print(f\"  Frustration: ‚úÖ EXCELLENT - Customer remained calm and satisfied\")\n",
    "    elif avg_frustration <= 4.0:\n",
    "        print(f\"  Frustration: ‚úÖ GOOD - Minor frustration levels\")\n",
    "    elif avg_frustration <= 6.0:\n",
    "        print(f\"  Frustration: ‚ö†Ô∏è MODERATE - Some customer frustration detected\")\n",
    "    else:\n",
    "        print(f\"  Frustration: ‚ùå HIGH - Significant customer frustration present\")\n",
    "    \n",
    "    # Trend analysis\n",
    "    print(f\"\\nüìà TREND ANALYSIS:\")\n",
    "    if len(quality_scores) > 2:\n",
    "        early_quality = sum(quality_scores[:len(quality_scores)//2]) / (len(quality_scores)//2)\n",
    "        late_quality = sum(quality_scores[len(quality_scores)//2:]) / (len(quality_scores) - len(quality_scores)//2)\n",
    "        \n",
    "        if late_quality > early_quality + 0.5:\n",
    "            print(f\"  Quality Trend: ‚ÜóÔ∏è IMPROVING ({early_quality:.1f} ‚Üí {late_quality:.1f})\")\n",
    "        elif late_quality < early_quality - 0.5:\n",
    "            print(f\"  Quality Trend: ‚ÜòÔ∏è DECLINING ({early_quality:.1f} ‚Üí {late_quality:.1f})\")\n",
    "        else:\n",
    "            print(f\"  Quality Trend: ‚û°Ô∏è STABLE ({avg_quality:.1f} average)\")\n",
    "        \n",
    "        early_frustration = sum(frustration_scores[:len(frustration_scores)//2]) / (len(frustration_scores)//2)\n",
    "        late_frustration = sum(frustration_scores[len(frustration_scores)//2:]) / (len(frustration_scores) - len(frustration_scores)//2)\n",
    "        \n",
    "        if late_frustration > early_frustration + 0.5:\n",
    "            print(f\"  Frustration Trend: ‚ö†Ô∏è ESCALATING ({early_frustration:.1f} ‚Üí {late_frustration:.1f})\")\n",
    "        elif late_frustration < early_frustration - 0.5:\n",
    "            print(f\"  Frustration Trend: ‚úÖ DE-ESCALATING ({early_frustration:.1f} ‚Üí {late_frustration:.1f})\")\n",
    "        else:\n",
    "            print(f\"  Frustration Trend: ‚û°Ô∏è STABLE ({avg_frustration:.1f} average)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    \n",
    "    if avg_quality < 7.0:\n",
    "        print(f\"  üîß Consider adjusting chatbot prompts or model settings to improve response quality\")\n",
    "    \n",
    "    if adjustments > len(conversation_history) * 0.3:\n",
    "        print(f\"  üìù High rate of quality adjustments suggests need for prompt engineering\")\n",
    "    \n",
    "    if avg_frustration > 5.0:\n",
    "        print(f\"  üò§ High frustration levels indicate need for better frustration handling\")\n",
    "    \n",
    "    if interventions > 0:\n",
    "        print(f\"  üö® {interventions} intervention(s) recommended - ensure human agents are available\")\n",
    "    \n",
    "    if len(conversation_history) > 8:\n",
    "        print(f\"  üìû Long conversation ({len(conversation_history)} turns) - consider proactive escalation\")\n",
    "    \n",
    "    # Success factors\n",
    "    print(f\"\\nüåü SESSION HIGHLIGHTS:\")\n",
    "    \n",
    "    best_quality_turn = max(conversation_history, key=lambda x: x['quality']['score'])\n",
    "    print(f\"  Best Quality Response: Turn {best_quality_turn['turn_number']} ({best_quality_turn['quality']['score']:.1f}/10.0)\")\n",
    "    \n",
    "    lowest_frustration_turn = min(conversation_history, key=lambda x: x['frustration']['score'])\n",
    "    print(f\"  Lowest Frustration: Turn {lowest_frustration_turn['turn_number']} ({lowest_frustration_turn['frustration']['score']:.1f}/10.0)\")\n",
    "    \n",
    "    if interventions == 0:\n",
    "        print(f\"  üéâ No human interventions needed - AI handled conversation successfully\")\n",
    "    \n",
    "    if avg_quality >= 8.0 and avg_frustration <= 3.0:\n",
    "        print(f\"  üèÜ OUTSTANDING SESSION - High quality responses with low customer frustration\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Session analysis complete! Export data for detailed review.\")\n",
    "\n",
    "# Generate insights button\n",
    "insights_button = widgets.Button(\n",
    "    description=\"Generate Insights\",\n",
    "    button_style='info',\n",
    "    icon='chart-line'\n",
    ")\n",
    "\n",
    "insights_output = widgets.Output()\n",
    "\n",
    "def on_insights_click(b):\n",
    "    with insights_output:\n",
    "        clear_output(wait=False)\n",
    "        generate_session_insights()\n",
    "\n",
    "insights_button.on_click(on_insights_click)\n",
    "\n",
    "print(\"üéØ Session Insights and Recommendations\")\n",
    "print(\"Click to generate detailed insights about your live chat session.\")\n",
    "display(insights_button)\n",
    "display(insights_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ You're Ready to Test!\n",
    "\n",
    "### Quick Start Guide:\n",
    "1. **Initialize Agents** (Step 1) - Run once to set up all AI agents\n",
    "2. **Configure Session** (Step 2) - Set customer type and complexity\n",
    "3. **Start Chatting** (Step 3) - Use the chat interface like a real customer\n",
    "4. **Watch Scores** - See real-time quality and frustration assessments\n",
    "5. **Analyze Results** (Step 4) - View conversation trends and metrics\n",
    "6. **Export Data** (Step 5) - Save complete results for analysis\n",
    "7. **Generate Insights** (Step 6) - Get recommendations and summary\n",
    "\n",
    "### Perfect for:\n",
    "- **Stakeholder Demos**: Show live AI quality control and frustration detection\n",
    "- **System Validation**: Test with real human interactions vs. simulated customers\n",
    "- **Configuration Tuning**: See immediate impact of agent settings changes\n",
    "- **Training Data**: Generate high-quality human-validated conversation examples\n",
    "- **Performance Testing**: Validate AI agent accuracy with real-world scenarios\n",
    "\n",
    "### Tips for Effective Testing:\n",
    "- Try different conversation styles (frustrated, polite, technical, etc.)\n",
    "- Test edge cases and challenging scenarios\n",
    "- Experiment with different customer types and complexity levels\n",
    "- Watch how scores change in real-time as the conversation evolves\n",
    "- Export results to compare different configuration settings\n",
    "\n",
    "**Happy testing! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}