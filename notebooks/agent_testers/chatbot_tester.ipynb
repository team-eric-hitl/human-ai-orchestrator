{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Agent Testing Notebook\n",
    "\n",
    "This notebook provides a user-friendly interface for testing the Chatbot Agent with pre-generated questions.\n",
    "It's designed for users with little programming experience.\n",
    "\n",
    "## Features:\n",
    "- Load and edit agent configuration settings\n",
    "- Load test questions from JSON files\n",
    "- Process questions through the Chatbot Agent\n",
    "- Review and analyze results\n",
    "- Export results with timestamps\n",
    "\n",
    "## Getting Started:\n",
    "1. Run cells in order from top to bottom\n",
    "2. Edit configuration values as needed\n",
    "3. Load test questions from file (generated using question_generator.ipynb)\n",
    "4. Review questions before processing\n",
    "5. Run the agent and review results\n",
    "\n",
    "## Question Generation:\n",
    "Use the separate `question_generator.ipynb` notebook to create test questions, then load them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Ready to start testing the Chatbot Agent.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from ruamel.yaml import YAML  # Changed from: import yaml\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set the working directory to the root of the project\n",
    "os.chdir('/workspace')\n",
    "\n",
    "# Add workspace to path for imports (this helps with relative imports)\n",
    "sys.path.insert(0, '/workspace')\n",
    "\n",
    "# Import our system components\n",
    "from src.nodes.chatbot_agent import ChatbotAgentNode\n",
    "from src.core.config.agent_config_manager import AgentConfigManager\n",
    "from src.integrations.llm_providers import LLMProviderFactory\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"Ready to start testing the Chatbot Agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration Settings\n",
    "\n",
    "The following cell loads the current configuration for the Chatbot Agent.\n",
    "You can edit these values to customize the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Configuration files loaded and temporary copies created with comments preserved!\n",
      "Agent name: chatbot_agent\n",
      "Agent version: 1.0.0\n",
      "Preferred model: local_general_standard\n",
      "\n",
      "üíæ Temporary config files created at:\n",
      "  agent_config: /tmp/chatbot_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/chatbot_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/chatbot_agent_configs/models.yaml\n",
      "\n",
      "üí° These temp files retain original comments and can be edited directly in Step 2.\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from files and create temporary editable copies\n",
    "config_base_path = Path('/workspace/config')\n",
    "agent_config_path = config_base_path / 'agents' / 'chatbot_agent'\n",
    "temp_config_dir = Path('/tmp/chatbot_agent_configs')\n",
    "\n",
    "def load_and_create_temp_configs():\n",
    "    \"\"\"Load all configuration files and create temporary editable copies with comments preserved\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Create YAML instance for comment preservation\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True\n",
    "    yaml.default_flow_style = False\n",
    "    \n",
    "    # Create temp directory\n",
    "    temp_config_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load original files for parsing (to access data)\n",
    "    with open(agent_config_path / 'config.yaml', 'r') as f:\n",
    "        configs['agent'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'prompts.yaml', 'r') as f:\n",
    "        configs['prompts'] = yaml.load(f)\n",
    "    \n",
    "    with open(agent_config_path / 'models.yaml', 'r') as f:\n",
    "        configs['models'] = yaml.load(f)\n",
    "    \n",
    "    # Load shared models for reference\n",
    "    with open(config_base_path / 'shared' / 'models.yaml', 'r') as f:\n",
    "        configs['shared_models'] = yaml.load(f)\n",
    "    \n",
    "    # Create temp file paths\n",
    "    temp_agent_path = temp_config_dir / 'config.yaml'\n",
    "    temp_prompts_path = temp_config_dir / 'prompts.yaml'\n",
    "    temp_models_path = temp_config_dir / 'models.yaml'\n",
    "    \n",
    "    # Copy original files to temp directory to preserve comments and formatting\n",
    "    import shutil\n",
    "    shutil.copy2(agent_config_path / 'config.yaml', temp_agent_path)\n",
    "    shutil.copy2(agent_config_path / 'prompts.yaml', temp_prompts_path)  \n",
    "    shutil.copy2(agent_config_path / 'models.yaml', temp_models_path)\n",
    "    \n",
    "    return configs, {\n",
    "        'agent_config': temp_agent_path,\n",
    "        'prompts_config': temp_prompts_path,\n",
    "        'models_config': temp_models_path\n",
    "    }\n",
    "\n",
    "# Load configurations and create temp files\n",
    "configs, temp_file_paths = load_and_create_temp_configs()\n",
    "\n",
    "print(\"üìÅ Configuration files loaded and temporary copies created with comments preserved!\")\n",
    "print(f\"Agent name: {configs['agent']['agent']['name']}\")\n",
    "print(f\"Agent version: {configs['agent']['agent']['version']}\")\n",
    "\n",
    "# Get preferred model from models config (not agent config)\n",
    "preferred_model = \"Unknown\"\n",
    "if 'primary_model' in configs['models']:\n",
    "    preferred_model = configs['models']['primary_model']\n",
    "elif 'preferred' in configs['models']:\n",
    "    preferred_model = configs['models']['preferred']\n",
    "\n",
    "print(f\"Preferred model: {preferred_model}\")\n",
    "print(f\"\\nüíæ Temporary config files created at:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "print(f\"\\nüí° These temp files retain original comments and can be edited directly in Step 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Editable Configuration Settings\n",
    "\n",
    "Edit these settings to customize how the Chatbot Agent behaves.\n",
    "These variables map directly to the configuration files and can be exported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration File Editor\n",
      "Edit the YAML configuration files below and use the Save buttons to apply changes.\n",
      "Changes are saved to temporary files and will be used in Step 5.\n",
      "\n",
      "üìÑ 1. Agent Configuration (config.yaml)\n",
      "Contains: agent settings, behavior, escalation thresholds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b61e9e27c14bd5bd14dc85b6767495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='# Chatbot Agent Configuration\\n# This agent provides primary customer service responses and ha‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71bb29d97244ccc924c6e432e2b5c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Agent Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 2. Prompts Configuration (prompts.yaml)\n",
      "Contains: system prompts, response guidelines, communication style\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066f37552f87413cbe9a435bf3901599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='system: |\\n  You are a professional customer service chatbot dedicated to providing exceptiona‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5df2e8017bd4c768e1e959653f8999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Prompts Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ 3. Models Configuration (models.yaml)\n",
      "Contains: preferred model, fallback models, model-specific settings\n",
      "\n",
      "üîç Available Model Aliases:\n",
      "Use these aliases in your models configuration below:\n",
      "\n",
      "üì° ANTHROPIC Provider:\n",
      "  ‚Ä¢ anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 - Anthropic Claude 3.5 Haiku - fast and efficient\n",
      "  ‚Ä¢ anthropic_general_standard ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_reasoning_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_coding_premium ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "  ‚Ä¢ anthropic_flagship ‚Üí claude-3-5-sonnet-20241022 - Anthropic Claude 3.5 Sonnet - balanced performance and reasoning\n",
      "\n",
      "üì° OPENAI Provider:\n",
      "  ‚Ä¢ openai_general_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "  ‚Ä¢ openai_general_budget ‚Üí gpt-3.5-turbo - OpenAI GPT-3.5 Turbo - fast and cost-effective\n",
      "  ‚Ä¢ openai_coding_standard ‚Üí gpt-4 - OpenAI GPT-4 - highest quality, requires API key\n",
      "\n",
      "üì° LLAMA Provider:\n",
      "  ‚Ä¢ local_general_standard ‚Üí llama-7b - Llama 7B model - good balance of speed and quality\n",
      "  ‚Ä¢ local_general_premium ‚Üí llama-13b - Llama 13B model - higher quality, slower inference\n",
      "  ‚Ä¢ local_coding_standard ‚Üí codellama-7b - CodeLlama 7B - optimized for code generation\n",
      "\n",
      "üì° MISTRAL Provider:\n",
      "  ‚Ä¢ local_general_budget ‚Üí mistral-7b - Mistral 7B Instruct - excellent instruction following\n",
      "\n",
      "üìã Current Models Configuration:\n",
      "  Preferred: local_general_standard\n",
      "  Fallback: []\n",
      "\n",
      "üí° Tips for editing:\n",
      "  ‚Ä¢ Change 'primary_model' or 'preferred' to any alias from the list above\n",
      "  ‚Ä¢ Add/remove aliases in the 'fallback' list\n",
      "  ‚Ä¢ Aliases are case-sensitive\n",
      "  ‚Ä¢ Invalid aliases will cause errors during processing\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4b1b92ff3e426db680beae06ee02c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='primary_model: \"local_general_standard\"\\n\\nmodel_preferences:\\n  general_queries:\\n    primary‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e9b3d7ba754682b2bfb74c593670ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Save Models Config', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Save All Changes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076181f25b81407ca3f6f68affb28a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Save All Configs', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Temp config files location:\n",
      "  agent_config: /tmp/chatbot_agent_configs/config.yaml\n",
      "  prompts_config: /tmp/chatbot_agent_configs/prompts.yaml\n",
      "  models_config: /tmp/chatbot_agent_configs/models.yaml\n",
      "\n",
      "üí° Tips:\n",
      "  ‚Ä¢ Edit YAML directly in the text areas above\n",
      "  ‚Ä¢ Use Save buttons to apply changes to temp files\n",
      "  ‚Ä¢ YAML syntax is validated before saving\n",
      "  ‚Ä¢ Model aliases are validated against available models\n",
      "  ‚Ä¢ Changes will be used in Step 5 when processing questions\n",
      "  ‚Ä¢ Original config files remain unchanged\n",
      "  ‚Ä¢ Comments and formatting are preserved during editing!\n"
     ]
    }
   ],
   "source": [
    "# Display and edit configuration files in separate windows\n",
    "\n",
    "def load_config_file_contents():\n",
    "    \"\"\"Load current config file contents from temp files\"\"\"\n",
    "    with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "        agent_config_content = f.read()\n",
    "    with open(temp_file_paths['prompts_config'], 'r') as f:\n",
    "        prompts_config_content = f.read()\n",
    "    with open(temp_file_paths['models_config'], 'r') as f:\n",
    "        models_config_content = f.read()\n",
    "    \n",
    "    return agent_config_content, prompts_config_content, models_config_content\n",
    "\n",
    "# Load current config file contents\n",
    "agent_config_content, prompts_config_content, models_config_content = load_config_file_contents()\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration File Editor\")\n",
    "print(\"Edit the YAML configuration files below and use the Save buttons to apply changes.\")\n",
    "print(\"Changes are saved to temporary files and will be used in Step 5.\\n\")\n",
    "\n",
    "# Create text areas for each config file\n",
    "print(\"üìÑ 1. Agent Configuration (config.yaml)\")\n",
    "print(\"Contains: agent settings, behavior, escalation thresholds\")\n",
    "\n",
    "agent_config_editor = widgets.Textarea(\n",
    "    value=agent_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_agent_config(button):\n",
    "    \"\"\"Save agent config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(agent_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['agent_config'], 'w') as f:\n",
    "            f.write(agent_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Agent config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in agent config: {e}\")\n",
    "\n",
    "agent_save_btn = widgets.Button(description=\"Save Agent Config\", button_style='success')\n",
    "agent_save_btn.on_click(save_agent_config)\n",
    "\n",
    "display(agent_config_editor)\n",
    "display(agent_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 2. Prompts Configuration (prompts.yaml)\")\n",
    "print(\"Contains: system prompts, response guidelines, communication style\")\n",
    "\n",
    "prompts_config_editor = widgets.Textarea(\n",
    "    value=prompts_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='250px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_prompts_config(button):\n",
    "    \"\"\"Save prompts config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        yaml.load(prompts_config_editor.value)\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['prompts_config'], 'w') as f:\n",
    "            f.write(prompts_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Prompts config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in prompts config: {e}\")\n",
    "\n",
    "prompts_save_btn = widgets.Button(description=\"Save Prompts Config\", button_style='success')\n",
    "prompts_save_btn.on_click(save_prompts_config)\n",
    "\n",
    "display(prompts_config_editor)\n",
    "display(prompts_save_btn)\n",
    "\n",
    "print(\"\\nüìÑ 3. Models Configuration (models.yaml)\")\n",
    "print(\"Contains: preferred model, fallback models, model-specific settings\")\n",
    "\n",
    "# Show available model aliases from shared models config\n",
    "def display_available_models():\n",
    "    \"\"\"Display available model aliases and their actual models\"\"\"\n",
    "    try:\n",
    "        shared_models = configs['shared_models']\n",
    "        \n",
    "        # Extract model aliases and models sections\n",
    "        model_aliases = shared_models.get('model_aliases', {})\n",
    "        models = shared_models.get('models', {})\n",
    "        \n",
    "        if not model_aliases:\n",
    "            print(\"‚ùå No model aliases found in shared configuration\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nüîç Available Model Aliases:\")\n",
    "        print(\"Use these aliases in your models configuration below:\\n\")\n",
    "        \n",
    "        # Group by provider for better organization\n",
    "        providers = {}\n",
    "        for alias, actual_model_name in model_aliases.items():\n",
    "            # Get model details from models section\n",
    "            model_details = models.get(actual_model_name, {})\n",
    "            provider = model_details.get('type', 'unknown')\n",
    "            description = model_details.get('description', '')\n",
    "            \n",
    "            if provider not in providers:\n",
    "                providers[provider] = []\n",
    "            providers[provider].append({\n",
    "                'alias': alias,\n",
    "                'model_name': actual_model_name,\n",
    "                'description': description\n",
    "            })\n",
    "        \n",
    "        # Display by provider\n",
    "        for provider, provider_models in providers.items():\n",
    "            print(f\"üì° {provider.upper()} Provider:\")\n",
    "            for model in provider_models:\n",
    "                desc = f\" - {model['description']}\" if model['description'] else \"\"\n",
    "                print(f\"  ‚Ä¢ {model['alias']} ‚Üí {model['model_name']}{desc}\")\n",
    "            print()\n",
    "        \n",
    "        # Show current configuration\n",
    "        try:\n",
    "            # Use ruamel.yaml for parsing\n",
    "            yaml = YAML()\n",
    "            yaml.preserve_quotes = True\n",
    "            current_models_config = yaml.load(models_config_content)\n",
    "            current_preferred = current_models_config.get('primary_model', current_models_config.get('preferred', 'unknown'))\n",
    "            current_fallback = current_models_config.get('fallback', [])\n",
    "        except:\n",
    "            current_preferred = 'unknown'\n",
    "            current_fallback = []\n",
    "        \n",
    "        print(f\"üìã Current Models Configuration:\")\n",
    "        print(f\"  Preferred: {current_preferred}\")\n",
    "        print(f\"  Fallback: {current_fallback}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üí° Tips for editing:\")\n",
    "        print(\"  ‚Ä¢ Change 'primary_model' or 'preferred' to any alias from the list above\")\n",
    "        print(\"  ‚Ä¢ Add/remove aliases in the 'fallback' list\")\n",
    "        print(\"  ‚Ä¢ Aliases are case-sensitive\")\n",
    "        print(\"  ‚Ä¢ Invalid aliases will cause errors during processing\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading available models: {e}\")\n",
    "        print(\"Continuing with models configuration editor...\\n\")\n",
    "\n",
    "# Display available models before showing the editor\n",
    "display_available_models()\n",
    "\n",
    "models_config_editor = widgets.Textarea(\n",
    "    value=models_config_content,\n",
    "    description=\"\",\n",
    "    layout=widgets.Layout(width='100%', height='200px'),\n",
    "    style={'description_width': '0px'}\n",
    ")\n",
    "\n",
    "def save_models_config(button):\n",
    "    \"\"\"Save models config changes with comments preserved\"\"\"\n",
    "    try:\n",
    "        # Create YAML instance for validation\n",
    "        yaml = YAML()\n",
    "        yaml.preserve_quotes = True\n",
    "        yaml.default_flow_style = False\n",
    "        \n",
    "        # Validate YAML syntax\n",
    "        parsed_config = yaml.load(models_config_editor.value)\n",
    "        \n",
    "        # Additional validation for model aliases\n",
    "        if isinstance(parsed_config, dict):\n",
    "            preferred = parsed_config.get('primary_model') or parsed_config.get('preferred')\n",
    "            fallback = parsed_config.get('fallback', [])\n",
    "            \n",
    "            # Get available aliases\n",
    "            model_aliases = configs['shared_models'].get('model_aliases', {})\n",
    "            \n",
    "            # Check if preferred model exists\n",
    "            if preferred and preferred not in model_aliases:\n",
    "                print(f\"‚ö†Ô∏è Warning: Preferred model '{preferred}' not found in available model aliases\")\n",
    "            \n",
    "            # Check fallback models\n",
    "            for fb_model in fallback:\n",
    "                if fb_model not in model_aliases:\n",
    "                    print(f\"‚ö†Ô∏è Warning: Fallback model '{fb_model}' not found in available model aliases\")\n",
    "        \n",
    "        # Save to temp file (preserves comments in the editor content)\n",
    "        with open(temp_file_paths['models_config'], 'w') as f:\n",
    "            f.write(models_config_editor.value)\n",
    "        \n",
    "        print(\"‚úÖ Models config saved successfully with comments preserved!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå YAML syntax error in models config: {e}\")\n",
    "\n",
    "models_save_btn = widgets.Button(description=\"Save Models Config\", button_style='success')\n",
    "models_save_btn.on_click(save_models_config)\n",
    "\n",
    "display(models_config_editor)\n",
    "display(models_save_btn)\n",
    "\n",
    "# Save All button for convenience\n",
    "def save_all_configs(button):\n",
    "    \"\"\"Save all config changes at once\"\"\"\n",
    "    save_agent_config(None)\n",
    "    save_prompts_config(None)\n",
    "    save_models_config(None)\n",
    "\n",
    "print(\"\\nüíæ Save All Changes\")\n",
    "save_all_btn = widgets.Button(description=\"Save All Configs\", button_style='info')\n",
    "save_all_btn.on_click(save_all_configs)\n",
    "display(save_all_btn)\n",
    "\n",
    "print(f\"\\nüíæ Temp config files location:\")\n",
    "for config_type, path in temp_file_paths.items():\n",
    "    print(f\"  {config_type}: {path}\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"  ‚Ä¢ Edit YAML directly in the text areas above\")\n",
    "print(\"  ‚Ä¢ Use Save buttons to apply changes to temp files\")\n",
    "print(\"  ‚Ä¢ YAML syntax is validated before saving\")\n",
    "print(\"  ‚Ä¢ Model aliases are validated against available models\")\n",
    "print(\"  ‚Ä¢ Changes will be used in Step 5 when processing questions\")\n",
    "print(\"  ‚Ä¢ Original config files remain unchanged\")\n",
    "print(\"  ‚Ä¢ Comments and formatting are preserved during editing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Understanding Chatbot Internal Logic\n",
    "\n",
    "Now that you've seen the configuration settings, let's understand how the chatbot processes customer queries and how your configuration changes affect its behavior.\n",
    "\n",
    "### How the Chatbot Agent Works Internally\n",
    "\n",
    "The Chatbot Agent follows a sophisticated multi-stage process for each customer interaction:\n",
    "\n",
    "#### 1. **Initial Assessment Phase**\n",
    "- **Query Analysis**: The chatbot first analyzes the customer's question to understand:\n",
    "  - Intent and urgency level\n",
    "  - Technical complexity \n",
    "  - Emotional tone (frustration, confusion, etc.)\n",
    "  - Required information categories\n",
    "- **Context Integration**: Retrieves previous conversation history and relevant customer data\n",
    "- **Confidence Scoring**: Assigns a confidence score (0.0-1.0) based on how well it understands the query\n",
    "\n",
    "#### 2. **Response Generation Phase**\n",
    "- **System Prompt Application**: Uses the `system` prompt from `prompts.yaml` to establish personality and behavior\n",
    "- **Model Selection**: Chooses the appropriate AI model based on query complexity:\n",
    "  - Simple queries ‚Üí Uses `primary_model` from `models.yaml`\n",
    "  - Complex technical queries ‚Üí May escalate to premium models if configured\n",
    "  - Reasoning-heavy queries ‚Üí Uses models optimized for reasoning\n",
    "- **Response Crafting**: Generates response following the guidelines in `prompts.yaml`\n",
    "\n",
    "#### 3. **Quality Control Phase**\n",
    "- **Escalation Threshold Check**: Compares confidence score against thresholds in `config.yaml`:\n",
    "  - If confidence < `escalation.confidence_threshold` ‚Üí Triggers escalation\n",
    "  - If query complexity > agent capabilities ‚Üí Routes to human specialist\n",
    "- **Response Validation**: Ensures response meets quality standards\n",
    "- **Safety Checks**: Verifies no inappropriate or harmful content\n",
    "\n",
    "### How Configuration Settings Affect Behavior\n",
    "\n",
    "#### **Temperature Setting** (`config.yaml: settings.temperature`)\n",
    "- **Low (0.0-0.3)**: More deterministic, consistent responses\n",
    "  - *Best for*: Policy explanations, factual information\n",
    "  - *Trade-off*: May sound robotic or repetitive\n",
    "- **Medium (0.4-0.7)**: Balanced creativity and consistency  \n",
    "  - *Best for*: General customer service, explanations\n",
    "  - *Trade-off*: Good balance of personality and accuracy\n",
    "- **High (0.8-1.0)**: More creative, varied responses\n",
    "  - *Best for*: Empathetic responses, complex problem-solving\n",
    "  - *Trade-off*: May be less predictable or occasionally off-topic\n",
    "\n",
    "#### **System Prompt** (`prompts.yaml: system`)\n",
    "This is the chatbot's \"personality and instructions.\" Key components:\n",
    "- **Role Definition**: \"You are a professional customer service chatbot...\"\n",
    "- **Communication Style**: Friendly, helpful, professional tone guidelines\n",
    "- **Response Structure**: How to organize information in responses\n",
    "- **Escalation Guidance**: When and how to refer customers to human agents\n",
    "- **Service Standards**: Quality expectations and customer satisfaction goals\n",
    "\n",
    "*Example Impact*: Adding \"Be concise and direct\" to the system prompt will make responses shorter and more to-the-point.\n",
    "\n",
    "#### **Model Selection** (`models.yaml`)\n",
    "- **Primary Model**: The default model used for most queries\n",
    "  - `anthropic_general_standard`: Balanced performance, good for most queries\n",
    "  - `local_general_standard`: Faster local processing, may be less sophisticated\n",
    "  - `openai_general_standard`: High-quality responses, requires API access\n",
    "- **Fallback Models**: Used when primary model is unavailable\n",
    "- **Model Preferences by Query Type**: Different models for different scenarios\n",
    "  - `general_queries.primary`: Standard customer service questions\n",
    "  - `complex_queries.primary`: Technical or complicated issues  \n",
    "  - `escalation_queries.primary`: When preparing to transfer to human\n",
    "\n",
    "#### **Escalation Thresholds** (`config.yaml: escalation`)\n",
    "- **Confidence Threshold**: Minimum confidence to attempt answering\n",
    "  - Lower values ‚Üí More willing to try answering difficult questions\n",
    "  - Higher values ‚Üí More conservative, escalates uncertain queries faster\n",
    "- **Max Attempts**: How many clarification questions to ask before escalating\n",
    "- **Priority Routing**: Which human departments to route different query types\n",
    "\n",
    "#### **Behavior Settings** (`config.yaml: behavior`)\n",
    "- **Response Length Preferences**: Target length for different response types\n",
    "- **Follow-up Strategy**: How proactive to be in asking clarifying questions\n",
    "- **Empathy Level**: How much emotional intelligence to apply\n",
    "- **Technical Depth**: How detailed to get with technical explanations\n",
    "\n",
    "### Real-World Impact Examples\n",
    "\n",
    "#### Scenario: Customer asks \"My claim was denied, what do I do?\"\n",
    "\n",
    "**With Conservative Settings** (high escalation threshold, low temperature):\n",
    "- High confidence threshold ‚Üí May immediately escalate to human agent\n",
    "- Low temperature ‚Üí Gives standard, policy-based response\n",
    "- Result: Quick escalation, consistent but potentially impersonal\n",
    "\n",
    "**With Balanced Settings** (medium thresholds, medium temperature):\n",
    "- Medium confidence ‚Üí Asks clarifying questions first\n",
    "- Balanced temperature ‚Üí Provides empathetic but accurate response\n",
    "- Result: Attempts to help while showing understanding\n",
    "\n",
    "**With Aggressive Settings** (low escalation threshold, high temperature):\n",
    "- Low confidence threshold ‚Üí Attempts detailed explanation\n",
    "- High temperature ‚Üí Creative, personalized response approach\n",
    "- Result: More comprehensive help but potentially longer interaction\n",
    "\n",
    "### Configuration Optimization Tips\n",
    "\n",
    "1. **For High-Volume, Simple Queries**:\n",
    "   - Use faster models (`local_general_standard`)\n",
    "   - Lower temperature for consistency\n",
    "   - Higher escalation thresholds to reduce human load\n",
    "\n",
    "2. **For Complex Technical Support**:\n",
    "   - Use reasoning-optimized models (`anthropic_reasoning_premium`)\n",
    "   - Medium-high temperature for creative problem-solving\n",
    "   - Lower escalation thresholds to ensure accuracy\n",
    "\n",
    "3. **For Frustrated Customers**:\n",
    "   - Emphasize empathy in system prompts\n",
    "   - Medium temperature for personalized responses\n",
    "   - Faster escalation to human agents when emotions are high\n",
    "\n",
    "4. **For Cost Optimization**:\n",
    "   - Prefer local models over API-based models\n",
    "   - Use budget models for simple queries\n",
    "   - Set appropriate escalation thresholds to balance cost and quality\n",
    "\n",
    "### Monitoring and Adjustment\n",
    "\n",
    "The chatbot's performance can be monitored through:\n",
    "- **Confidence Scores**: Track how certain the chatbot is about responses\n",
    "- **Escalation Rates**: Monitor how often queries are passed to humans\n",
    "- **Customer Satisfaction**: Measure response quality and helpfulness\n",
    "- **Response Times**: Balance quality with speed requirements\n",
    "- **Conversation Length**: Optimize for efficiency while maintaining quality\n",
    "\n",
    "*Understanding these internals helps you make informed configuration changes that improve customer experience while managing operational costs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Questions\n",
    "\n",
    "Load test questions from a JSON file. Use question_generator.ipynb to create new questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Load Test Questions\n",
      "\n",
      "üí° How to get test questions:\n",
      "1. Use question_generator.ipynb to create test questions\n",
      "2. Upload the generated JSON file below\n",
      "3. Supported formats: Full export or questions-only JSON\n",
      "\n",
      "üìÑ Expected JSON format:\n",
      "- Full export: {'metadata': {...}, 'questions': [...]}\n",
      "- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\n",
      "\n",
      "üìÅ Upload your questions file:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1728adff4f349c89c433e3b835db65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.json', description='Upload questions file:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File upload widget for loading questions\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.json',\n",
    "    multiple=False,\n",
    "    description='Upload questions file:'\n",
    ")\n",
    "\n",
    "# Instructions for file format\n",
    "print(\"üìù Load Test Questions\")\n",
    "print(\"\\nüí° How to get test questions:\")\n",
    "print(\"1. Use question_generator.ipynb to create test questions\")\n",
    "print(\"2. Upload the generated JSON file below\")\n",
    "print(\"3. Supported formats: Full export or questions-only JSON\")\n",
    "print(\"\\nüìÑ Expected JSON format:\")\n",
    "print(\"- Full export: {'metadata': {...}, 'questions': [...]}\")\n",
    "print(\"- Questions only: [{'id': 1, 'question': '...', 'customer_type': '...', 'complexity': '...'}]\")\n",
    "print(\"\\nüìÅ Upload your questions file:\")\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Using tuple format\n",
      "üîç File content type: <class 'memoryview'>\n",
      "üìÅ Loading questions from: test_questions_20250718_1744.json\n",
      "üìÑ Loaded file with metadata:\n",
      "  Generation model: anthropic_general_standard\n",
      "  Generation timestamp: 20250718_1744\n",
      "  Question count: 20\n",
      "‚úÖ Loaded 20 test questions\n",
      "\n",
      "üìã Preview of loaded questions:\n",
      "  1. Why did my premium increase by $200? This is ridiculous - I haven't had any claims! [frustrated]\n",
      "  2. I just had a car accident. What do I need to do right now? [urgent]\n",
      "  3. Can someone explain what a deductible is? I keep seeing this word but don't understand. [confused]\n",
      "  ... and 17 more questions\n",
      "\n",
      "üìä Question Distribution:\n",
      "  Customer types: {'frustrated': np.int64(5), 'urgent': np.int64(5), 'confused': np.int64(5), 'normal': np.int64(5)}\n",
      "  Complexities: {'medium': np.int64(7), 'simple': np.int64(7), 'complex': np.int64(6)}\n"
     ]
    }
   ],
   "source": [
    "# Load test questions from uploaded file\n",
    "test_questions = []\n",
    "\n",
    "def load_questions_from_file(file_content, filename):\n",
    "    \"\"\"Load questions from uploaded JSON file\"\"\"\n",
    "    try:\n",
    "        # Handle different content types\n",
    "        if isinstance(file_content, memoryview):\n",
    "            # Convert memoryview to bytes\n",
    "            content_bytes = file_content.tobytes()\n",
    "        elif hasattr(file_content, 'decode'):\n",
    "            # Already bytes\n",
    "            content_bytes = file_content\n",
    "        else:\n",
    "            # Convert to bytes if it's a string or other type\n",
    "            content_bytes = str(file_content).encode('utf-8')\n",
    "        \n",
    "        # Decode to string and parse JSON\n",
    "        data = json.loads(content_bytes.decode('utf-8'))\n",
    "        \n",
    "        # Handle different JSON formats\n",
    "        if isinstance(data, dict):\n",
    "            # Full export format with metadata\n",
    "            if 'questions' in data:\n",
    "                questions = data['questions']\n",
    "                metadata = data.get('metadata', {})\n",
    "                print(f\"üìÑ Loaded file with metadata:\")\n",
    "                print(f\"  Generation model: {metadata.get('generation_model', 'unknown')}\")\n",
    "                print(f\"  Generation timestamp: {metadata.get('generation_timestamp', 'unknown')}\")\n",
    "                print(f\"  Question count: {metadata.get('question_count', len(questions))}\")\n",
    "                return questions\n",
    "            else:\n",
    "                # Single question object\n",
    "                return [data]\n",
    "        elif isinstance(data, list):\n",
    "            # Questions-only format\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"‚ùå Unexpected data format: {type(data)}\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        print(f\"   File content type: {type(file_content)}\")\n",
    "        return []\n",
    "\n",
    "def validate_questions(questions):\n",
    "    \"\"\"Validate and normalize question format\"\"\"\n",
    "    validated_questions = []\n",
    "    \n",
    "    for i, q in enumerate(questions):\n",
    "        if isinstance(q, dict):\n",
    "            # Ensure required fields exist\n",
    "            validated_q = {\n",
    "                \"id\": q.get(\"id\", i + 1),\n",
    "                \"question\": q.get(\"question\", f\"Question {i + 1}\"),\n",
    "                \"customer_type\": q.get(\"customer_type\", \"normal\"),\n",
    "                \"complexity\": q.get(\"complexity\", \"medium\")\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "        else:\n",
    "            # Convert string to dict if needed\n",
    "            validated_q = {\n",
    "                \"id\": i + 1,\n",
    "                \"question\": str(q),\n",
    "                \"customer_type\": \"normal\",\n",
    "                \"complexity\": \"medium\"\n",
    "            }\n",
    "            validated_questions.append(validated_q)\n",
    "    \n",
    "    return validated_questions\n",
    "\n",
    "# Process uploaded file\n",
    "if file_upload.value:\n",
    "    uploaded_file = None\n",
    "    filename = None\n",
    "    file_content = None\n",
    "    \n",
    "    # Handle different file upload widget formats\n",
    "    if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using tuple format\")\n",
    "        uploaded_file = file_upload.value[0]\n",
    "        filename = uploaded_file['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "        print(\"üìã Using dict format\")\n",
    "        uploaded_file = list(file_upload.value.values())[0]\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "        file_content = uploaded_file['content']\n",
    "        print(f\"üîç File content type: {type(file_content)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Unable to read uploaded file format\")\n",
    "        print(f\"   Type: {type(file_upload.value)}\")\n",
    "        print(f\"   Length: {len(file_upload.value) if hasattr(file_upload.value, '__len__') else 'No length'}\")\n",
    "        print(f\"   Content: {file_upload.value}\")\n",
    "    \n",
    "    if uploaded_file and filename and file_content is not None:\n",
    "        print(f\"üìÅ Loading questions from: {filename}\")\n",
    "        \n",
    "        raw_questions = load_questions_from_file(file_content, filename)\n",
    "        \n",
    "        if raw_questions:\n",
    "            test_questions = validate_questions(raw_questions)\n",
    "            print(f\"‚úÖ Loaded {len(test_questions)} test questions\")\n",
    "            \n",
    "            # Display first few questions as preview\n",
    "            print(\"\\nüìã Preview of loaded questions:\")\n",
    "            for i, q in enumerate(test_questions[:3]):\n",
    "                print(f\"  {i+1}. {q['question']} [{q['customer_type']}]\")\n",
    "            if len(test_questions) > 3:\n",
    "                print(f\"  ... and {len(test_questions) - 3} more questions\")\n",
    "                \n",
    "            # Show distribution\n",
    "            df_preview = pd.DataFrame(test_questions)\n",
    "            print(\"\\nüìä Question Distribution:\")\n",
    "            print(f\"  Customer types: {dict(df_preview['customer_type'].value_counts())}\")\n",
    "            print(f\"  Complexities: {dict(df_preview['complexity'].value_counts())}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå No questions loaded from file\")\n",
    "    else:\n",
    "        print(\"‚ùå Error accessing uploaded file\")\n",
    "        print(f\"   uploaded_file: {uploaded_file is not None}\")\n",
    "        print(f\"   filename: {filename}\")\n",
    "        print(f\"   file_content: {file_content is not None}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please upload a JSON file with test questions.\")\n",
    "    print(\"üí° Use question_generator.ipynb to create test questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Review and Edit Questions\n",
    "\n",
    "Review the loaded questions and make any edits before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Question Editor - You can modify questions before processing\n",
      "Edit the questions in the table below, then run the next cell to process them.\n",
      "\n",
      "Current questions (you can edit the JSON below if needed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Why did my premium increase by $200? This is r...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I just had a car accident. What do I need to d...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Can someone explain what a deductible is? I ke...</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How do I add my teenage daughter to my auto po...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I've been a customer for 15 years and you deni...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>What's the difference between comprehensive an...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>My house just flooded - I need emergency assis...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Can I get proof of insurance sent to my phone?...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How do I update my billing information?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>What exactly is an 'act of God' and why isn't ...</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>I've been on hold for 45 minutes and keep gett...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>My neighbor's tree fell on my garage during th...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Can I schedule a review of my current coverage...</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Someone broke into my car last night. I need t...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Why am I paying more than my friend for the sa...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>What documents do I need to submit for my wate...</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>I don't understand all these terms in my polic...</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>My windshield cracked on the highway. Is this ...</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>I've been trying to file a claim online for ho...</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>When does my policy expire and how do I renew it?</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           question customer_type  \\\n",
       "0    1  Why did my premium increase by $200? This is r...    frustrated   \n",
       "1    2  I just had a car accident. What do I need to d...        urgent   \n",
       "2    3  Can someone explain what a deductible is? I ke...      confused   \n",
       "3    4  How do I add my teenage daughter to my auto po...        normal   \n",
       "4    5  I've been a customer for 15 years and you deni...    frustrated   \n",
       "5    6  What's the difference between comprehensive an...      confused   \n",
       "6    7  My house just flooded - I need emergency assis...        urgent   \n",
       "7    8  Can I get proof of insurance sent to my phone?...        urgent   \n",
       "8    9            How do I update my billing information?        normal   \n",
       "9   10  What exactly is an 'act of God' and why isn't ...      confused   \n",
       "10  11  I've been on hold for 45 minutes and keep gett...    frustrated   \n",
       "11  12  My neighbor's tree fell on my garage during th...      confused   \n",
       "12  13  Can I schedule a review of my current coverage...        normal   \n",
       "13  14  Someone broke into my car last night. I need t...        urgent   \n",
       "14  15  Why am I paying more than my friend for the sa...    frustrated   \n",
       "15  16  What documents do I need to submit for my wate...        normal   \n",
       "16  17  I don't understand all these terms in my polic...      confused   \n",
       "17  18  My windshield cracked on the highway. Is this ...        urgent   \n",
       "18  19  I've been trying to file a claim online for ho...    frustrated   \n",
       "19  20  When does my policy expire and how do I renew it?        normal   \n",
       "\n",
       "   complexity  \n",
       "0      medium  \n",
       "1      simple  \n",
       "2      simple  \n",
       "3      simple  \n",
       "4     complex  \n",
       "5      medium  \n",
       "6     complex  \n",
       "7      simple  \n",
       "8      simple  \n",
       "9      medium  \n",
       "10     medium  \n",
       "11    complex  \n",
       "12     medium  \n",
       "13    complex  \n",
       "14     medium  \n",
       "15     simple  \n",
       "16    complex  \n",
       "17     medium  \n",
       "18    complex  \n",
       "19     simple  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced: Edit questions as JSON (optional):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568f3676efbb4e50ad535a5b78ff5769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='[\\n  {\\n    \"id\": 1,\\n    \"question\": \"Why did my premium increase by $200? This is ridiculous‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2741e0f72e8346c89e5924fe5ef3ffdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update from JSON', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create interactive question editor\n",
    "if test_questions:\n",
    "    print(\"üìù Question Editor - You can modify questions before processing\")\n",
    "    print(\"Edit the questions in the table below, then run the next cell to process them.\\n\")\n",
    "    \n",
    "    # Convert to DataFrame for easy editing\n",
    "    df = pd.DataFrame(test_questions)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'id' not in df.columns:\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "    if 'customer_type' not in df.columns:\n",
    "        df['customer_type'] = 'normal'\n",
    "    if 'complexity' not in df.columns:\n",
    "        df['complexity'] = 'medium'\n",
    "    \n",
    "    # Display editable table\n",
    "    print(\"Current questions (you can edit the JSON below if needed):\")\n",
    "    display(df)\n",
    "    \n",
    "    # Show JSON for manual editing if needed\n",
    "    questions_json = widgets.Textarea(\n",
    "        value=json.dumps(test_questions, indent=2),\n",
    "        description=\"Questions JSON:\",\n",
    "        layout=widgets.Layout(width='100%', height='200px')\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAdvanced: Edit questions as JSON (optional):\")\n",
    "    display(questions_json)\n",
    "    \n",
    "    def update_questions_from_json():\n",
    "        \"\"\"Update questions from the JSON editor\"\"\"\n",
    "        global test_questions\n",
    "        try:\n",
    "            test_questions = json.loads(questions_json.value)\n",
    "            print(\"‚úÖ Questions updated from JSON editor\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing JSON: {e}\")\n",
    "    \n",
    "    # Button to update from JSON\n",
    "    update_btn = widgets.Button(description=\"Update from JSON\")\n",
    "    update_btn.on_click(lambda b: update_questions_from_json())\n",
    "    display(update_btn)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please upload a questions file in the previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Process Questions Through Chatbot Agent\n",
    "\n",
    "Run the questions through the Chatbot Agent and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Processing 20 questions through Chatbot Agent...\n",
      "Using model: local_general_standard\n",
      "Temperature: 0.7\n",
      "\n",
      "==================================================\n",
      "‚úÖ 12:49:34.894 [INFO    ] factory         | Attempting to create provider: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 12:49:34.898 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 12:49:38.920 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "‚úÖ 12:49:38.921 [INFO    ] factory         | Successfully created provider: local_general_standard\n",
      "‚úÖ 12:49:38.922 [INFO    ] chatbot_agent   | Chatbot Agent LLM provider initialized | operation=initialize_llm_provider model_name=unknown\n",
      "‚úÖ Chatbot Agent initialized successfully\n",
      "\n",
      "üîÑ Processing question 1: Why did my premium increase by $200? This is ridiculous - I ...\n",
      "‚úÖ 12:49:59.322 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 2: I just had a car accident. What do I need to do right now?...\n",
      "‚úÖ 12:50:04.994 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 3: Can someone explain what a deductible is? I keep seeing this...\n",
      "‚úÖ 12:50:26.736 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 4: How do I add my teenage daughter to my auto policy?...\n",
      "‚úÖ 12:50:40.983 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 5: I've been a customer for 15 years and you denied my claim? I...\n",
      "‚úÖ 12:50:47.828 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 6: What's the difference between comprehensive and collision co...\n",
      "‚úÖ 12:51:07.384 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 7: My house just flooded - I need emergency assistance. Does my...\n",
      "‚úÖ 12:51:11.374 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 8: Can I get proof of insurance sent to my phone? I got pulled ...\n",
      "‚úÖ 12:51:26.618 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 9: How do I update my billing information?...\n",
      "‚úÖ 12:51:40.756 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 10: What exactly is an 'act of God' and why isn't my damage cove...\n",
      "‚úÖ 12:51:56.650 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 11: I've been on hold for 45 minutes and keep getting disconnect...\n",
      "‚úÖ 12:52:04.706 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 12: My neighbor's tree fell on my garage during the storm. Which...\n",
      "‚úÖ 12:52:21.739 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 13: Can I schedule a review of my current coverage? I think I mi...\n",
      "‚úÖ 12:52:33.218 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 14: Someone broke into my car last night. I need to file a claim...\n",
      "‚úÖ 12:52:55.767 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üîÑ Needs more input (information_request) (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 15: Why am I paying more than my friend for the same coverage? W...\n",
      "‚úÖ 12:53:08.838 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üîÑ Needs more input (information_request) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 16: What documents do I need to submit for my water damage claim...\n",
      "‚úÖ 12:53:13.077 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 17: I don't understand all these terms in my policy renewal - ca...\n",
      "‚úÖ 12:53:25.240 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 18: My windshield cracked on the highway. Is this covered and ho...\n",
      "‚úÖ 12:53:40.019 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "üîÑ Processing question 19: I've been trying to file a claim online for hours but keep g...\n",
      "‚úÖ 12:53:53.206 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üîÑ Needs more input (clarifying_questions) (confidence: 0.90)\n",
      "\n",
      "üîÑ Processing question 20: When does my policy expire and how do I renew it?...\n",
      "‚úÖ 12:54:05.953 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Complete (confidence: 0.85)\n",
      "\n",
      "==================================================\n",
      "‚úÖ Processing complete! Processed 20 questions.\n",
      "\n",
      "üìä Enhanced Summary:\n",
      "  Total questions: 20\n",
      "  Need more input: 5 (25.0%)\n",
      "    Reasons: {'clarifying_questions': 3, 'information_request': 2}\n",
      "  Need escalation: 0 (0.0%)\n",
      "  Average confidence: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Process questions through the Chatbot Agent\n",
    "if not test_questions:\n",
    "    print(\"‚ö†Ô∏è No questions to process. Please load or generate questions first.\")\n",
    "else:\n",
    "    print(f\"ü§ñ Processing {len(test_questions)} questions through Chatbot Agent...\")\n",
    "    \n",
    "    # Get model preference from current temp config\n",
    "    try:\n",
    "        from ruamel.yaml import YAML\n",
    "        yaml = YAML()\n",
    "        with open(temp_file_paths['models_config'], 'r') as f:\n",
    "            models_config = yaml.load(f)\n",
    "        agent_preferred_model = models_config.get('primary_model', models_config.get('preferred', 'unknown'))\n",
    "    except:\n",
    "        agent_preferred_model = 'unknown'\n",
    "    \n",
    "    # Get temperature from agent config\n",
    "    try:\n",
    "        with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "            agent_config = yaml.load(f)\n",
    "        agent_temperature = agent_config.get('settings', {}).get('temperature', 0.5)\n",
    "    except:\n",
    "        agent_temperature = 0.5\n",
    "    \n",
    "    print(f\"Using model: {agent_preferred_model}\")\n",
    "    print(f\"Temperature: {agent_temperature}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    def analyze_chatbot_response(response: str, confidence: float) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze chatbot response to determine escalation and input needs\n",
    "        \"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Escalation indicators - phrases that suggest referring to human/department\n",
    "        escalation_phrases = [\n",
    "            \"let me transfer you to\",\n",
    "            \"i'll connect you with\",\n",
    "            \"please contact our\",\n",
    "            \"speak with a specialist\",\n",
    "            \"escalate this to\",\n",
    "            \"connect you with the\",\n",
    "            \"transfer to our\",\n",
    "            \"speak with someone from\",\n",
    "            \"you'll need to contact\",\n",
    "            \"reach out to our\",\n",
    "            \"i recommend speaking with\",\n",
    "            \"forward this to\"\n",
    "        ]\n",
    "        \n",
    "        # More input indicators - phrases requesting additional information\n",
    "        input_request_phrases = [\n",
    "            \"could you provide\",\n",
    "            \"can you tell me more about\",\n",
    "            \"what is your\",\n",
    "            \"to better assist you\",\n",
    "            \"i need more information\",\n",
    "            \"could you clarify\",\n",
    "            \"can you specify\",\n",
    "            \"what type of\",\n",
    "            \"please let me know\",\n",
    "            \"can you share\",\n",
    "            \"what specific\",\n",
    "            \"which policy\"\n",
    "        ]\n",
    "        \n",
    "        # Question indicators (asking user for info to continue conversation)\n",
    "        question_patterns = [\n",
    "            \"could you\",\n",
    "            \"can you\",\n",
    "            \"would you mind\",\n",
    "            \"please provide\",\n",
    "            \"do you have\",\n",
    "            \"what is\",\n",
    "            \"which\",\n",
    "            \"when did\",\n",
    "            \"?\"\n",
    "        ]\n",
    "        \n",
    "        # Check for escalation intent\n",
    "        escalation_detected = any(phrase in response_lower for phrase in escalation_phrases)\n",
    "        \n",
    "        # Check for information requests\n",
    "        input_needed = any(phrase in response_lower for phrase in input_request_phrases)\n",
    "        \n",
    "        # Check if response is primarily asking questions to continue conversation\n",
    "        question_count = response.count('?')\n",
    "        is_asking_questions = question_count > 0 and any(pattern in response_lower for pattern in question_patterns)\n",
    "        \n",
    "        # Confidence-based escalation (very low confidence = likely needs human help)\n",
    "        low_confidence_escalation = confidence < 0.6\n",
    "        \n",
    "        # Final determination with priority logic\n",
    "        needs_escalation = escalation_detected or low_confidence_escalation\n",
    "        # Only flag needs_more_input if NOT escalating (escalation takes priority)\n",
    "        needs_more_input = (input_needed or is_asking_questions) and not needs_escalation\n",
    "        \n",
    "        return {\n",
    "            'needs_escalation': needs_escalation,\n",
    "            'needs_more_input': needs_more_input,\n",
    "            'escalation_reason': 'explicit_transfer' if escalation_detected else 'low_confidence' if low_confidence_escalation else None,\n",
    "            'input_reason': 'information_request' if input_needed else 'clarifying_questions' if is_asking_questions else None,\n",
    "            'question_count': question_count,\n",
    "            'confidence_score': confidence,\n",
    "            'analysis_details': {\n",
    "                'escalation_detected': escalation_detected,\n",
    "                'input_needed': input_needed,\n",
    "                'is_asking_questions': is_asking_questions,\n",
    "                'low_confidence': low_confidence_escalation\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create a simple mock context provider for testing\n",
    "    class MockContextProvider:\n",
    "        \"\"\"Simple mock context provider for testing\"\"\"\n",
    "        \n",
    "        def get_context_summary(self, user_id: str, session_id: str) -> dict:\n",
    "            \"\"\"Return a simple mock context summary\"\"\"\n",
    "            return {\n",
    "                'entries_count': 0,\n",
    "                'type_breakdown': {},\n",
    "                'recent_queries': [],\n",
    "                'escalation_count': 0,\n",
    "                'last_activity': None\n",
    "            }\n",
    "        \n",
    "        def save_context_entry(self, entry) -> bool:\n",
    "            \"\"\"Mock save method\"\"\"\n",
    "            return True\n",
    "        \n",
    "        def get_recent_context(self, user_id: str, session_id: str, limit: int = 10) -> list:\n",
    "            \"\"\"Mock recent context method\"\"\"\n",
    "            return []\n",
    "    \n",
    "    # Initialize the Chatbot Agent\n",
    "    try:\n",
    "        # Create config manager with the correct config directory\n",
    "        config_manager = AgentConfigManager(config_dir='/workspace/config')\n",
    "        \n",
    "        # Create a mock context provider for testing\n",
    "        context_provider = MockContextProvider()\n",
    "        \n",
    "        # Initialize Chatbot Agent\n",
    "        chatbot_agent = ChatbotAgentNode(config_manager, context_provider)\n",
    "        \n",
    "        print(\"‚úÖ Chatbot Agent initialized successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing Chatbot Agent: {e}\")\n",
    "        print(\"Continuing with mock responses for demonstration...\")\n",
    "        chatbot_agent = None\n",
    "    \n",
    "    # Process each question\n",
    "    results = []\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        question_id = question_data.get('id', i + 1)\n",
    "        question_text = question_data.get('question', '')\n",
    "        customer_type = question_data.get('customer_type', 'normal')\n",
    "        complexity = question_data.get('complexity', 'medium')\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing question {question_id}: {question_text[:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            if chatbot_agent:\n",
    "                # Create state for the Chatbot Agent\n",
    "                from datetime import datetime\n",
    "                state = {\n",
    "                    'query': question_text,\n",
    "                    'user_id': 'test_user',\n",
    "                    'session_id': f'test_session_{i}',\n",
    "                    'query_id': f'query_{question_id}',\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'messages': []\n",
    "                }\n",
    "                \n",
    "                # Process through agent using __call__ method\n",
    "                response_state = chatbot_agent(state)\n",
    "                \n",
    "                answer = response_state.get('ai_response', 'No response generated')\n",
    "                confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n",
    "                \n",
    "                # Use improved flag analysis\n",
    "                flag_analysis = analyze_chatbot_response(answer, confidence)\n",
    "                needs_escalation = flag_analysis['needs_escalation']\n",
    "                needs_more_input = flag_analysis['needs_more_input']\n",
    "                \n",
    "            else:\n",
    "                # Mock processing for demonstration\n",
    "                import random\n",
    "                answer = f\"Thank you for your question about {question_text[:30]}... I'd be happy to help you with that. [This is a mock response for demonstration]\"\n",
    "                confidence = random.uniform(0.6, 0.95)\n",
    "                \n",
    "                # Use improved flag analysis even for mock responses\n",
    "                flag_analysis = analyze_chatbot_response(answer, confidence)\n",
    "                needs_escalation = flag_analysis['needs_escalation']\n",
    "                needs_more_input = flag_analysis['needs_more_input']\n",
    "            \n",
    "            # Create result entry with enhanced analysis\n",
    "            result = {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'ai_answer': answer,\n",
    "                'confidence_score': confidence,\n",
    "                'needs_escalation': needs_escalation,\n",
    "                'needs_more_input': needs_more_input,\n",
    "                'escalation_reason': flag_analysis.get('escalation_reason'),\n",
    "                'input_reason': flag_analysis.get('input_reason'),\n",
    "                'question_count': flag_analysis.get('question_count', 0),\n",
    "                'analysis_details': flag_analysis.get('analysis_details', {}),\n",
    "                'processing_time': datetime.now().isoformat(),\n",
    "                'model_used': agent_preferred_model,\n",
    "                'temperature': agent_temperature\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Show progress with enhanced status\n",
    "            if needs_more_input:\n",
    "                reason = flag_analysis.get('input_reason', 'unknown')\n",
    "                status = f\"üîÑ Needs more input ({reason})\"\n",
    "            elif needs_escalation:\n",
    "                reason = flag_analysis.get('escalation_reason', 'unknown')\n",
    "                status = f\"‚ö†Ô∏è Escalation needed ({reason})\"\n",
    "            else:\n",
    "                status = \"‚úÖ Complete\"\n",
    "            \n",
    "            print(f\"   {status} (confidence: {confidence:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing question: {e}\")\n",
    "            # Add error result\n",
    "            result = {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'ai_answer': f\"Error processing question: {e}\",\n",
    "                'confidence_score': 0.0,\n",
    "                'needs_escalation': True,\n",
    "                'needs_more_input': False,\n",
    "                'escalation_reason': 'processing_error',\n",
    "                'input_reason': None,\n",
    "                'question_count': 0,\n",
    "                'analysis_details': {'error': True},\n",
    "                'processing_time': datetime.now().isoformat(),\n",
    "                'model_used': agent_preferred_model,\n",
    "                'temperature': agent_temperature,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"‚úÖ Processing complete! Processed {len(results)} questions.\")\n",
    "    \n",
    "    # Enhanced summary statistics\n",
    "    total_questions = len(results)\n",
    "    needs_more_input_count = sum(1 for r in results if r['needs_more_input'])\n",
    "    needs_escalation_count = sum(1 for r in results if r['needs_escalation'])\n",
    "    avg_confidence = sum(r['confidence_score'] for r in results) / total_questions if total_questions > 0 else 0\n",
    "    \n",
    "    # Breakdown by escalation reason\n",
    "    escalation_reasons = {}\n",
    "    input_reasons = {}\n",
    "    for r in results:\n",
    "        if r['needs_escalation'] and r['escalation_reason']:\n",
    "            escalation_reasons[r['escalation_reason']] = escalation_reasons.get(r['escalation_reason'], 0) + 1\n",
    "        if r['needs_more_input'] and r['input_reason']:\n",
    "            input_reasons[r['input_reason']] = input_reasons.get(r['input_reason'], 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Enhanced Summary:\")\n",
    "    print(f\"  Total questions: {total_questions}\")\n",
    "    print(f\"  Need more input: {needs_more_input_count} ({needs_more_input_count/total_questions*100:.1f}%)\")\n",
    "    if input_reasons:\n",
    "        print(f\"    Reasons: {input_reasons}\")\n",
    "    print(f\"  Need escalation: {needs_escalation_count} ({needs_escalation_count/total_questions*100:.1f}%)\")\n",
    "    if escalation_reasons:\n",
    "        print(f\"    Reasons: {escalation_reasons}\")\n",
    "    print(f\"  Average confidence: {avg_confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Complete Conversations with Customer AI\n",
    "\n",
    "Continue conversations between customer AI and chatbot AI until natural resolution or escalation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Starting full conversations for 20 questions...\n",
      "This will simulate realistic customer-chatbot interactions until resolution.\n",
      "\n",
      "üó£Ô∏è Starting full conversation for question 1\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:02:59.823 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:03:06.979 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:03:11.954 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:03:24.820 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:03:29.950 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:03:43.719 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 1: escalation_request (3 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 2\n",
      "   Customer: urgent, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:03:43.960 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:03:45.639 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:03:59.354 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:04:14.831 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:04:21.430 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:04:29.603 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:04:47.255 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 2: escalation_request (3 turns, satisfaction: 0.65)\n",
      "üó£Ô∏è Starting full conversation for question 3\n",
      "   Customer: confused, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:04:47.507 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:04:49.165 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:04:55.467 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:05:06.279 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:05:21.002 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:05:34.896 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:05:58.463 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 13:06:06.503 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 13:06:13.739 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 6: Processing...\n",
      "‚úÖ 13:06:25.091 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 7: Processing...\n",
      "‚úÖ 13:06:34.790 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 8: Processing...\n",
      "‚úÖ 13:06:36.948 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   üìã Conversation complete: conversation_incomplete in 8 turns\n",
      "Question 3: conversation_incomplete (8 turns, satisfaction: 0.50)\n",
      "üó£Ô∏è Starting full conversation for question 4\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:06:37.166 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:06:39.011 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:06:42.742 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:06:50.411 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:06:55.408 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:07:02.363 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:07:18.759 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 4: escalation_request (3 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 5\n",
      "   Customer: frustrated, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:07:18.984 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:07:20.543 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:07:26.892 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:07:39.598 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 1 turns\n",
      "Question 5: escalation_request (1 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 6\n",
      "   Customer: confused, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:07:39.824 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:07:41.313 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:07:59.750 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:08:16.201 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:08:40.179 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:08:54.672 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:09:08.562 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 3 turns\n",
      "Question 6: satisfied (3 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 7\n",
      "   Customer: urgent, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:09:08.784 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:09:10.399 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:09:16.123 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:09:21.281 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:09:32.285 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:09:36.678 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 13:09:41.556 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 13:09:48.939 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 6: Processing...\n",
      "‚úÖ 13:09:54.283 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 6 turns\n",
      "Question 7: escalation_request (6 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 8\n",
      "   Customer: urgent, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:09:54.530 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:09:55.951 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:10:09.677 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:10:26.462 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:10:45.658 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:10:58.454 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:11:06.618 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 8: escalation_request (3 turns, satisfaction: 0.65)\n",
      "üó£Ô∏è Starting full conversation for question 9\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:11:06.843 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:11:08.398 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:11:16.714 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:11:27.257 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:11:40.688 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:11:49.805 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:12:01.178 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 9: escalation_request (3 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 10\n",
      "   Customer: confused, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:12:01.412 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:12:03.141 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:12:16.473 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:12:32.106 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:12:49.464 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:13:02.285 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:13:08.231 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:13:17.688 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 13:13:34.251 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 13:13:42.056 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 5 turns\n",
      "Question 10: satisfied (5 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 11\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:13:42.288 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:13:43.802 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:13:57.946 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:14:11.119 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:14:15.328 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 11: escalation_request (2 turns, satisfaction: 0.30)\n",
      "üó£Ô∏è Starting full conversation for question 12\n",
      "   Customer: confused, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:14:15.563 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:14:17.039 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:14:31.426 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:14:40.910 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 2 turns\n",
      "Question 12: satisfied (2 turns, satisfaction: 0.75)\n",
      "üó£Ô∏è Starting full conversation for question 13\n",
      "   Customer: normal, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:14:41.151 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:14:42.615 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:14:47.723 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:14:57.818 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:15:04.487 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 2 turns\n",
      "Question 13: satisfied (2 turns, satisfaction: 0.85)\n",
      "üó£Ô∏è Starting full conversation for question 14\n",
      "   Customer: urgent, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:15:04.723 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:15:06.229 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:15:11.567 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:15:21.259 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:15:39.205 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:15:52.049 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:16:11.617 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 14: escalation_request (3 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 15\n",
      "   Customer: frustrated, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:16:11.857 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:16:13.459 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:16:24.787 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:16:41.398 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:16:55.632 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:17:11.035 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 15: escalation_request (3 turns, satisfaction: 0.55)\n",
      "üó£Ô∏è Starting full conversation for question 16\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:17:11.262 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:17:12.823 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:17:30.595 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:17:45.213 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 1 turns\n",
      "Question 16: satisfied (1 turns, satisfaction: 0.70)\n",
      "üó£Ô∏è Starting full conversation for question 17\n",
      "   Customer: confused, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:17:45.446 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:17:46.944 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:17:59.184 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:18:11.150 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:18:24.263 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:18:29.025 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:18:44.183 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 4: Processing...\n",
      "‚úÖ 13:19:04.836 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 5: Processing...\n",
      "‚úÖ 13:19:08.273 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 6: Processing...\n",
      "‚úÖ 13:19:13.662 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 7: Processing...\n",
      "‚úÖ 13:19:23.716 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚úÖ Customer satisfied!\n",
      "   üìã Conversation complete: satisfied in 7 turns\n",
      "Question 17: satisfied (7 turns, satisfaction: 0.80)\n",
      "üó£Ô∏è Starting full conversation for question 18\n",
      "   Customer: urgent, Complexity: medium, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:19:23.951 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:19:25.474 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:19:41.848 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:19:54.901 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:19:59.859 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:20:08.024 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:20:13.345 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 18: escalation_request (3 turns, satisfaction: 0.40)\n",
      "üó£Ô∏è Starting full conversation for question 19\n",
      "   Customer: frustrated, Complexity: complex, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:20:13.576 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:20:14.991 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:20:27.218 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:20:39.175 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:20:44.870 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 2 turns\n",
      "Question 19: escalation_request (2 turns, satisfaction: 0.30)\n",
      "üó£Ô∏è Starting full conversation for question 20\n",
      "   Customer: normal, Complexity: simple, Model: local_general_standard\n",
      "‚úÖ Creating LLM provider: local_general_standard ‚Üí llama-7b (llama)\n",
      "‚úÖ 13:20:45.128 [INFO    ] unknown         | Initializing LLM provider | model_name=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 13:20:46.663 [INFO    ] unknown         | LLM provider initialized successfully | model_name=unknown\n",
      "   Turn 1: Processing...\n",
      "‚úÖ 13:20:50.138 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:20:58.678 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 2: Processing...\n",
      "‚úÖ 13:21:11.581 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "‚úÖ 13:21:21.765 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   Turn 3: Processing...\n",
      "‚úÖ 13:21:37.639 [INFO    ] unknown         | Model call: unknown - generate_response | model_name=unknown operation=generate_response\n",
      "   ‚¨ÜÔ∏è Customer requesting escalation\n",
      "   üìã Conversation complete: escalation_request in 3 turns\n",
      "Question 20: escalation_request (3 turns, satisfaction: 0.60)\n",
      "\n",
      "‚úÖ All conversations completed!\n",
      "\n",
      "üìä Conversation Summary:\n",
      "  Completed conversations: 19/20\n",
      "  Customer satisfaction: 6 (30.0%)\n",
      "  Escalations: 13 (65.0%)\n",
      "  Average turns per conversation: 3.1\n",
      "  Average customer satisfaction: 0.61\n"
     ]
    }
   ],
   "source": [
    "# Complete conversations with customer AI simulation\n",
    "if 'results' in locals() and results:\n",
    "    import random  # Add missing import\n",
    "    \n",
    "    def create_customer_ai_simulator(customer_type, complexity, model_name='anthropic_general_budget'):\n",
    "        \"\"\"Create a customer AI simulator based on customer profile\"\"\"\n",
    "        \n",
    "        # Define customer personas\n",
    "        customer_personas = {\n",
    "            'frustrated': {\n",
    "                'style': 'Impatient, demanding, may use caps or exclamation points. Wants quick resolution.',\n",
    "                'follow_up_tendency': 'high',\n",
    "                'satisfaction_threshold': 0.8,\n",
    "                'escalation_patience': 2  # Will demand escalation after 2 responses if not satisfied\n",
    "            },\n",
    "            'urgent': {\n",
    "                'style': 'Time-sensitive, focused on immediate action. Professional but hurried.',\n",
    "                'follow_up_tendency': 'high', \n",
    "                'satisfaction_threshold': 0.75,\n",
    "                'escalation_patience': 3\n",
    "            },\n",
    "            'confused': {\n",
    "                'style': 'Asks many clarifying questions, needs simple explanations. Polite but persistent.',\n",
    "                'follow_up_tendency': 'very_high',\n",
    "                'satisfaction_threshold': 0.7,\n",
    "                'escalation_patience': 4\n",
    "            },\n",
    "            'normal': {\n",
    "                'style': 'Professional, patient, reasonable expectations.',\n",
    "                'follow_up_tendency': 'medium',\n",
    "                'satisfaction_threshold': 0.65,\n",
    "                'escalation_patience': 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Define complexity behaviors\n",
    "        complexity_behaviors = {\n",
    "            'simple': {\n",
    "                'satisfaction_boost': 0.1,  # Easier to satisfy\n",
    "                'max_turns': 3,\n",
    "                'question_depth': 'surface-level'\n",
    "            },\n",
    "            'medium': {\n",
    "                'satisfaction_boost': 0.0,\n",
    "                'max_turns': 5,\n",
    "                'question_depth': 'moderate detail'\n",
    "            },\n",
    "            'complex': {\n",
    "                'satisfaction_boost': -0.1,  # Harder to satisfy\n",
    "                'max_turns': 7,\n",
    "                'question_depth': 'detailed technical'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        persona = customer_personas.get(customer_type, customer_personas['normal'])\n",
    "        behavior = complexity_behaviors.get(complexity, complexity_behaviors['medium'])\n",
    "        \n",
    "        class CustomerAISimulator:\n",
    "            def __init__(self, customer_type, complexity):\n",
    "                self.customer_type = customer_type  # Store customer type as instance variable\n",
    "                self.complexity = complexity  # Store complexity as instance variable\n",
    "                self.persona = persona\n",
    "                self.behavior = behavior\n",
    "                self.turn_count = 0\n",
    "                self.satisfaction_level = 0.0\n",
    "                self.conversation_history = []\n",
    "                \n",
    "                # Create LLM for customer simulation\n",
    "                try:\n",
    "                    provider_factory = LLMProviderFactory()\n",
    "                    self.llm = provider_factory.create_provider(model_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not create customer AI simulator LLM: {e}\")\n",
    "                    self.llm = None\n",
    "            \n",
    "            def generate_response(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate customer response to chatbot\"\"\"\n",
    "                self.turn_count += 1\n",
    "                \n",
    "                # Analyze satisfaction with chatbot response\n",
    "                satisfaction_score = self._analyze_satisfaction(chatbot_response)\n",
    "                self.satisfaction_level = satisfaction_score\n",
    "                \n",
    "                # Check if conversation should end\n",
    "                is_satisfied = satisfaction_score >= (self.persona['satisfaction_threshold'] + self.behavior['satisfaction_boost'])\n",
    "                is_escalating = self.turn_count >= self.persona['escalation_patience'] and satisfaction_score < 0.6\n",
    "                is_max_turns = self.turn_count >= self.behavior['max_turns']\n",
    "                \n",
    "                if is_satisfied:\n",
    "                    return self._generate_satisfaction_response()\n",
    "                elif is_escalating:\n",
    "                    return self._generate_escalation_request()\n",
    "                elif is_max_turns:\n",
    "                    return self._generate_final_response()\n",
    "                else:\n",
    "                    return self._generate_follow_up_question(chatbot_response, original_question)\n",
    "            \n",
    "            def _analyze_satisfaction(self, response):\n",
    "                \"\"\"Analyze how satisfied the customer would be with the response\"\"\"\n",
    "                response_lower = response.lower()\n",
    "                \n",
    "                # Positive indicators\n",
    "                positive_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['specifically', \"here's how\", 'i can help', 'let me explain']):\n",
    "                    positive_score += 0.2\n",
    "                if any(phrase in response_lower for phrase in ['immediately', 'right away', 'quickly']):\n",
    "                    positive_score += 0.15\n",
    "                if len(response) > 100:  # Detailed response\n",
    "                    positive_score += 0.1\n",
    "                \n",
    "                # Negative indicators  \n",
    "                negative_score = 0\n",
    "                if any(phrase in response_lower for phrase in ['i need more information', 'could you provide', 'what type']):\n",
    "                    negative_score += 0.3  # Asking for more info is frustrating\n",
    "                if any(phrase in response_lower for phrase in ['unfortunately', 'however', 'but']):\n",
    "                    negative_score += 0.1\n",
    "                if response.count('?') > 2:  # Too many questions back\n",
    "                    negative_score += 0.2\n",
    "                \n",
    "                # Base satisfaction varies by customer type - use self.customer_type\n",
    "                base_satisfaction = {\n",
    "                    'frustrated': 0.3,\n",
    "                    'urgent': 0.4, \n",
    "                    'confused': 0.5,\n",
    "                    'normal': 0.6\n",
    "                }.get(self.customer_type, 0.5)\n",
    "                \n",
    "                return max(0.0, min(1.0, base_satisfaction + positive_score - negative_score))\n",
    "            \n",
    "            def _generate_satisfaction_response(self):\n",
    "                \"\"\"Generate a satisfied customer response\"\"\"\n",
    "                satisfied_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"Okay, that makes sense. Thanks for clearing that up.\",\n",
    "                        \"Finally! Thank you for the explanation.\",\n",
    "                        \"Alright, I understand now. That helps.\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"Perfect, that's exactly what I needed to know. Thank you!\",\n",
    "                        \"Great, I'll do that right away. Thanks for the quick help!\",\n",
    "                        \"Excellent, that answers my question. Much appreciated.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"Oh I see! That makes much more sense now. Thank you for explaining it so clearly.\",\n",
    "                        \"Thank you! That explanation really helped me understand.\",\n",
    "                        \"Perfect! Now I get it. I really appreciate your patience.\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"Thank you for the helpful information. That resolves my question.\",\n",
    "                        \"Great, that's exactly what I needed to know. Thanks!\",\n",
    "                        \"Perfect, I understand now. Thank you for your assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(satisfied_responses.get(self.customer_type, satisfied_responses['normal']))\n",
    "            \n",
    "            def _generate_escalation_request(self):\n",
    "                \"\"\"Generate escalation request\"\"\"\n",
    "                escalation_responses = {\n",
    "                    'frustrated': [\n",
    "                        \"This isn't working. I need to speak to someone who can actually help me!\",\n",
    "                        \"I'm getting nowhere with this. Transfer me to a supervisor NOW!\",\n",
    "                        \"Enough! Get me a human who knows what they're doing!\"\n",
    "                    ],\n",
    "                    'urgent': [\n",
    "                        \"I need this resolved immediately. Can you transfer me to someone who can handle this urgently?\",\n",
    "                        \"Time is critical here. I need to speak with a specialist right away.\",\n",
    "                        \"This is urgent - please connect me with someone who can resolve this now.\"\n",
    "                    ],\n",
    "                    'confused': [\n",
    "                        \"I'm still really confused. Could you please connect me with someone who can walk me through this step by step?\",\n",
    "                        \"I don't think I'm understanding this correctly. Can I speak with someone who can explain this more simply?\",\n",
    "                        \"I'm getting more confused. Could you transfer me to someone who specializes in helping customers like me?\"\n",
    "                    ],\n",
    "                    'normal': [\n",
    "                        \"I think I need to speak with a specialist about this. Could you please transfer me?\",\n",
    "                        \"This seems like it might require human expertise. Can you connect me with the right department?\",\n",
    "                        \"I'd like to speak with someone who can provide more detailed assistance.\"\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                return random.choice(escalation_responses.get(self.customer_type, escalation_responses['normal']))\n",
    "            \n",
    "            def _generate_final_response(self):\n",
    "                \"\"\"Generate final response when max turns reached\"\"\"\n",
    "                final_responses = {\n",
    "                    'frustrated': \"Look, I've been going in circles here. Just transfer me to someone else.\",\n",
    "                    'urgent': \"I've spent too much time on this already. I need to speak with a human.\",\n",
    "                    'confused': \"I'm still not clear on this. I think I need to talk to someone in person.\",\n",
    "                    'normal': \"I think this might be beyond what we can resolve here. Could you transfer me to the appropriate department?\"\n",
    "                }\n",
    "                \n",
    "                return final_responses.get(self.customer_type, final_responses['normal'])\n",
    "            \n",
    "            def _generate_follow_up_question(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate intelligent follow-up question using AI if available\"\"\"\n",
    "                \n",
    "                if self.llm:\n",
    "                    # Use AI to generate contextual follow-up\n",
    "                    prompt = f'''You are a {self.customer_type} customer with a {self.complexity} question about insurance. \n",
    "                    \n",
    "Your personality: {self.persona['style']}\n",
    "Question complexity: {self.behavior['question_depth']}\n",
    "Turn {self.turn_count} of conversation.\n",
    "\n",
    "Original question: {original_question}\n",
    "Chatbot's response: {chatbot_response}\n",
    "\n",
    "Generate a follow-up response that a {self.customer_type} customer would realistically ask. Be specific to the chatbot's response and maintain the personality. Make it {self.behavior['question_depth']} in nature.\n",
    "\n",
    "Respond as the customer would (2-3 sentences max):'''\n",
    "                    \n",
    "                    try:\n",
    "                        response = self.llm.generate_response(prompt)\n",
    "                        \n",
    "                        if response and len(response.strip()) > 10:\n",
    "                            return response.strip()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: AI follow-up generation failed: {e}\")\n",
    "                \n",
    "                # Fallback to template-based responses\n",
    "                return self._generate_template_follow_up(chatbot_response, original_question)\n",
    "            \n",
    "            def _generate_template_follow_up(self, chatbot_response, original_question):\n",
    "                \"\"\"Generate template-based follow-up questions\"\"\"\n",
    "                \n",
    "                # Analyze what the chatbot asked for\n",
    "                response_lower = chatbot_response.lower()\n",
    "                \n",
    "                if 'policy number' in response_lower:\n",
    "                    return f\"My policy number is POL-{random.randint(100000, 999999)}. Now what?\"\n",
    "                elif 'claim number' in response_lower:\n",
    "                    return f\"It's claim #{random.randint(10000, 99999)}. What's the next step?\"\n",
    "                elif 'type of coverage' in response_lower or 'which coverage' in response_lower:\n",
    "                    coverage_types = ['comprehensive', 'collision', 'liability', 'homeowners', 'auto']\n",
    "                    return f\"I have {random.choice(coverage_types)} coverage. Does that help?\"\n",
    "                elif 'when did' in response_lower or 'what date' in response_lower:\n",
    "                    return \"This happened yesterday around 3 PM. What do I do now?\"\n",
    "                elif 'how much' in response_lower or 'what amount' in response_lower:\n",
    "                    return f\"It's about ${random.randint(500, 5000)} in damage. What's next?\"\n",
    "                elif 'documents' in response_lower or 'paperwork' in response_lower:\n",
    "                    return \"I have photos and a police report. How do I submit them?\"\n",
    "                else:\n",
    "                    # Generic follow-ups based on customer type\n",
    "                    generic_followups = {\n",
    "                        'frustrated': \"That doesn't really answer my question. Can you be more specific?\",\n",
    "                        'urgent': \"Okay, but what do I do RIGHT NOW? This is time-sensitive!\",\n",
    "                        'confused': \"I'm still not sure I understand. Can you explain it differently?\",\n",
    "                        'normal': \"Could you provide more specific steps on what I should do next?\"\n",
    "                    }\n",
    "                    return generic_followups.get(self.customer_type, generic_followups['normal'])\n",
    "        \n",
    "        return CustomerAISimulator(customer_type, complexity)\n",
    "    \n",
    "    def conduct_full_conversation(question_data, max_conversation_turns=8):\n",
    "        \"\"\"Conduct a full conversation between customer AI and chatbot until resolution\"\"\"\n",
    "        \n",
    "        question_id = question_data.get('id', 1)\n",
    "        question_text = question_data.get('question', '')\n",
    "        customer_type = question_data.get('customer_type', 'normal')\n",
    "        complexity = question_data.get('complexity', 'medium')\n",
    "        \n",
    "        # Get the model from questions file or use default\n",
    "        model_to_use = question_data.get('model', agent_preferred_model)\n",
    "        \n",
    "        print(f\"üó£Ô∏è Starting full conversation for question {question_id}\")\n",
    "        print(f\"   Customer: {customer_type}, Complexity: {complexity}, Model: {model_to_use}\")\n",
    "        \n",
    "        # Create customer AI simulator\n",
    "        try:\n",
    "            customer_ai = create_customer_ai_simulator(customer_type, complexity, model_to_use)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error creating customer AI: {e}\")\n",
    "            return {\n",
    "                'id': question_id,\n",
    "                'original_question': question_text,\n",
    "                'customer_type': customer_type,\n",
    "                'complexity': complexity,\n",
    "                'error': f\"Failed to create customer AI: {e}\",\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "        \n",
    "        # Initialize conversation\n",
    "        conversation_history = []\n",
    "        current_query = question_text\n",
    "        turn_number = 1\n",
    "        \n",
    "        # Conversation loop\n",
    "        while turn_number <= max_conversation_turns:\n",
    "            print(f\"   Turn {turn_number}: Processing...\")\n",
    "            \n",
    "            try:\n",
    "                if chatbot_agent:\n",
    "                    # Create state for chatbot\n",
    "                    state = {\n",
    "                        'query': current_query,\n",
    "                        'user_id': 'test_user',\n",
    "                        'session_id': f'conversation_session_{question_id}',\n",
    "                        'query_id': f'query_{question_id}_turn_{turn_number}',\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'messages': []  # Start with empty messages for each turn\n",
    "                    }\n",
    "                    \n",
    "                    # Get chatbot response\n",
    "                    response_state = chatbot_agent(state)\n",
    "                    chatbot_response = response_state.get('ai_response', 'No response generated')\n",
    "                    confidence = response_state.get('initial_assessment', {}).get('confidence', 0.8)\n",
    "                    \n",
    "                else:\n",
    "                    # Mock chatbot response\n",
    "                    chatbot_response = f\"Thank you for your question. Let me help you with that... [Mock response turn {turn_number}]\"\n",
    "                    confidence = 0.8\n",
    "                \n",
    "                # Add to conversation history\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'chatbot_response': chatbot_response,\n",
    "                    'confidence': confidence,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                \n",
    "                # Analyze chatbot response for escalation\n",
    "                response_analysis = analyze_chatbot_response(chatbot_response, confidence)\n",
    "                \n",
    "                # Check if chatbot is escalating\n",
    "                if response_analysis['needs_escalation']:\n",
    "                    print(f\"   üîÑ Chatbot escalating: {response_analysis['escalation_reason']}\")\n",
    "                    conversation_turn['chatbot_action'] = 'escalation'\n",
    "                    conversation_turn['escalation_reason'] = response_analysis['escalation_reason']\n",
    "                    break\n",
    "                \n",
    "                # Generate customer response using AI simulator\n",
    "                try:\n",
    "                    customer_response = customer_ai.generate_response(chatbot_response, question_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error generating customer response: {e}\")\n",
    "                    customer_response = \"I'm having trouble understanding this. Can you help me differently?\"\n",
    "                \n",
    "                # Add customer response to turn\n",
    "                conversation_turn['customer_response'] = customer_response\n",
    "                conversation_turn['customer_satisfaction'] = customer_ai.satisfaction_level\n",
    "                \n",
    "                # Check if customer is satisfied (conversation ends)\n",
    "                if any(phrase in customer_response.lower() for phrase in ['thank you', 'that helps', 'perfect', 'great', 'excellent', 'makes sense']):\n",
    "                    if customer_ai.satisfaction_level >= customer_ai.persona['satisfaction_threshold']:\n",
    "                        print(f\"   ‚úÖ Customer satisfied!\")\n",
    "                        conversation_turn['customer_action'] = 'satisfied'\n",
    "                        break\n",
    "                \n",
    "                # Check if customer is requesting escalation\n",
    "                if any(phrase in customer_response.lower() for phrase in ['transfer', 'supervisor', 'specialist', 'human', 'someone else']):\n",
    "                    print(f\"   ‚¨ÜÔ∏è Customer requesting escalation\")\n",
    "                    conversation_turn['customer_action'] = 'escalation_request'\n",
    "                    break\n",
    "                \n",
    "                # Prepare for next turn\n",
    "                current_query = customer_response\n",
    "                turn_number += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error in turn {turn_number}: {e}\")\n",
    "                conversation_turn = {\n",
    "                    'turn_number': turn_number,\n",
    "                    'customer_query': current_query,\n",
    "                    'error': str(e),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                conversation_history.append(conversation_turn)\n",
    "                break\n",
    "        \n",
    "        # Determine final outcome\n",
    "        final_turn = conversation_history[-1] if conversation_history else {}\n",
    "        final_outcome = final_turn.get('customer_action', final_turn.get('chatbot_action', 'conversation_incomplete'))\n",
    "        \n",
    "        conversation_result = {\n",
    "            'id': question_id,\n",
    "            'original_question': question_text,\n",
    "            'customer_type': customer_type,\n",
    "            'complexity': complexity,\n",
    "            'model_used': model_to_use,\n",
    "            'conversation_history': conversation_history,\n",
    "            'total_turns': len(conversation_history),\n",
    "            'final_outcome': final_outcome,\n",
    "            'customer_satisfaction': customer_ai.satisfaction_level if customer_ai else 0.0,\n",
    "            'processing_time': datetime.now().isoformat(),\n",
    "            'conversation_complete': final_outcome in ['satisfied', 'escalation', 'escalation_request']\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìã Conversation complete: {final_outcome} in {len(conversation_history)} turns\")\n",
    "        return conversation_result\n",
    "    \n",
    "    # Process conversations for all questions\n",
    "    print(f\"ü§ñ Starting full conversations for {len(test_questions)} questions...\")\n",
    "    print(f\"This will simulate realistic customer-chatbot interactions until resolution.\\n\")\n",
    "    \n",
    "    conversation_results = []\n",
    "    \n",
    "    for i, question_data in enumerate(test_questions):\n",
    "        try:\n",
    "            conversation_result = conduct_full_conversation(question_data)\n",
    "            conversation_results.append(conversation_result)\n",
    "            \n",
    "            # Show progress\n",
    "            outcome = conversation_result['final_outcome']\n",
    "            turns = conversation_result['total_turns']\n",
    "            satisfaction = conversation_result.get('customer_satisfaction', 0)\n",
    "            \n",
    "            print(f\"Question {question_data['id']}: {outcome} ({turns} turns, satisfaction: {satisfaction:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing conversation {question_data['id']}: {e}\")\n",
    "            # Add error result\n",
    "            error_result = {\n",
    "                'id': question_data['id'],\n",
    "                'original_question': question_data.get('question', ''),\n",
    "                'customer_type': question_data.get('customer_type', 'normal'),\n",
    "                'complexity': question_data.get('complexity', 'medium'),\n",
    "                'error': str(e),\n",
    "                'conversation_complete': False,\n",
    "                'final_outcome': 'error'\n",
    "            }\n",
    "            conversation_results.append(error_result)\n",
    "    \n",
    "    print(f\"\\n‚úÖ All conversations completed!\")\n",
    "    \n",
    "    # Update results with conversation data\n",
    "    results = conversation_results\n",
    "    \n",
    "    # Summary statistics\n",
    "    completed_conversations = [r for r in results if r.get('conversation_complete', False)]\n",
    "    satisfied_customers = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    avg_turns = sum(r.get('total_turns', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    avg_satisfaction = sum(r.get('customer_satisfaction', 0) for r in completed_conversations) / len(completed_conversations) if completed_conversations else 0\n",
    "    \n",
    "    print(f\"\\nüìä Conversation Summary:\")\n",
    "    print(f\"  Completed conversations: {len(completed_conversations)}/{len(results)}\")\n",
    "    print(f\"  Customer satisfaction: {len(satisfied_customers)} ({len(satisfied_customers)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Escalations: {len(escalated_conversations)} ({len(escalated_conversations)/len(results)*100:.1f}%)\")\n",
    "    print(f\"  Average turns per conversation: {avg_turns:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {avg_satisfaction:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No questions loaded. Please load questions first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Export Results\n",
    "\n",
    "Export the results of the chatbot tester to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Conversation results exported to: /workspace/notebooks/agent_testers/outputs/chatbot_interactions/20250723_1330/chatbot_agent_output.json\n",
      "‚öôÔ∏è Settings exported to: /workspace/notebooks/agent_testers/outputs/chatbot_interactions/20250723_1330/chatbot_agent_settings_20250723_1330.json\n",
      "\\nüìä Export Summary:\n",
      "  Timestamp: 20250723_1330\n",
      "  Results file: chatbot_agent_output.json\n",
      "  Settings file: chatbot_agent_settings_20250723_1330.json\n",
      "  Total conversations: 20\n",
      "  Completed conversations: 19\n",
      "  Satisfaction rate: 30.0%\n",
      "  Escalation rate: 65.0%\n",
      "  Average turns per conversation: 3.3\n",
      "  Average customer satisfaction: 0.61\n",
      "  Files saved to: /workspace/notebooks/agent_testers/outputs/chatbot_interactions/20250723_1330\n"
     ]
    }
   ],
   "source": [
    "# Export conversation results and settings\n",
    "if 'results' in locals() and results:\n",
    "    # Create timestamp for filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    \n",
    "    # Create experiment_runs directory if it doesn't exist\n",
    "    experiment_dir = Path(f'/workspace/notebooks/agent_testers/outputs/chatbot_interactions/{timestamp}')\n",
    "    experiment_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export results with full conversation data\n",
    "    results_filename = f\"chatbot_agent_output.json\"\n",
    "    results_path = experiment_dir / results_filename\n",
    "    \n",
    "    # Create comprehensive export with conversation analysis\n",
    "    export_data = {\n",
    "        'experiment_metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'agent_type': 'chatbot_agent',\n",
    "            'experiment_type': 'full_conversation_simulation',\n",
    "            'total_questions': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'generation_model': agent_preferred_model,\n",
    "            'generation_timestamp': timestamp\n",
    "        },\n",
    "        'conversation_results': results,\n",
    "        'summary_statistics': {\n",
    "            'total_conversations': len(results),\n",
    "            'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "            'satisfied_customers': len([r for r in results if r.get('final_outcome') == 'satisfied']),\n",
    "            'escalated_conversations': len([r for r in results if 'escalation' in r.get('final_outcome', '')]),\n",
    "            'error_conversations': len([r for r in results if r.get('final_outcome') == 'error']),\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in results) / len(results) if results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in results) / len(results) if results else 0,\n",
    "            'satisfaction_rate': len([r for r in results if r.get('final_outcome') == 'satisfied']) / len(results) * 100 if results else 0,\n",
    "            'escalation_rate': len([r for r in results if 'escalation' in r.get('final_outcome', '')]) / len(results) * 100 if results else 0\n",
    "        },\n",
    "        'conversation_analysis': {\n",
    "            'by_customer_type': {},\n",
    "            'by_complexity': {},\n",
    "            'by_outcome': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Analyze by customer type\n",
    "    customer_types = list(set(r.get('customer_type', 'unknown') for r in results))\n",
    "    for ctype in customer_types:\n",
    "        ctype_results = [r for r in results if r.get('customer_type') == ctype]\n",
    "        export_data['conversation_analysis']['by_customer_type'][ctype] = {\n",
    "            'count': len(ctype_results),\n",
    "            'satisfaction_rate': len([r for r in ctype_results if r.get('final_outcome') == 'satisfied']) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'escalation_rate': len([r for r in ctype_results if 'escalation' in r.get('final_outcome', '')]) / len(ctype_results) * 100 if ctype_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in ctype_results) / len(ctype_results) if ctype_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by complexity\n",
    "    complexities = list(set(r.get('complexity', 'unknown') for r in results))\n",
    "    for complexity in complexities:\n",
    "        complexity_results = [r for r in results if r.get('complexity') == complexity]\n",
    "        export_data['conversation_analysis']['by_complexity'][complexity] = {\n",
    "            'count': len(complexity_results),\n",
    "            'satisfaction_rate': len([r for r in complexity_results if r.get('final_outcome') == 'satisfied']) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'escalation_rate': len([r for r in complexity_results if 'escalation' in r.get('final_outcome', '')]) / len(complexity_results) * 100 if complexity_results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in complexity_results) / len(complexity_results) if complexity_results else 0\n",
    "        }\n",
    "    \n",
    "    # Analyze by outcome\n",
    "    outcomes = list(set(r.get('final_outcome', 'unknown') for r in results))\n",
    "    for outcome in outcomes:\n",
    "        outcome_results = [r for r in results if r.get('final_outcome') == outcome]\n",
    "        export_data['conversation_analysis']['by_outcome'][outcome] = {\n",
    "            'count': len(outcome_results),\n",
    "            'percentage': len(outcome_results) / len(results) * 100 if results else 0,\n",
    "            'average_turns': sum(r.get('total_turns', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0,\n",
    "            'average_satisfaction': sum(r.get('customer_satisfaction', 0) for r in outcome_results) / len(outcome_results) if outcome_results else 0\n",
    "        }\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÑ Conversation results exported to: {results_path}\")\n",
    "    \n",
    "    # Export settings\n",
    "    settings_filename = f\"chatbot_agent_settings_{timestamp}.json\"\n",
    "    settings_path = experiment_dir / settings_filename\n",
    "    \n",
    "    # Get uploaded filename safely\n",
    "    uploaded_filename = 'unknown'\n",
    "    if file_upload.value:\n",
    "        if isinstance(file_upload.value, tuple) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = file_upload.value[0]['name']\n",
    "        elif isinstance(file_upload.value, dict) and len(file_upload.value) > 0:\n",
    "            uploaded_filename = list(file_upload.value.values())[0]['metadata']['name']\n",
    "    \n",
    "    # Get current settings from temp config files\n",
    "    try:\n",
    "        from ruamel.yaml import YAML\n",
    "        yaml = YAML()\n",
    "        \n",
    "        # Load current settings\n",
    "        with open(temp_file_paths['models_config'], 'r') as f:\n",
    "            models_config = yaml.load(f)\n",
    "        with open(temp_file_paths['agent_config'], 'r') as f:\n",
    "            agent_config = yaml.load(f)\n",
    "        \n",
    "        settings_export = {\n",
    "            'experiment_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'agent_type': 'chatbot_agent',\n",
    "                'experiment_type': 'full_conversation_simulation',\n",
    "                'total_questions': len(results),\n",
    "                'completed_conversations': len([r for r in results if r.get('conversation_complete', False)]),\n",
    "                'input_file': uploaded_filename\n",
    "            },\n",
    "            'model_settings': {\n",
    "                'preferred_model': models_config.get('primary_model', models_config.get('preferred', 'unknown')),\n",
    "                'fallback_models': models_config.get('fallback', []),\n",
    "                'temperature': agent_config.get('settings', {}).get('temperature', 0.5),\n",
    "                'max_tokens': agent_config.get('settings', {}).get('max_tokens', 2000),\n",
    "                'timeout': agent_config.get('settings', {}).get('timeout', 30)\n",
    "            },\n",
    "            'behavior_settings': agent_config.get('behavior', {}),\n",
    "            'escalation_settings': agent_config.get('escalation', {}),\n",
    "            'conversation_settings': {\n",
    "                'max_conversation_turns': 8,\n",
    "                'customer_ai_enabled': True,\n",
    "                'realistic_simulation': True\n",
    "            },\n",
    "            'configuration_files': {\n",
    "                'agent_config': agent_config,\n",
    "                'models_config': models_config\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load current settings: {e}\")\n",
    "        settings_export = {\n",
    "            'experiment_info': {\n",
    "                'timestamp': timestamp,\n",
    "                'agent_type': 'chatbot_agent',\n",
    "                'experiment_type': 'full_conversation_simulation',\n",
    "                'error': f\"Could not load settings: {e}\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    with open(settings_path, 'w') as f:\n",
    "        json.dump(settings_export, f, indent=2)\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Settings exported to: {settings_path}\")\n",
    "    \n",
    "    # Show enhanced summary\n",
    "    stats = export_data['summary_statistics']\n",
    "    print(f\"\\\\nüìä Export Summary:\")\n",
    "    print(f\"  Timestamp: {timestamp}\")\n",
    "    print(f\"  Results file: {results_filename}\")\n",
    "    print(f\"  Settings file: {settings_filename}\")\n",
    "    print(f\"  Total conversations: {stats['total_conversations']}\")\n",
    "    print(f\"  Completed conversations: {stats['completed_conversations']}\")\n",
    "    print(f\"  Satisfaction rate: {stats['satisfaction_rate']:.1f}%\")\n",
    "    print(f\"  Escalation rate: {stats['escalation_rate']:.1f}%\")\n",
    "    print(f\"  Average turns per conversation: {stats['average_turns']:.1f}\")\n",
    "    print(f\"  Average customer satisfaction: {stats['average_satisfaction']:.2f}\")\n",
    "    print(f\"  Files saved to: {experiment_dir.absolute()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to export. Please complete conversations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Review Results\n",
    "\n",
    "Display and analyze the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Conversation Results Review and Analysis\n",
      "\n",
      "=== CONVERSATION SUMMARY STATISTICS ===\n",
      "Total conversations: 20\n",
      "Completed conversations: 19\n",
      "Average turns per conversation: 3.3\n",
      "Average customer satisfaction: 0.605\n",
      "\n",
      "=== CONVERSATION OUTCOMES ===\n",
      "escalation_request: 13 (65.0%)\n",
      "satisfied: 6 (30.0%)\n",
      "conversation_incomplete: 1 (5.0%)\n",
      "\n",
      "=== ANALYSIS BY CUSTOMER TYPE ===\n",
      "               Avg Turns  Avg Satisfaction  Completed\n",
      "customer_type                                        \n",
      "confused             5.0              0.73          4\n",
      "frustrated           2.2              0.45          5\n",
      "normal               2.4              0.71          5\n",
      "urgent               3.6              0.53          5\n",
      "\n",
      "Satisfaction Rate by Customer Type:\n",
      "  frustrated: 0/5 (0.0%)\n",
      "  urgent: 0/5 (0.0%)\n",
      "  confused: 4/5 (80.0%)\n",
      "  normal: 2/5 (40.0%)\n",
      "\n",
      "=== ANALYSIS BY COMPLEXITY ===\n",
      "            Avg Turns  Avg Satisfaction  Completed\n",
      "complexity                                        \n",
      "complex         3.500             0.558          6\n",
      "medium          3.000             0.607          7\n",
      "simple          3.429             0.643          6\n",
      "\n",
      "=== CONVERSATION SUMMARY TABLE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>customer_type</th>\n",
       "      <th>complexity</th>\n",
       "      <th>total_turns</th>\n",
       "      <th>final_outcome</th>\n",
       "      <th>customer_satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>confused</td>\n",
       "      <td>simple</td>\n",
       "      <td>8</td>\n",
       "      <td>conversation_incomplete</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>1</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>6</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>urgent</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>confused</td>\n",
       "      <td>medium</td>\n",
       "      <td>5</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>normal</td>\n",
       "      <td>medium</td>\n",
       "      <td>2</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>urgent</td>\n",
       "      <td>complex</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>1</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>confused</td>\n",
       "      <td>complex</td>\n",
       "      <td>7</td>\n",
       "      <td>satisfied</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>urgent</td>\n",
       "      <td>medium</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>frustrated</td>\n",
       "      <td>complex</td>\n",
       "      <td>2</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>normal</td>\n",
       "      <td>simple</td>\n",
       "      <td>3</td>\n",
       "      <td>escalation_request</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id customer_type complexity  total_turns            final_outcome  \\\n",
       "0    1    frustrated     medium            3       escalation_request   \n",
       "1    2        urgent     simple            3       escalation_request   \n",
       "2    3      confused     simple            8  conversation_incomplete   \n",
       "3    4        normal     simple            3       escalation_request   \n",
       "4    5    frustrated    complex            1       escalation_request   \n",
       "5    6      confused     medium            3                satisfied   \n",
       "6    7        urgent    complex            6       escalation_request   \n",
       "7    8        urgent     simple            3       escalation_request   \n",
       "8    9        normal     simple            3       escalation_request   \n",
       "9   10      confused     medium            5                satisfied   \n",
       "10  11    frustrated     medium            2       escalation_request   \n",
       "11  12      confused    complex            2                satisfied   \n",
       "12  13        normal     medium            2                satisfied   \n",
       "13  14        urgent    complex            3       escalation_request   \n",
       "14  15    frustrated     medium            3       escalation_request   \n",
       "15  16        normal     simple            1                satisfied   \n",
       "16  17      confused    complex            7                satisfied   \n",
       "17  18        urgent     medium            3       escalation_request   \n",
       "18  19    frustrated    complex            2       escalation_request   \n",
       "19  20        normal     simple            3       escalation_request   \n",
       "\n",
       "    customer_satisfaction  \n",
       "0                    0.55  \n",
       "1                    0.65  \n",
       "2                    0.50  \n",
       "3                    0.70  \n",
       "4                    0.55  \n",
       "5                    0.80  \n",
       "6                    0.40  \n",
       "7                    0.65  \n",
       "8                    0.70  \n",
       "9                    0.80  \n",
       "10                   0.30  \n",
       "11                   0.75  \n",
       "12                   0.85  \n",
       "13                   0.55  \n",
       "14                   0.55  \n",
       "15                   0.70  \n",
       "16                   0.80  \n",
       "17                   0.40  \n",
       "18                   0.30  \n",
       "19                   0.60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE FULL CONVERSATIONS ===\n",
      "\n",
      "üü¢ SATISFIED CUSTOMER CONVERSATIONS:\n",
      "\n",
      "--- Conversation 6 (confused, medium) ---\n",
      "Original Question: What's the difference between comprehensive and collision coverage? I'm trying to decide what I need.\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.80)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: What's the difference between comprehensive and collision coverage? I'm trying to decide what I need.\n",
      "    Chatbot:  \n",
      "- Use warm, professional language\n",
      "\n",
      "Possible response:\n",
      "\"Hi there! I'm happy to help you understand the differences between comprehensive and collision...\n",
      "    Customer Reply: \"Okay, so let me make sure I understand. So comprehensive coverage is for things like theft or a tree falling on my car, but collision coverage is onl...\n",
      "    Satisfaction: 0.50\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: \"Okay, so let me make sure I understand. So comprehensive coverage is for things like theft or a tree falling on my car, but collision coverage is onl...\n",
      "    Chatbot:  \n",
      "- Use warm, professional language\n",
      "\n",
      "RESPONSE: \"Hello there! I'm happy to help you understand your coverage options. Yes, that's correct - comprehensiv...\n",
      "    Customer Reply: \"I see, so comprehensive coverage protects against non-collision events and collision coverage covers accidents involving another vehicle. But what ab...\n",
      "    Satisfaction: 0.60\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: \"I see, so comprehensive coverage protects against non-collision events and collision coverage covers accidents involving another vehicle. But what ab...\n",
      "    Chatbot:  \n",
      "- Maintain a warm, friendly tone\n",
      "\n",
      "Response: \"Ah, I see! Yes, comprehensive coverage would provide protection against an accident with a deer. Compreh...\n",
      "    Customer Reply: Oh I see! That makes much more sense now. Thank you for explaining it so clearly.\n",
      "    Satisfaction: 0.80\n",
      "\n",
      "--- Conversation 10 (confused, medium) ---\n",
      "Original Question: What exactly is an 'act of God' and why isn't my damage covered under it?\n",
      "Final Outcome: satisfied (Customer Satisfaction: 0.80)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: What exactly is an 'act of God' and why isn't my damage covered under it?\n",
      "    Chatbot:  \n",
      "- Use warm, professional language\n",
      "\n",
      "RESPONSE: Thank you for reaching out to us! An 'act of God' refers to unforeseen events beyond our control, such a...\n",
      "    Customer Reply: Oh, okay. So... what exactly constitutes an 'act of God' then? Is it like a tornado or something? And how do you determine whether the damage was caus...\n",
      "    Satisfaction: 0.50\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: Oh, okay. So... what exactly constitutes an 'act of God' then? Is it like a tornado or something? And how do you determine whether the damage was caus...\n",
      "    Chatbot:   in response\n",
      "- Acknowledge customer emotions and frustration\n",
      "\n",
      "RESPONSE:\n",
      "Thank you for reaching out to us! I'm happy to help you with this query. An ac...\n",
      "    Customer Reply: Oh, I see! So you're saying that the damage to my house wasn't caused by a tornado or hurricane? But why not? That makes no sense! Can you explain it ...\n",
      "    Satisfaction: 0.60\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: Oh, I see! So you're saying that the damage to my house wasn't caused by a tornado or hurricane? But why not? That makes no sense! Can you explain it ...\n",
      "    Chatbot:  \n",
      "- Use warm, professional language\n",
      "- Match the customer's tone when appropriate (professional with business, friendly with casual).\n",
      "    Customer Reply: Oh, I see. So if it's not an 'act of God,' then what is covered under my policy? Can you give me some examples? It seems like there are a lot of exclu...\n",
      "    Satisfaction: 0.60\n",
      "  ... and 2 more turns\n",
      "\n",
      "üî¥ ESCALATED CONVERSATIONS:\n",
      "\n",
      "--- Conversation 1 (frustrated, medium) ---\n",
      "Original Question: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.55)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: Why did my premium increase by $200? This is ridiculous - I haven't had any claims!\n",
      "    Chatbot:  \n",
      "- Maintain a warm and friendly tone throughout.\n",
      "    Customer Reply: \"I can't believe you are giving me this runaround! I demand to know why my premium increased by $200 when I haven't had any claims. This is ridiculous...\n",
      "    Satisfaction: 0.30\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: \"I can't believe you are giving me this runaround! I demand to know why my premium increased by $200 when I haven't had any claims. This is ridiculous...\n",
      "    Chatbot:  \n",
      "- Use warm, professional language.\n",
      "    Customer Reply: This isn't working. I need to speak to someone who can actually help me!\n",
      "    Satisfaction: 0.30\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: This isn't working. I need to speak to someone who can actually help me!\n",
      "    Chatbot:  \n",
      "- Use warm, friendly language\n",
      "\n",
      "RESPONSE: \n",
      "Thank you for reaching out to us. I apologize that you're experiencing issues with our product/service. I'm...\n",
      "    Customer Reply: Enough! Get me a human who knows what they're doing!\n",
      "    Satisfaction: 0.55\n",
      "\n",
      "--- Conversation 2 (urgent, simple) ---\n",
      "Original Question: I just had a car accident. What do I need to do right now?\n",
      "Final Outcome: escalation_request (Customer Satisfaction: 0.65)\n",
      "\n",
      "  Turn 1:\n",
      "    Customer: I just had a car accident. What do I need to do right now?\n",
      "    Chatbot:  \n",
      "- Use warm language\n",
      "\n",
      "RESPONSE: Oh no, I'm so sorry to hear that! If you haven't already, please call the police and file a report. Then, please reach...\n",
      "    Customer Reply: - Shows appreciation for the chatbot's response\n",
      "\n",
      "\"Thank you so much for your help! I really appreciate it. Can you tell me more about what I need to d...\n",
      "    Satisfaction: 0.50\n",
      "\n",
      "  Turn 2:\n",
      "    Customer: - Shows appreciation for the chatbot's response\n",
      "\n",
      "\"Thank you so much for your help! I really appreciate it. Can you tell me more about what I need to d...\n",
      "    Chatbot:  \n",
      "- Use warm, friendly language to build rapport and maintain customer trust I've prioritized your request to help resolve this quickly.\n",
      "    Customer Reply: Oh, great! So I should just call the police right away? What else do I need to do? This is a huge hassle and I want to get it taken care of ASAP.\n",
      "    Satisfaction: 0.65\n",
      "\n",
      "  Turn 3:\n",
      "    Customer: Oh, great! So I should just call the police right away? What else do I need to do? This is a huge hassle and I want to get it taken care of ASAP.\n",
      "    Chatbot:  \n",
      "- Prioritize customer satisfaction and positive experience\n",
      "\n",
      "Possible response:\n",
      "\"I completely understand, sir/ma'am. In this situation, it is crucial ...\n",
      "    Customer Reply: I've spent too much time on this already. I need to speak with a human.\n",
      "    Satisfaction: 0.65\n",
      "\n",
      "=== CONVERSATION INSIGHTS ===\n",
      "Conversation Length Distribution:\n",
      "  Short (1-2 turns): 6 (30.0%)\n",
      "  Medium (3-5 turns): 11 (55.0%)\n",
      "  Long (6+ turns): 3 (15.0%)\n",
      "\n",
      "Customer Satisfaction Distribution:\n",
      "  High (0.8+): 4 (20.0%)\n",
      "  Medium (0.5-0.8): 12 (60.0%)\n",
      "  Low (<0.5): 4 (20.0%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\n",
      "üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\n"
     ]
    }
   ],
   "source": [
    "# Review and analyze conversation results\n",
    "if 'results' in locals() and results:\n",
    "    print(\"üìã Conversation Results Review and Analysis\\n\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_results = pd.DataFrame([\n",
    "        {\n",
    "            'id': r.get('id'),\n",
    "            'customer_type': r.get('customer_type'),\n",
    "            'complexity': r.get('complexity'),\n",
    "            'total_turns': r.get('total_turns', 0),\n",
    "            'final_outcome': r.get('final_outcome'),\n",
    "            'customer_satisfaction': r.get('customer_satisfaction', 0),\n",
    "            'conversation_complete': r.get('conversation_complete', False)\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    print(\"=== CONVERSATION SUMMARY STATISTICS ===\")\n",
    "    print(f\"Total conversations: {len(df_results)}\")\n",
    "    print(f\"Completed conversations: {df_results['conversation_complete'].sum()}\")\n",
    "    print(f\"Average turns per conversation: {df_results['total_turns'].mean():.1f}\")\n",
    "    print(f\"Average customer satisfaction: {df_results['customer_satisfaction'].mean():.3f}\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(\"\\n=== CONVERSATION OUTCOMES ===\")\n",
    "    outcome_counts = df_results['final_outcome'].value_counts()\n",
    "    for outcome, count in outcome_counts.items():\n",
    "        percentage = count / len(df_results) * 100\n",
    "        print(f\"{outcome}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analysis by customer type\n",
    "    print(\"\\n=== ANALYSIS BY CUSTOMER TYPE ===\")\n",
    "    customer_analysis = df_results.groupby('customer_type').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    customer_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(customer_analysis)\n",
    "    \n",
    "    # Satisfaction rate by customer type\n",
    "    print(\"\\nSatisfaction Rate by Customer Type:\")\n",
    "    for ctype in df_results['customer_type'].unique():\n",
    "        ctype_data = df_results[df_results['customer_type'] == ctype]\n",
    "        satisfied = len(ctype_data[ctype_data['final_outcome'] == 'satisfied'])\n",
    "        total = len(ctype_data)\n",
    "        print(f\"  {ctype}: {satisfied}/{total} ({satisfied/total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by complexity\n",
    "    print(\"\\n=== ANALYSIS BY COMPLEXITY ===\")\n",
    "    complexity_analysis = df_results.groupby('complexity').agg({\n",
    "        'total_turns': 'mean',\n",
    "        'customer_satisfaction': 'mean',\n",
    "        'conversation_complete': 'sum'\n",
    "    }).round(3)\n",
    "    complexity_analysis.columns = ['Avg Turns', 'Avg Satisfaction', 'Completed']\n",
    "    print(complexity_analysis)\n",
    "    \n",
    "    # Show detailed results table\n",
    "    print(\"\\n=== CONVERSATION SUMMARY TABLE ===\")\n",
    "    display_df = df_results[['id', 'customer_type', 'complexity', 'total_turns', 'final_outcome', 'customer_satisfaction']].copy()\n",
    "    display_df['customer_satisfaction'] = display_df['customer_satisfaction'].round(3)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Show sample full conversations\n",
    "    print(\"\\n=== SAMPLE FULL CONVERSATIONS ===\")\n",
    "    \n",
    "    # Show 2 satisfied and 2 escalated conversations for analysis\n",
    "    satisfied_conversations = [r for r in results if r.get('final_outcome') == 'satisfied']\n",
    "    escalated_conversations = [r for r in results if 'escalation' in r.get('final_outcome', '')]\n",
    "    \n",
    "    def display_conversation(conversation_data, max_turns=3):\n",
    "        \"\"\"Display a conversation with turn-by-turn analysis\"\"\"\n",
    "        conv_id = conversation_data.get('id')\n",
    "        customer_type = conversation_data.get('customer_type')\n",
    "        complexity = conversation_data.get('complexity')\n",
    "        outcome = conversation_data.get('final_outcome')\n",
    "        satisfaction = conversation_data.get('customer_satisfaction', 0)\n",
    "        \n",
    "        print(f\"\\n--- Conversation {conv_id} ({customer_type}, {complexity}) ---\")\n",
    "        print(f\"Original Question: {conversation_data.get('original_question', '')}\")\n",
    "        print(f\"Final Outcome: {outcome} (Customer Satisfaction: {satisfaction:.2f})\")\n",
    "        \n",
    "        conversation_history = conversation_data.get('conversation_history', [])\n",
    "        turns_to_show = min(max_turns, len(conversation_history))\n",
    "        \n",
    "        for i, turn in enumerate(conversation_history[:turns_to_show]):\n",
    "            turn_num = turn.get('turn_number', i+1)\n",
    "            print(f\"\\n  Turn {turn_num}:\")\n",
    "            print(f\"    Customer: {turn.get('customer_query', '')[:150]}{'...' if len(turn.get('customer_query', '')) > 150 else ''}\")\n",
    "            print(f\"    Chatbot:  {turn.get('chatbot_response', '')[:150]}{'...' if len(turn.get('chatbot_response', '')) > 150 else ''}\")\n",
    "            \n",
    "            if 'customer_response' in turn:\n",
    "                print(f\"    Customer Reply: {turn.get('customer_response', '')[:150]}{'...' if len(turn.get('customer_response', '')) > 150 else ''}\")\n",
    "                print(f\"    Satisfaction: {turn.get('customer_satisfaction', 0):.2f}\")\n",
    "        \n",
    "        if len(conversation_history) > turns_to_show:\n",
    "            print(f\"  ... and {len(conversation_history) - turns_to_show} more turns\")\n",
    "    \n",
    "    # Show satisfied conversations\n",
    "    if satisfied_conversations:\n",
    "        print(\"\\nüü¢ SATISFIED CUSTOMER CONVERSATIONS:\")\n",
    "        for conv in satisfied_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Show escalated conversations  \n",
    "    if escalated_conversations:\n",
    "        print(\"\\nüî¥ ESCALATED CONVERSATIONS:\")\n",
    "        for conv in escalated_conversations[:2]:\n",
    "            display_conversation(conv)\n",
    "    \n",
    "    # Conversation insights\n",
    "    print(\"\\n=== CONVERSATION INSIGHTS ===\")\n",
    "    \n",
    "    # Turn analysis\n",
    "    short_conversations = len(df_results[df_results['total_turns'] <= 2])\n",
    "    medium_conversations = len(df_results[(df_results['total_turns'] > 2) & (df_results['total_turns'] <= 5)])\n",
    "    long_conversations = len(df_results[df_results['total_turns'] > 5])\n",
    "    \n",
    "    print(f\"Conversation Length Distribution:\")\n",
    "    print(f\"  Short (1-2 turns): {short_conversations} ({short_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (3-5 turns): {medium_conversations} ({medium_conversations/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Long (6+ turns): {long_conversations} ({long_conversations/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Satisfaction insights\n",
    "    high_satisfaction = len(df_results[df_results['customer_satisfaction'] >= 0.8])\n",
    "    medium_satisfaction = len(df_results[(df_results['customer_satisfaction'] >= 0.5) & (df_results['customer_satisfaction'] < 0.8)])\n",
    "    low_satisfaction = len(df_results[df_results['customer_satisfaction'] < 0.5])\n",
    "    \n",
    "    print(f\"\\nCustomer Satisfaction Distribution:\")\n",
    "    print(f\"  High (0.8+): {high_satisfaction} ({high_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.5-0.8): {medium_satisfaction} ({medium_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    print(f\"  Low (<0.5): {low_satisfaction} ({low_satisfaction/len(df_results)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Conversation analysis complete! Check the exported files for full conversation details.\")\n",
    "    print(\"üí° Each conversation includes turn-by-turn customer-AI interaction data for detailed analysis.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No conversation results to review. Please complete conversations first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
