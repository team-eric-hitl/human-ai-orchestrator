{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Playground - Modular LangGraph Hybrid System\n",
    "\n",
    "This notebook provides a playground for testing various aspects of the Modular LangGraph Hybrid System, starting with the Answer Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /workspace\n",
      "‚úÖ All modules loaded successfully using direct imports\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from pprint import pprint\n",
    "\n",
    "os.chdir('..') \n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path().cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Import project modules using direct imports\n",
    "try:\n",
    "    # Core imports\n",
    "    from src.core.config.agent_config_manager import AgentConfigManager\n",
    "    from src.core.context_manager import SQLiteContextProvider\n",
    "    from src.interfaces.core.state_schema import HybridSystemState\n",
    "    from src.nodes.answer_agent import AnswerAgentNode\n",
    "    from src.integrations.llm_providers import *\n",
    "    \n",
    "    print(\"‚úÖ All modules loaded successfully using direct imports\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüí° Trying simpler approach...\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback: Create minimal versions for testing\n",
    "        class MockAgentConfigManager:\n",
    "            def __init__(self, config_dir, environment='development'):\n",
    "                self.config_dir = Path(config_dir)\n",
    "                self.environment = environment\n",
    "                print(f\"Mock AgentConfigManager initialized with {config_dir}\")\n",
    "            \n",
    "            def get_summary(self):\n",
    "                return {\"status\": \"mock\", \"config_dir\": str(self.config_dir)}\n",
    "                \n",
    "            def get_agent_config(self, agent_name):\n",
    "                return None\n",
    "                \n",
    "        AgentConfigManager = MockAgentConfigManager\n",
    "        SQLiteContextProvider = None\n",
    "        AnswerAgentNode = None\n",
    "        HybridSystemState = dict  # Use dict as fallback\n",
    "        \n",
    "        print(\"‚ö†Ô∏è Using mock implementations - limited functionality available\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "        print(\"\\nüîß Manual Setup Required:\")\n",
    "        print(\"1. Ensure you're running from the project root directory\")\n",
    "        print(\"2. Fix relative imports in the source files\") \n",
    "        print(\"3. Or run: cd /workspace && python -m jupyter notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration Manager Summary:\n",
      "{'agent_names': ['evaluator_agent',\n",
      "                 'human_interface',\n",
      "                 'escalation_router',\n",
      "                 'answer_agent'],\n",
      " 'agents_loaded': 4,\n",
      " 'config_directory': '/workspace/config',\n",
      " 'config_files_structure': {'agents': True,\n",
      "                            'environments': True,\n",
      "                            'shared': True},\n",
      " 'environment': 'development',\n",
      " 'models_configured': 8,\n",
      " 'providers_configured': 3,\n",
      " 'system_name': 'Modular LangGraph Hybrid System',\n",
      " 'system_version': '1.0.0',\n",
      " 'thresholds': {'confidence_threshold': 0.6,\n",
      "                'escalation_score': 5.0,\n",
      "                'max_retries': 5,\n",
      "                'response_time_limit': 60,\n",
      "                'session_timeout': 3600}}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the configuration manager\n",
    "config_dir = project_root / 'config'\n",
    "config_manager = AgentConfigManager(config_dir, environment='development')\n",
    "\n",
    "print(\"üìã Configuration Manager Summary:\")\n",
    "pprint(config_manager.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Agent Configuration Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Answer Agent Configuration:\n",
      "Name: answer_agent\n",
      "Description: Primary response generation agent that provides direct answers to user queries\n",
      "Type: llm_agent\n",
      "\n",
      "üéØ Model Configuration:\n",
      "Preferred Model: anthropic_general_budget\n",
      "Fallback Models: ['local_general_budget', 'local_general_standard']\n",
      "\n",
      "‚öôÔ∏è Settings:\n",
      "{'max_retries': 3,\n",
      " 'max_tokens': 2000,\n",
      " 'response_time_limit': 30,\n",
      " 'temperature': 0.7,\n",
      " 'timeout': 30}\n",
      "\n",
      "üß† Behavior Configuration:\n",
      "{'context_integration': True,\n",
      " 'personalization': True,\n",
      " 'response_style': 'clear_and_professional'}\n",
      "\n",
      "üìà Escalation Configuration:\n",
      "{'confidence_threshold': 0.7,\n",
      " 'enable_auto_escalation': True,\n",
      " 'escalation_triggers': ['low_confidence',\n",
      "                         'user_dissatisfaction',\n",
      "                         'repeat_query']}\n",
      "\n",
      "üí¨ System Prompt:\n",
      "'You are a helpful AI assistant. Provide accurate, helpful responses to user queries.  Use context from previous conversations when available to provide more personalized assistance.\n",
      "'\n",
      "\n",
      "üìù Available Prompt Templates:\n",
      "  greeting: Hello! How can I assist you today?\n",
      "  no_context: I'll help you with your question. Let me provide a comprehensive response.\n",
      "\n",
      "  with_context: Based on our previous conversation, I can see you're working on {context_topic}.  Let me help you wi...\n",
      "  clarification_request: I want to make sure I understand your question correctly. Could you please clarify:\n",
      "\n",
      "  confidence_low: I'm not entirely certain about this answer. Let me provide what I know and suggest  where you might ...\n"
     ]
    }
   ],
   "source": [
    "# Get the answer agent configuration\n",
    "answer_agent_config = config_manager.get_agent_config('answer_agent')\n",
    "\n",
    "if answer_agent_config:\n",
    "    print(\"ü§ñ Answer Agent Configuration:\")\n",
    "    print(f\"Name: {answer_agent_config.name}\")\n",
    "    print(f\"Description: {answer_agent_config.description}\")\n",
    "    print(f\"Type: {answer_agent_config.type}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üéØ Model Configuration:\")\n",
    "    print(f\"Preferred Model: {answer_agent_config.get_preferred_model()}\")\n",
    "    print(f\"Fallback Models: {answer_agent_config.get_fallback_models()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"‚öôÔ∏è Settings:\")\n",
    "    pprint(answer_agent_config.settings)\n",
    "    print()\n",
    "    \n",
    "    print(\"üß† Behavior Configuration:\")\n",
    "    pprint(answer_agent_config.behavior)\n",
    "    print()\n",
    "    \n",
    "    print(\"üìà Escalation Configuration:\")\n",
    "    pprint(answer_agent_config.escalation)\n",
    "    print()\n",
    "    \n",
    "    print(\"üí¨ System Prompt:\")\n",
    "    print(f\"'{answer_agent_config.get_prompt('system')}'\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üìù Available Prompt Templates:\")\n",
    "    templates = answer_agent_config.prompts.get('templates', {})\n",
    "    for template_name, template_content in templates.items():\n",
    "        print(f\"  {template_name}: {template_content[:100]}{'...' if len(template_content) > 100 else ''}\")\n",
    "else:\n",
    "    print(\"‚ùå Answer agent configuration not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Context Provider and Answer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 19:08:16.175 [INFO    ] factory         | Attempting to create provider: anthropic_general_budget\n",
      "‚úÖ Creating LLM provider: anthropic_general_budget ‚Üí claude-3-5-haiku-20241022 (anthropic)\n",
      "‚úÖ 19:08:16.177 [INFO    ] claude-3-5-haiku-20241022 | Initializing LLM provider | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 19:08:16.181 [INFO    ] claude-3-5-haiku-20241022 | LLM provider initialized successfully | model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ 19:08:16.183 [INFO    ] factory         | Successfully created provider: anthropic_general_budget\n",
      "‚úÖ 19:08:16.184 [INFO    ] answer_agent    | Answer Agent LLM provider initialized | operation=initialize_llm_provider model_name=claude-3-5-haiku-20241022\n",
      "‚úÖ Answer Agent initialized successfully\n",
      "üîó LLM Provider: LLMProvider\n",
      "üìä Model Config: {'type': 'anthropic', 'model_name': 'claude-3-5-haiku-20241022', 'temperature': 0.7, 'max_tokens': 2000, 'description': 'Anthropic Claude 3.5 Haiku - fast and efficient'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize context provider\n",
    "context_provider = SQLiteContextProvider(str(project_root / 'hybrid_system.db'))\n",
    "\n",
    "# Initialize answer agent\n",
    "try:\n",
    "    answer_agent = AnswerAgentNode(config_manager, context_provider)\n",
    "    print(\"‚úÖ Answer Agent initialized successfully\")\n",
    "    \n",
    "    # Display LLM provider information if available\n",
    "    if answer_agent.llm_provider:\n",
    "        print(f\"üîó LLM Provider: {type(answer_agent.llm_provider).__name__}\")\n",
    "        if hasattr(answer_agent.llm_provider, 'model_config'):\n",
    "            print(f\"üìä Model Config: {answer_agent.llm_provider.model_config}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è LLM Provider not initialized (likely due to missing API keys or model files)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize Answer Agent: {e}\")\n",
    "    answer_agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Questions and Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper function to create test state\n",
    "def create_test_state(query: str, user_id: str = \"test_user\", session_id: str = None) -> HybridSystemState:\n",
    "    if session_id is None:\n",
    "        session_id = f\"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    return {\n",
    "        \"query_id\": str(uuid.uuid4()),\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"query\": query,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"messages\": [],\n",
    "        \"context\": {},\n",
    "        \"next_action\": \"answer\"\n",
    "    }\n",
    "\n",
    "# Helper function to display answer results\n",
    "def display_answer_result(query: str, result: HybridSystemState):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üôã Question: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'ai_response' in result:\n",
    "        print(f\"ü§ñ Answer: {result['ai_response']}\")\n",
    "        print()\n",
    "        \n",
    "        if 'initial_assessment' in result:\n",
    "            assessment = result['initial_assessment']\n",
    "            print(\"üìä Assessment:\")\n",
    "            print(f\"  Context Used: {assessment.get('context_used', 'N/A')}\")\n",
    "            print(f\"  Confidence: {assessment.get('confidence', 'N/A')}\")\n",
    "            print(f\"  Response Time: {assessment.get('response_time', 'N/A')}s\")\n",
    "            print(f\"  Next Action: {result.get('next_action', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"‚ùå No AI response generated\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Answer Agent with various questions...\n",
      "\n",
      "\n",
      "üîÑ Test 1/5\n",
      "‚úÖ 19:08:21.316 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: What is artificial intelligence?\n",
      "============================================================\n",
      "ü§ñ Answer: Artificial Intelligence (AI) is a branch of computer science focused on creating intelligent machines that can simulate human-like cognitive functions such as learning, problem-solving, perception, reasoning, and understanding natural language. AI systems use algorithms, machine learning, and large datasets to perform tasks that traditionally require human intelligence. There are different types of AI, including:\n",
      "\n",
      "1. Narrow/Weak AI: Designed for specific tasks (like voice assistants or recommendation systems)\n",
      "2. General AI: Theoretical systems with human-like cognitive abilities across multiple domains\n",
      "3. Machine Learning: AI systems that can learn and improve from experience without being explicitly programmed\n",
      "4. Deep Learning: Advanced machine learning using neural networks inspired by the human brain\n",
      "\n",
      "AI is used in many fields like healthcare, finance, transportation, robotics, and more. Examples include self-driving cars, medical diagnosis systems, language translation, image recognition, and predictive analytics.\n",
      "\n",
      "The goal of AI is to develop systems that can understand, learn, adapt, and potentially make decisions in ways similar to humans, while also being able to process and analyze vast amounts of data much faster than human capabilities.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: False\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Test 2/5\n",
      "‚úÖ 19:08:26.453 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: How does machine learning work?\n",
      "============================================================\n",
      "ü§ñ Answer: Machine learning is a subset of artificial intelligence that focuses on enabling computers to learn and improve from experience without being explicitly programmed. Here's a basic overview of how machine learning works:\n",
      "\n",
      "1. Data Collection\n",
      "- ML algorithms need large amounts of data to learn from\n",
      "- Data can be labeled (supervised learning) or unlabeled (unsupervised learning)\n",
      "\n",
      "2. Training Process\n",
      "- The algorithm is fed training data\n",
      "- It identifies patterns and relationships within the data\n",
      "- Creates a mathematical model that can make predictions or decisions\n",
      "\n",
      "3. Types of Machine Learning\n",
      "- Supervised Learning: Uses labeled data to predict outcomes\n",
      "- Unsupervised Learning: Finds hidden patterns in unlabeled data\n",
      "- Reinforcement Learning: Learns through trial and error with rewards/penalties\n",
      "\n",
      "4. Key Components\n",
      "- Input data\n",
      "- Features (characteristics of the data)\n",
      "- Algorithm/model\n",
      "- Output/prediction\n",
      "\n",
      "5. Improvement Mechanism\n",
      "- The model continuously refines itself as it receives more data\n",
      "- Performance is measured and the model is adjusted to increase accuracy\n",
      "\n",
      "Machine learning powers many technologies we use daily, like recommendation systems, image recognition, and voice assistants.\n",
      "\n",
      "Would you like me to elaborate on any of these points?\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Test 3/5\n",
      "‚úÖ 19:08:31.836 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: Can you explain the difference between supervised and unsupervised learning?\n",
      "============================================================\n",
      "ü§ñ Answer: Based on our previous discussions about machine learning and artificial intelligence, I'll be happy to explain the difference between supervised and unsupervised learning.\n",
      "\n",
      "Supervised Learning:\n",
      "- In supervised learning, the algorithm is trained using labeled data\n",
      "- Each training example has an input and a corresponding correct output or label\n",
      "- The goal is for the algorithm to learn a function that can map inputs to correct outputs\n",
      "- Examples include classification (predicting a category) and regression (predicting a numerical value)\n",
      "- Common algorithms: Linear Regression, Decision Trees, Support Vector Machines\n",
      "\n",
      "Unsupervised Learning:\n",
      "- In unsupervised learning, the algorithm works with unlabeled data\n",
      "- There are no predefined correct outputs or labels\n",
      "- The goal is to discover hidden patterns or structures in the data\n",
      "- Common techniques include clustering (grouping similar data points) and dimensionality reduction\n",
      "- Examples: K-means clustering, Principal Component Analysis (PCA)\n",
      "\n",
      "The key difference is the presence or absence of labeled data during the training process. Supervised learning learns from known outcomes, while unsupervised learning discovers patterns independently.\n",
      "\n",
      "Would you like me to elaborate on any part of this explanation?\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Test 4/5\n",
      "‚úÖ 19:08:37.377 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: What are the benefits of using LangGraph for building AI workflows?\n",
      "============================================================\n",
      "ü§ñ Answer: LangGraph is a powerful library for building complex, stateful AI workflows and multi-agent systems. Here are some key benefits:\n",
      "\n",
      "1. State Management\n",
      "- Allows you to create and manage complex state across multiple AI agents or steps\n",
      "- Enables tracking of conversation history, context, and intermediate results\n",
      "- Supports dynamic workflow modification during runtime\n",
      "\n",
      "2. Flexible Workflow Design\n",
      "- Supports creating directed graphs for AI interactions\n",
      "- Can define complex branching logic and decision-making processes\n",
      "- Enables creation of multi-agent systems with different roles and interactions\n",
      "\n",
      "3. Integration with LangChain\n",
      "- Seamlessly works with LangChain's ecosystem of tools and models\n",
      "- Provides enhanced capabilities for building AI applications\n",
      "- Supports various large language models and AI frameworks\n",
      "\n",
      "4. Advanced Control Flow\n",
      "- Implements cyclic and conditional workflows\n",
      "- Allows for retry mechanisms and error handling\n",
      "- Supports human-in-the-loop interactions\n",
      "\n",
      "5. Scalability and Modularity\n",
      "- Easy to break down complex AI tasks into smaller, manageable components\n",
      "- Facilitates easier debugging and maintenance of AI workflows\n",
      "- Supports parallel processing and distributed computing\n",
      "\n",
      "These benefits make LangGraph particularly useful for building sophisticated AI applications like conversational agents, research assistants, and complex reasoning systems.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Test 5/5\n",
      "‚úÖ 19:08:45.574 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: How would you implement a simple chatbot?\n",
      "============================================================\n",
      "ü§ñ Answer: Based on our recent discussions about AI and machine learning, I'll outline a few approaches to implementing a simple chatbot:\n",
      "\n",
      "1. Rule-Based Approach:\n",
      "- Define predefined rules and response patterns\n",
      "- Use if-else statements or pattern matching\n",
      "- Good for simple, predictable interactions\n",
      "- Limited flexibility and intelligence\n",
      "\n",
      "2. Machine Learning Approach:\n",
      "- Use supervised learning techniques\n",
      "- Train on conversational datasets\n",
      "- Can use models like:\n",
      "  - Sequence-to-sequence models\n",
      "  - Transformer-based models (BERT, GPT)\n",
      "- More adaptive and context-aware\n",
      "\n",
      "3. Framework-Based Implementation:\n",
      "- Leverage libraries/frameworks like:\n",
      "  - LangChain\n",
      "  - Rasa\n",
      "  - Dialogflow\n",
      "- Provides pre-built components for natural language processing\n",
      "- Easier to develop and deploy\n",
      "\n",
      "Recommended Basic Implementation Steps:\n",
      "1. Choose a natural language processing technique\n",
      "2. Prepare training data\n",
      "3. Design conversation flow\n",
      "4. Implement intent recognition\n",
      "5. Create response generation mechanism\n",
      "6. Add error handling and fallback responses\n",
      "\n",
      "Would you like me to elaborate on any of these approaches or provide a sample code implementation?\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test questions to evaluate the answer agent\n",
    "test_questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"Can you explain the difference between supervised and unsupervised learning?\",\n",
    "    \"What are the benefits of using LangGraph for building AI workflows?\",\n",
    "    \"How would you implement a simple chatbot?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Answer Agent with various questions...\\n\")\n",
    "\n",
    "if answer_agent:\n",
    "    session_id = f\"test_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nüîÑ Test {i}/{len(test_questions)}\")\n",
    "        \n",
    "        try:\n",
    "            # Create test state\n",
    "            test_state = create_test_state(question, session_id=session_id)\n",
    "            \n",
    "            # Generate answer\n",
    "            result = answer_agent(test_state)\n",
    "            \n",
    "            # Display result\n",
    "            display_answer_result(question, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing question '{question}': {e}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"‚ùå Answer agent not available - skipping tests\")\n",
    "    print(\"üí° This is likely due to missing LLM configuration or API keys\")\n",
    "    print(\"üí° Check your environment variables and model configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Testing - Follow-up Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Testing context integration with follow-up questions...\n",
      "\n",
      "üîÑ Establishing context...\n",
      "‚úÖ 19:08:51.225 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: What is Python programming language?\n",
      "============================================================\n",
      "ü§ñ Answer: Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility. Here are some key characteristics of Python:\n",
      "\n",
      "1. Key Features:\n",
      "- Easy to learn and read\n",
      "- Supports multiple programming paradigms (object-oriented, functional, procedural)\n",
      "- Dynamically typed\n",
      "- Extensive standard library\n",
      "- Cross-platform compatibility\n",
      "\n",
      "2. Uses:\n",
      "- Web development\n",
      "- Data science and machine learning\n",
      "- Scientific computing\n",
      "- Artificial intelligence\n",
      "- Automation and scripting\n",
      "- Game development\n",
      "- Desktop applications\n",
      "\n",
      "3. Advantages:\n",
      "- Clean and straightforward syntax\n",
      "- Large and supportive community\n",
      "- Numerous third-party libraries and frameworks\n",
      "- Rapid development speed\n",
      "- Free and open-source\n",
      "\n",
      "4. Created by:\n",
      "- Guido van Rossum in 1991\n",
      "- Named after the comedy group Monty Python\n",
      "\n",
      "5. Popular Libraries/Frameworks:\n",
      "- Django (web development)\n",
      "- NumPy (scientific computing)\n",
      "- Pandas (data analysis)\n",
      "- TensorFlow (machine learning)\n",
      "- Pygame (game development)\n",
      "\n",
      "Python is widely used by companies like Google, Netflix, Spotify, and NASA, making it a powerful and versatile programming language.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: False\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Follow-up 1/3\n",
      "‚úÖ 19:08:56.130 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: What are its main advantages?\n",
      "============================================================\n",
      "ü§ñ Answer: Based on our previous discussion about Python, here are its main advantages:\n",
      "\n",
      "1. Easy to Learn and Read\n",
      "- Simple, clear syntax\n",
      "- Resembles plain English\n",
      "- Great for beginners and experienced programmers\n",
      "\n",
      "2. Versatility\n",
      "- Used in web development\n",
      "- Data science and machine learning\n",
      "- Scientific computing\n",
      "- Automation and scripting\n",
      "- Artificial intelligence\n",
      "- Game development\n",
      "\n",
      "3. Large Standard Library\n",
      "- Extensive built-in functions and modules\n",
      "- Reduces need for external libraries\n",
      "- Speeds up development process\n",
      "\n",
      "4. Open Source and Free\n",
      "- Free to use and distribute\n",
      "- Large, active community\n",
      "- Constant improvements and updates\n",
      "\n",
      "5. Cross-Platform Compatibility\n",
      "- Runs on Windows, macOS, Linux\n",
      "- Code can be easily transferred between platforms\n",
      "\n",
      "6. Strong Community Support\n",
      "- Extensive documentation\n",
      "- Many online resources\n",
      "- Active forums and support groups\n",
      "\n",
      "7. High-Level Language\n",
      "- Automatic memory management\n",
      "- Dynamic typing\n",
      "- Faster development compared to lower-level languages\n",
      "\n",
      "8. Integration Capabilities\n",
      "- Can be integrated with other languages like C, C++\n",
      "- Supports multiple programming paradigms\n",
      "\n",
      "These advantages make Python a popular choice for many programming tasks and industries.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Follow-up 2/3\n",
      "‚úÖ 19:09:01.239 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: Can you give me some examples of what it's used for?\n",
      "============================================================\n",
      "ü§ñ Answer: Based on the context of our previous conversation about Python, I can provide some common use cases for the Python programming language:\n",
      "\n",
      "1. Web Development\n",
      "- Using frameworks like Django and Flask\n",
      "- Creating backend server-side applications\n",
      "- Building web APIs\n",
      "\n",
      "2. Data Science and Machine Learning\n",
      "- Data analysis with libraries like Pandas\n",
      "- Machine learning and AI with scikit-learn and TensorFlow\n",
      "- Data visualization using Matplotlib and Seaborn\n",
      "\n",
      "3. Scientific and Numeric Computing\n",
      "- Scientific research and computational tasks\n",
      "- Numerical computing with NumPy\n",
      "- Complex mathematical calculations\n",
      "\n",
      "4. Automation and Scripting\n",
      "- Writing scripts to automate repetitive tasks\n",
      "- System administration\n",
      "- Network automation\n",
      "\n",
      "5. Game Development\n",
      "- Creating games using Pygame library\n",
      "- Developing game prototypes\n",
      "\n",
      "6. Cybersecurity and Networking\n",
      "- Writing security tools\n",
      "- Network scanning and penetration testing\n",
      "- Analyzing network traffic\n",
      "\n",
      "7. Desktop GUI Applications\n",
      "- Creating graphical user interfaces with Tkinter\n",
      "- Developing cross-platform desktop applications\n",
      "\n",
      "These diverse applications showcase Python's versatility and why it's so popular across different industries and domains.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n",
      "\n",
      "üîÑ Follow-up 3/3\n",
      "‚úÖ 19:09:04.346 [INFO    ] claude-3-5-haiku-20241022 | Model call: claude-3-5-haiku-20241022 - generate_response | model_name=claude-3-5-haiku-20241022 operation=generate_response\n",
      "\n",
      "============================================================\n",
      "üôã Question: How does it compare to Java?\n",
      "============================================================\n",
      "ü§ñ Answer: I apologize, but I notice that the context doesn't specify what \"it\" refers to. Without knowing the specific technology or programming language being discussed, I can't provide a direct comparison to Java. \n",
      "\n",
      "To give a helpful response, could you clarify:\n",
      "1. What specific technology or language are you asking about?\n",
      "2. Are you looking to compare it with Java in terms of performance, use cases, syntax, or something else?\n",
      "\n",
      "If you can provide more context from your previous conversations, I'll be happy to give a precise comparison with Java.\n",
      "\n",
      "üìä Assessment:\n",
      "  Context Used: True\n",
      "  Confidence: 0.85\n",
      "  Response Time: 2.3s\n",
      "  Next Action: evaluate\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test context integration with follow-up questions\n",
    "print(\"üîó Testing context integration with follow-up questions...\\n\")\n",
    "\n",
    "if answer_agent:\n",
    "    context_session_id = f\"context_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # First question to establish context\n",
    "    first_question = \"What is Python programming language?\"\n",
    "    print(\"üîÑ Establishing context...\")\n",
    "    \n",
    "    try:\n",
    "        test_state = create_test_state(first_question, session_id=context_session_id)\n",
    "        result1 = answer_agent(test_state)\n",
    "        display_answer_result(first_question, result1)\n",
    "        \n",
    "        # Follow-up questions that should use context\n",
    "        followup_questions = [\n",
    "            \"What are its main advantages?\",\n",
    "            \"Can you give me some examples of what it's used for?\",\n",
    "            \"How does it compare to Java?\"\n",
    "        ]\n",
    "        \n",
    "        for i, followup in enumerate(followup_questions, 1):\n",
    "            print(f\"\\nüîÑ Follow-up {i}/{len(followup_questions)}\")\n",
    "            \n",
    "            test_state = create_test_state(followup, session_id=context_session_id)\n",
    "            result = answer_agent(test_state)\n",
    "            display_answer_result(followup, result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in context testing: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Answer agent not available - skipping context tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Configuration Experimentation Section\n",
      "\n",
      "üîß Current Agent Settings:\n",
      "  Temperature: 0.7\n",
      "  Max Tokens: 2000\n",
      "  Response Time Limit: 30\n",
      "  Context Integration: True\n",
      "  Confidence Threshold: 0.7\n",
      "\n",
      "üéõÔ∏è Available Configuration Options:\n",
      "  - Modify temperature for creativity vs consistency\n",
      "  - Adjust max_tokens for response length\n",
      "  - Enable/disable context integration\n",
      "  - Change confidence thresholds for escalation\n",
      "  - Switch between different models\n",
      "\n",
      "üí° To experiment with settings:\n",
      "  1. Modify config/agents/answer_agent/config.yaml\n",
      "  2. Call config_manager.reload() to apply changes\n",
      "  3. Reinitialize the answer agent\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different configuration settings\n",
    "print(\"üß™ Configuration Experimentation Section\")\n",
    "print()\n",
    "\n",
    "# Display current agent settings\n",
    "if answer_agent_config:\n",
    "    print(\"üîß Current Agent Settings:\")\n",
    "    print(f\"  Temperature: {answer_agent_config.get_setting('temperature', 'Not set')}\")\n",
    "    print(f\"  Max Tokens: {answer_agent_config.get_setting('max_tokens', 'Not set')}\")\n",
    "    print(f\"  Response Time Limit: {answer_agent_config.get_setting('response_time_limit', 'Not set')}\")\n",
    "    print(f\"  Context Integration: {answer_agent_config.get_setting('context_integration', 'Not set')}\")\n",
    "    print(f\"  Confidence Threshold: {answer_agent_config.get_setting('confidence_threshold', 'Not set')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üéõÔ∏è Available Configuration Options:\")\n",
    "    print(\"  - Modify temperature for creativity vs consistency\")\n",
    "    print(\"  - Adjust max_tokens for response length\")\n",
    "    print(\"  - Enable/disable context integration\")\n",
    "    print(\"  - Change confidence thresholds for escalation\")\n",
    "    print(\"  - Switch between different models\")\n",
    "    print()\n",
    "    \n",
    "    print(\"üí° To experiment with settings:\")\n",
    "    print(\"  1. Modify config/agents/answer_agent/config.yaml\")\n",
    "    print(\"  2. Call config_manager.reload() to apply changes\")\n",
    "    print(\"  3. Reinitialize the answer agent\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Agent configuration not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Status and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç System Status and Diagnostics\n",
      "\n",
      "üìã Configuration Status:\n",
      "  config_directory: /workspace/config\n",
      "  environment: development\n",
      "  system_name: Modular LangGraph Hybrid System\n",
      "  system_version: 1.0.0\n",
      "  agents_loaded: 4\n",
      "  agent_names: ['evaluator_agent', 'human_interface', 'escalation_router', 'answer_agent']\n",
      "  models_configured: 8\n",
      "  providers_configured: 3\n",
      "  config_files_structure: {'shared': True, 'agents': True, 'environments': True}\n",
      "\n",
      "ü§ñ Available Agents:\n",
      "  evaluator_agent: Response quality evaluation and escalation decision specialist\n",
      "  human_interface: Manages handoffs between AI and human agents with context preservation\n",
      "  escalation_router: Routes escalated queries to appropriate human agents based on expertise and priority\n",
      "  answer_agent: Primary response generation agent that provides direct answers to user queries\n",
      "\n",
      "üóÑÔ∏è Context Provider Status:\n",
      "  Database connection: ‚úÖ Working\n",
      "  Context entries for test user: 0\n",
      "\n",
      "üîó LLM Provider Status:\n",
      "  Provider: ‚úÖ LLMProvider\n",
      "  Model: Unknown\n",
      "  Type: Unknown\n",
      "\n",
      "‚úÖ Diagnostics complete!\n"
     ]
    }
   ],
   "source": [
    "# System status and diagnostics\n",
    "print(\"üîç System Status and Diagnostics\")\n",
    "print()\n",
    "\n",
    "# Configuration status\n",
    "print(\"üìã Configuration Status:\")\n",
    "summary = config_manager.get_summary()\n",
    "for key, value in summary.items():\n",
    "    if key != 'thresholds':  # Skip detailed thresholds for cleaner output\n",
    "        print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Available agents\n",
    "print(\"ü§ñ Available Agents:\")\n",
    "agents = config_manager.get_available_agents()\n",
    "for agent in agents:\n",
    "    agent_config = config_manager.get_agent_config(agent)\n",
    "    if agent_config:\n",
    "        print(f\"  {agent}: {agent_config.description}\")\n",
    "print()\n",
    "\n",
    "# Context provider status\n",
    "print(\"üóÑÔ∏è Context Provider Status:\")\n",
    "try:\n",
    "    # Test context provider\n",
    "    test_summary = context_provider.get_context_summary(\"test_user\", \"test_session\")\n",
    "    print(f\"  Database connection: ‚úÖ Working\")\n",
    "    print(f\"  Context entries for test user: {test_summary.get('entries_count', 0)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Database connection: ‚ùå Error - {e}\")\n",
    "print()\n",
    "\n",
    "# LLM Provider status\n",
    "print(\"üîó LLM Provider Status:\")\n",
    "if answer_agent and answer_agent.llm_provider:\n",
    "    print(f\"  Provider: ‚úÖ {type(answer_agent.llm_provider).__name__}\")\n",
    "    if hasattr(answer_agent.llm_provider, 'model_config'):\n",
    "        model_config = answer_agent.llm_provider.model_config\n",
    "        print(f\"  Model: {getattr(model_config, 'name', 'Unknown')}\")\n",
    "        print(f\"  Type: {getattr(model_config, 'type', 'Unknown')}\")\n",
    "else:\n",
    "    print(f\"  Provider: ‚ùå Not available\")\n",
    "    print(f\"  Reason: Likely missing API keys or model files\")\n",
    "\n",
    "print(\"\\n‚úÖ Diagnostics complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This playground provides a foundation for testing the Answer Agent. Here are some next steps you can explore:\n",
    "\n",
    "1. **Test Other Agents**: Load and test the EvaluatorAgent and EscalationRouter\n",
    "2. **Workflow Testing**: Test the complete hybrid workflow with all agents\n",
    "3. **Configuration Experiments**: Modify agent configurations and observe behavior changes\n",
    "4. **Performance Testing**: Test with larger volumes of queries\n",
    "5. **Context Analysis**: Examine how context affects response quality over time\n",
    "6. **Model Comparison**: Test different LLM models and compare their performance\n",
    "\n",
    "Use the cells above as templates for building more comprehensive tests!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
