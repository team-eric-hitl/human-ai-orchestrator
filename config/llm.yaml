# LLM provider configuration
# This file configures which model to use and how to use it

# Provider strategy: auto, local, openai, or specific model name
provider: "auto"

# Primary model to use (from models.yaml)
primary_model: "llama-7b"

# Enable fallback to other models if primary fails
enable_fallback: true

# Use case optimization (general, code, fast, high_quality)
use_case: "general"

# Override model selection based on environment
# This allows .env to override config files for deployment flexibility
environment_overrides:
  enable: true
  # If LLM_PROVIDER is set in .env, use that strategy
  provider_env_var: "LLM_PROVIDER"
  # If LLM_MODEL is set in .env, use that specific model
  model_env_var: "LLM_MODEL"

# Model validation
validation:
  # Check if model files exist before attempting to use them
  check_model_files: true
  # Timeout for model loading (seconds)
  loading_timeout: 30
  # Skip models that fail validation
  skip_invalid_models: true

# Performance settings
performance:
  # Enable model caching
  enable_caching: true
  # Cache size (number of models to keep in memory)
  cache_size: 2
  # Preload primary model at startup
  preload_primary: false

# Logging and monitoring
logging:
  # Log model selection decisions
  log_model_selection: true
  # Log model performance metrics
  log_performance: true
  # Log to LangSmith if available
  enable_langsmith: true