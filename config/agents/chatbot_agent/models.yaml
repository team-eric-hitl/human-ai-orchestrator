primary_model: "llm_demo_speed"

model_overrides:
  temperature: 0.7
  max_tokens: 800          # Reduced from 2000 for faster responses
  
  # Model-specific overrides
  per_model:
    codellama-7b:
      max_tokens: 3000
      
