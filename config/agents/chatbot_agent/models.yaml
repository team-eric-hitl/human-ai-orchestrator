primary_model: "llm_demo_speed"

model_overrides:
  temperature: 0.7
  max_tokens: 3000         # Increased for comprehensive responses
  
  # Model-specific overrides
  per_model:
    codellama-7b:
      max_tokens: 3000
      
