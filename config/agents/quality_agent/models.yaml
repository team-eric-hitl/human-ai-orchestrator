primary_model: "llm_demo_speed"

model_overrides:
  temperature: 0.1          # Very low temperature for fast, deterministic responses
  max_tokens: 100           # Further reduced for ultra-fast responses
  
  # Task-specific overrides
  per_model:
    quality_assessment_compact:
      temperature: 0.1      # Very low for objective assessment
      max_tokens: 30        # Ultra-compact: "8.2|0.85|accurate, concise"
      
    quality_assessment:
      temperature: 0.2      # Very low for objective assessment
      max_tokens: 500       # Keep full tokens for verbose mode
      
    response_adjustment:
      temperature: 0.3      # Further reduced for speed
      max_tokens: 400       # Further reduced for speed
      
    context_analysis:
      temperature: 0.1      # Further reduced for speed
      max_tokens: 150       # Further reduced for speed