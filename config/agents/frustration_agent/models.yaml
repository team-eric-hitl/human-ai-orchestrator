primary_model: "llm_demo_speed"

model_overrides:
  temperature: 0.1          # Very low temperature for fast, deterministic responses
  max_tokens: 30           # Further reduced for ultra-fast responses
  
  # Task-specific overrides
  per_model:
    frustration_analysis_compact:
      temperature: 0.1      # Very low for objective sentiment analysis
      max_tokens: 30        # Ultra-compact: "7.5|0.9|angry language"
      
    frustration_analysis:
      temperature: 0.1      # Very low for objective sentiment analysis
      max_tokens: 30       # Keep full tokens for verbose mode
      
    pattern_detection:
      temperature: 0.1      # Reduced for consistency
      max_tokens: 100       # Further reduced for speed
      
    trend_analysis:
      temperature: 0.1      # Further reduced for speed
      max_tokens: 150       # Further reduced from 200